{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sampled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>136330</td>\n",
       "      <td>2.108286</td>\n",
       "      <td>-0.020359</td>\n",
       "      <td>-2.234273</td>\n",
       "      <td>-0.124080</td>\n",
       "      <td>0.559843</td>\n",
       "      <td>-1.315913</td>\n",
       "      <td>0.631887</td>\n",
       "      <td>-0.385490</td>\n",
       "      <td>0.150628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162940</td>\n",
       "      <td>0.519705</td>\n",
       "      <td>-0.091751</td>\n",
       "      <td>-0.379542</td>\n",
       "      <td>0.418822</td>\n",
       "      <td>0.248646</td>\n",
       "      <td>-0.098857</td>\n",
       "      <td>-0.094773</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>116819</td>\n",
       "      <td>2.080143</td>\n",
       "      <td>-0.075408</td>\n",
       "      <td>-1.359381</td>\n",
       "      <td>0.261263</td>\n",
       "      <td>0.232201</td>\n",
       "      <td>-0.797886</td>\n",
       "      <td>0.233487</td>\n",
       "      <td>-0.330165</td>\n",
       "      <td>0.493154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.317765</td>\n",
       "      <td>-0.784150</td>\n",
       "      <td>0.233078</td>\n",
       "      <td>-0.636937</td>\n",
       "      <td>-0.191137</td>\n",
       "      <td>0.234986</td>\n",
       "      <td>-0.070648</td>\n",
       "      <td>-0.062794</td>\n",
       "      <td>17.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>27917</td>\n",
       "      <td>1.152650</td>\n",
       "      <td>0.204938</td>\n",
       "      <td>0.441832</td>\n",
       "      <td>1.211595</td>\n",
       "      <td>-0.254895</td>\n",
       "      <td>-0.383380</td>\n",
       "      <td>-0.004286</td>\n",
       "      <td>-0.019590</td>\n",
       "      <td>0.085094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066674</td>\n",
       "      <td>0.209828</td>\n",
       "      <td>-0.054351</td>\n",
       "      <td>0.058103</td>\n",
       "      <td>0.515330</td>\n",
       "      <td>-0.299402</td>\n",
       "      <td>0.032551</td>\n",
       "      <td>0.022865</td>\n",
       "      <td>21.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>97443</td>\n",
       "      <td>-0.389692</td>\n",
       "      <td>0.410148</td>\n",
       "      <td>0.616187</td>\n",
       "      <td>-0.686644</td>\n",
       "      <td>1.040312</td>\n",
       "      <td>-0.322199</td>\n",
       "      <td>0.523625</td>\n",
       "      <td>-0.037453</td>\n",
       "      <td>1.319237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116692</td>\n",
       "      <td>0.685267</td>\n",
       "      <td>-0.182303</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>-0.242738</td>\n",
       "      <td>0.009678</td>\n",
       "      <td>0.115370</td>\n",
       "      <td>0.176041</td>\n",
       "      <td>6.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>17920</td>\n",
       "      <td>-1.306986</td>\n",
       "      <td>0.183306</td>\n",
       "      <td>3.164803</td>\n",
       "      <td>0.810135</td>\n",
       "      <td>1.529576</td>\n",
       "      <td>2.388322</td>\n",
       "      <td>0.551279</td>\n",
       "      <td>-0.054401</td>\n",
       "      <td>1.675481</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.203993</td>\n",
       "      <td>0.556523</td>\n",
       "      <td>0.146513</td>\n",
       "      <td>-1.128693</td>\n",
       "      <td>-0.633623</td>\n",
       "      <td>-0.454903</td>\n",
       "      <td>-0.826329</td>\n",
       "      <td>-0.826186</td>\n",
       "      <td>4.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0  136330  2.108286 -0.020359 -2.234273 -0.124080  0.559843 -1.315913   \n",
       "1  116819  2.080143 -0.075408 -1.359381  0.261263  0.232201 -0.797886   \n",
       "2   27917  1.152650  0.204938  0.441832  1.211595 -0.254895 -0.383380   \n",
       "3   97443 -0.389692  0.410148  0.616187 -0.686644  1.040312 -0.322199   \n",
       "4   17920 -1.306986  0.183306  3.164803  0.810135  1.529576  2.388322   \n",
       "\n",
       "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0  0.631887 -0.385490  0.150628  ...  0.162940  0.519705 -0.091751 -0.379542   \n",
       "1  0.233487 -0.330165  0.493154  ... -0.317765 -0.784150  0.233078 -0.636937   \n",
       "2 -0.004286 -0.019590  0.085094  ...  0.066674  0.209828 -0.054351  0.058103   \n",
       "3  0.523625 -0.037453  1.319237  ...  0.116692  0.685267 -0.182303  0.767857   \n",
       "4  0.551279 -0.054401  1.675481  ... -0.203993  0.556523  0.146513 -1.128693   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.418822  0.248646 -0.098857 -0.094773    7.70      0  \n",
       "1 -0.191137  0.234986 -0.070648 -0.062794   17.99      0  \n",
       "2  0.515330 -0.299402  0.032551  0.022865   21.00      0  \n",
       "3 -0.242738  0.009678  0.115370  0.176041    6.60      0  \n",
       "4 -0.633623 -0.454903 -0.826329 -0.826186    4.95      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#표준화 하기 전 데이터를 target과 feature로 나누자\n",
    "df_t = df.iloc[:, -1] #target\n",
    "df_f = df.iloc[:, :-1] #feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.87599191,  1.10227326, -0.01383661, ..., -0.24466092,\n",
       "        -0.29889867, -0.34162683],\n",
       "       [ 0.46599202,  1.08762774, -0.04747727, ..., -0.17331757,\n",
       "        -0.19381068, -0.29798293],\n",
       "       [-1.4021752 ,  0.60496356,  0.12384231, ...,  0.08768117,\n",
       "         0.08768119, -0.28521635],\n",
       "       ...,\n",
       "       [-0.42810237, -1.77186915, -0.42136644, ...,  0.4808021 ,\n",
       "        -0.90645255,  0.37580428],\n",
       "       [ 0.90608364,  0.67055034, -1.06901447, ..., -0.29992819,\n",
       "         0.00759972,  1.21623571],\n",
       "       [-1.90993121,  0.72058018, -0.22056932, ..., -0.04120382,\n",
       "         0.07781589, -0.25934387]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scailing - StandardScaler 사용\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(df_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.875992</td>\n",
       "      <td>1.102273</td>\n",
       "      <td>-0.013837</td>\n",
       "      <td>-1.519427</td>\n",
       "      <td>-0.092494</td>\n",
       "      <td>0.415984</td>\n",
       "      <td>-0.996862</td>\n",
       "      <td>0.529260</td>\n",
       "      <td>-0.325542</td>\n",
       "      <td>0.140034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.389289</td>\n",
       "      <td>0.217877</td>\n",
       "      <td>0.717147</td>\n",
       "      <td>-0.171055</td>\n",
       "      <td>-0.622575</td>\n",
       "      <td>0.811479</td>\n",
       "      <td>0.520065</td>\n",
       "      <td>-0.244661</td>\n",
       "      <td>-0.298899</td>\n",
       "      <td>-0.341627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.465992</td>\n",
       "      <td>1.087628</td>\n",
       "      <td>-0.047477</td>\n",
       "      <td>-0.930583</td>\n",
       "      <td>0.178984</td>\n",
       "      <td>0.173824</td>\n",
       "      <td>-0.604853</td>\n",
       "      <td>0.194623</td>\n",
       "      <td>-0.278678</td>\n",
       "      <td>0.453561</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.174391</td>\n",
       "      <td>-0.431114</td>\n",
       "      <td>-1.088996</td>\n",
       "      <td>0.399380</td>\n",
       "      <td>-1.044356</td>\n",
       "      <td>-0.370659</td>\n",
       "      <td>0.491879</td>\n",
       "      <td>-0.173318</td>\n",
       "      <td>-0.193811</td>\n",
       "      <td>-0.297983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-1.402175</td>\n",
       "      <td>0.604964</td>\n",
       "      <td>0.123842</td>\n",
       "      <td>0.281721</td>\n",
       "      <td>0.848501</td>\n",
       "      <td>-0.186188</td>\n",
       "      <td>-0.291182</td>\n",
       "      <td>-0.005096</td>\n",
       "      <td>-0.015599</td>\n",
       "      <td>0.080048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225029</td>\n",
       "      <td>0.087910</td>\n",
       "      <td>0.287896</td>\n",
       "      <td>-0.105378</td>\n",
       "      <td>0.094572</td>\n",
       "      <td>0.998518</td>\n",
       "      <td>-0.610760</td>\n",
       "      <td>0.087681</td>\n",
       "      <td>0.087681</td>\n",
       "      <td>-0.285216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.058829</td>\n",
       "      <td>-0.197667</td>\n",
       "      <td>0.249246</td>\n",
       "      <td>0.399070</td>\n",
       "      <td>-0.488826</td>\n",
       "      <td>0.771099</td>\n",
       "      <td>-0.244885</td>\n",
       "      <td>0.438324</td>\n",
       "      <td>-0.030730</td>\n",
       "      <td>1.209704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.125580</td>\n",
       "      <td>0.155438</td>\n",
       "      <td>0.946489</td>\n",
       "      <td>-0.330075</td>\n",
       "      <td>1.257612</td>\n",
       "      <td>-0.470666</td>\n",
       "      <td>0.026986</td>\n",
       "      <td>0.297138</td>\n",
       "      <td>0.591042</td>\n",
       "      <td>-0.346292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-1.612250</td>\n",
       "      <td>-0.675024</td>\n",
       "      <td>0.110623</td>\n",
       "      <td>2.114412</td>\n",
       "      <td>0.565669</td>\n",
       "      <td>1.132714</td>\n",
       "      <td>1.806258</td>\n",
       "      <td>0.461553</td>\n",
       "      <td>-0.045086</td>\n",
       "      <td>1.535788</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.682098</td>\n",
       "      <td>-0.277512</td>\n",
       "      <td>0.768150</td>\n",
       "      <td>0.247363</td>\n",
       "      <td>-1.850173</td>\n",
       "      <td>-1.228224</td>\n",
       "      <td>-0.931617</td>\n",
       "      <td>-2.084501</td>\n",
       "      <td>-2.702442</td>\n",
       "      <td>-0.353291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28475</td>\n",
       "      <td>-1.269851</td>\n",
       "      <td>-0.303659</td>\n",
       "      <td>0.045113</td>\n",
       "      <td>1.652929</td>\n",
       "      <td>0.549944</td>\n",
       "      <td>-0.343748</td>\n",
       "      <td>1.176108</td>\n",
       "      <td>-0.300411</td>\n",
       "      <td>0.632508</td>\n",
       "      <td>0.600344</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118970</td>\n",
       "      <td>0.152658</td>\n",
       "      <td>0.798688</td>\n",
       "      <td>-0.012156</td>\n",
       "      <td>-0.489836</td>\n",
       "      <td>-0.263086</td>\n",
       "      <td>-0.676960</td>\n",
       "      <td>0.320643</td>\n",
       "      <td>0.103960</td>\n",
       "      <td>-0.148559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28476</td>\n",
       "      <td>0.717842</td>\n",
       "      <td>1.005432</td>\n",
       "      <td>-0.160124</td>\n",
       "      <td>-1.274450</td>\n",
       "      <td>0.223870</td>\n",
       "      <td>0.311822</td>\n",
       "      <td>-0.612151</td>\n",
       "      <td>0.464676</td>\n",
       "      <td>-0.280385</td>\n",
       "      <td>0.046481</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071334</td>\n",
       "      <td>0.111694</td>\n",
       "      <td>0.395568</td>\n",
       "      <td>-0.135871</td>\n",
       "      <td>-0.513670</td>\n",
       "      <td>0.430712</td>\n",
       "      <td>1.253071</td>\n",
       "      <td>-0.281188</td>\n",
       "      <td>-0.248409</td>\n",
       "      <td>-0.038367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28477</td>\n",
       "      <td>-0.428102</td>\n",
       "      <td>-1.771869</td>\n",
       "      <td>-0.421366</td>\n",
       "      <td>0.944253</td>\n",
       "      <td>-1.515500</td>\n",
       "      <td>0.294278</td>\n",
       "      <td>-0.779094</td>\n",
       "      <td>-2.189361</td>\n",
       "      <td>-2.011887</td>\n",
       "      <td>1.684925</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.629370</td>\n",
       "      <td>-2.046568</td>\n",
       "      <td>0.404609</td>\n",
       "      <td>-4.579363</td>\n",
       "      <td>0.428147</td>\n",
       "      <td>-0.572160</td>\n",
       "      <td>-0.415799</td>\n",
       "      <td>0.480802</td>\n",
       "      <td>-0.906453</td>\n",
       "      <td>0.375804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28478</td>\n",
       "      <td>0.906084</td>\n",
       "      <td>0.670550</td>\n",
       "      <td>-1.069014</td>\n",
       "      <td>-0.594471</td>\n",
       "      <td>0.304593</td>\n",
       "      <td>-0.827940</td>\n",
       "      <td>-0.147452</td>\n",
       "      <td>-0.270542</td>\n",
       "      <td>0.052176</td>\n",
       "      <td>0.971411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633389</td>\n",
       "      <td>0.552837</td>\n",
       "      <td>0.515797</td>\n",
       "      <td>-0.295245</td>\n",
       "      <td>-0.511557</td>\n",
       "      <td>-1.032926</td>\n",
       "      <td>1.283600</td>\n",
       "      <td>-0.299928</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>1.216236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28479</td>\n",
       "      <td>-1.909931</td>\n",
       "      <td>0.720580</td>\n",
       "      <td>-0.220569</td>\n",
       "      <td>0.456264</td>\n",
       "      <td>-0.322366</td>\n",
       "      <td>-0.768278</td>\n",
       "      <td>-0.751067</td>\n",
       "      <td>-0.503880</td>\n",
       "      <td>-0.249034</td>\n",
       "      <td>0.433548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102164</td>\n",
       "      <td>-0.315702</td>\n",
       "      <td>-0.796420</td>\n",
       "      <td>0.213515</td>\n",
       "      <td>0.490653</td>\n",
       "      <td>0.403848</td>\n",
       "      <td>-1.070219</td>\n",
       "      <td>-0.041204</td>\n",
       "      <td>0.077816</td>\n",
       "      <td>-0.259344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28480 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0      0.875992  1.102273 -0.013837 -1.519427 -0.092494  0.415984 -0.996862   \n",
       "1      0.465992  1.087628 -0.047477 -0.930583  0.178984  0.173824 -0.604853   \n",
       "2     -1.402175  0.604964  0.123842  0.281721  0.848501 -0.186188 -0.291182   \n",
       "3      0.058829 -0.197667  0.249246  0.399070 -0.488826  0.771099 -0.244885   \n",
       "4     -1.612250 -0.675024  0.110623  2.114412  0.565669  1.132714  1.806258   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "28475 -1.269851 -0.303659  0.045113  1.652929  0.549944 -0.343748  1.176108   \n",
       "28476  0.717842  1.005432 -0.160124 -1.274450  0.223870  0.311822 -0.612151   \n",
       "28477 -0.428102 -1.771869 -0.421366  0.944253 -1.515500  0.294278 -0.779094   \n",
       "28478  0.906084  0.670550 -1.069014 -0.594471  0.304593 -0.827940 -0.147452   \n",
       "28479 -1.909931  0.720580 -0.220569  0.456264 -0.322366 -0.768278 -0.751067   \n",
       "\n",
       "             V7        V8        V9  ...       V20       V21       V22  \\\n",
       "0      0.529260 -0.325542  0.140034  ... -0.389289  0.217877  0.717147   \n",
       "1      0.194623 -0.278678  0.453561  ... -0.174391 -0.431114 -1.088996   \n",
       "2     -0.005096 -0.015599  0.080048  ... -0.225029  0.087910  0.287896   \n",
       "3      0.438324 -0.030730  1.209704  ... -0.125580  0.155438  0.946489   \n",
       "4      0.461553 -0.045086  1.535788  ... -0.682098 -0.277512  0.768150   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "28475 -0.300411  0.632508  0.600344  ... -0.118970  0.152658  0.798688   \n",
       "28476  0.464676 -0.280385  0.046481  ... -0.071334  0.111694  0.395568   \n",
       "28477 -2.189361 -2.011887  1.684925  ... -0.629370 -2.046568  0.404609   \n",
       "28478 -0.270542  0.052176  0.971411  ...  0.633389  0.552837  0.515797   \n",
       "28479 -0.503880 -0.249034  0.433548  ...  0.102164 -0.315702 -0.796420   \n",
       "\n",
       "            V23       V24       V25       V26       V27       V28    Amount  \n",
       "0     -0.171055 -0.622575  0.811479  0.520065 -0.244661 -0.298899 -0.341627  \n",
       "1      0.399380 -1.044356 -0.370659  0.491879 -0.173318 -0.193811 -0.297983  \n",
       "2     -0.105378  0.094572  0.998518 -0.610760  0.087681  0.087681 -0.285216  \n",
       "3     -0.330075  1.257612 -0.470666  0.026986  0.297138  0.591042 -0.346292  \n",
       "4      0.247363 -1.850173 -1.228224 -0.931617 -2.084501 -2.702442 -0.353291  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "28475 -0.012156 -0.489836 -0.263086 -0.676960  0.320643  0.103960 -0.148559  \n",
       "28476 -0.135871 -0.513670  0.430712  1.253071 -0.281188 -0.248409 -0.038367  \n",
       "28477 -4.579363  0.428147 -0.572160 -0.415799  0.480802 -0.906453  0.375804  \n",
       "28478 -0.295245 -0.511557 -1.032926  1.283600 -0.299928  0.007600  1.216236  \n",
       "28479  0.213515  0.490653  0.403848 -1.070219 -0.041204  0.077816 -0.259344  \n",
       "\n",
       "[28480 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#스케일링한 Feature 데이터를 담은 df_new 새로 생성\n",
    "df_new = pd.DataFrame(scaler.fit_transform(df_f), columns = df_f.columns)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.875992</td>\n",
       "      <td>1.102273</td>\n",
       "      <td>-0.013837</td>\n",
       "      <td>-1.519427</td>\n",
       "      <td>-0.092494</td>\n",
       "      <td>0.415984</td>\n",
       "      <td>-0.996862</td>\n",
       "      <td>0.529260</td>\n",
       "      <td>-0.325542</td>\n",
       "      <td>0.140034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217877</td>\n",
       "      <td>0.717147</td>\n",
       "      <td>-0.171055</td>\n",
       "      <td>-0.622575</td>\n",
       "      <td>0.811479</td>\n",
       "      <td>0.520065</td>\n",
       "      <td>-0.244661</td>\n",
       "      <td>-0.298899</td>\n",
       "      <td>-0.341627</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.465992</td>\n",
       "      <td>1.087628</td>\n",
       "      <td>-0.047477</td>\n",
       "      <td>-0.930583</td>\n",
       "      <td>0.178984</td>\n",
       "      <td>0.173824</td>\n",
       "      <td>-0.604853</td>\n",
       "      <td>0.194623</td>\n",
       "      <td>-0.278678</td>\n",
       "      <td>0.453561</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.431114</td>\n",
       "      <td>-1.088996</td>\n",
       "      <td>0.399380</td>\n",
       "      <td>-1.044356</td>\n",
       "      <td>-0.370659</td>\n",
       "      <td>0.491879</td>\n",
       "      <td>-0.173318</td>\n",
       "      <td>-0.193811</td>\n",
       "      <td>-0.297983</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-1.402175</td>\n",
       "      <td>0.604964</td>\n",
       "      <td>0.123842</td>\n",
       "      <td>0.281721</td>\n",
       "      <td>0.848501</td>\n",
       "      <td>-0.186188</td>\n",
       "      <td>-0.291182</td>\n",
       "      <td>-0.005096</td>\n",
       "      <td>-0.015599</td>\n",
       "      <td>0.080048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087910</td>\n",
       "      <td>0.287896</td>\n",
       "      <td>-0.105378</td>\n",
       "      <td>0.094572</td>\n",
       "      <td>0.998518</td>\n",
       "      <td>-0.610760</td>\n",
       "      <td>0.087681</td>\n",
       "      <td>0.087681</td>\n",
       "      <td>-0.285216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.058829</td>\n",
       "      <td>-0.197667</td>\n",
       "      <td>0.249246</td>\n",
       "      <td>0.399070</td>\n",
       "      <td>-0.488826</td>\n",
       "      <td>0.771099</td>\n",
       "      <td>-0.244885</td>\n",
       "      <td>0.438324</td>\n",
       "      <td>-0.030730</td>\n",
       "      <td>1.209704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155438</td>\n",
       "      <td>0.946489</td>\n",
       "      <td>-0.330075</td>\n",
       "      <td>1.257612</td>\n",
       "      <td>-0.470666</td>\n",
       "      <td>0.026986</td>\n",
       "      <td>0.297138</td>\n",
       "      <td>0.591042</td>\n",
       "      <td>-0.346292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-1.612250</td>\n",
       "      <td>-0.675024</td>\n",
       "      <td>0.110623</td>\n",
       "      <td>2.114412</td>\n",
       "      <td>0.565669</td>\n",
       "      <td>1.132714</td>\n",
       "      <td>1.806258</td>\n",
       "      <td>0.461553</td>\n",
       "      <td>-0.045086</td>\n",
       "      <td>1.535788</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277512</td>\n",
       "      <td>0.768150</td>\n",
       "      <td>0.247363</td>\n",
       "      <td>-1.850173</td>\n",
       "      <td>-1.228224</td>\n",
       "      <td>-0.931617</td>\n",
       "      <td>-2.084501</td>\n",
       "      <td>-2.702442</td>\n",
       "      <td>-0.353291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0  0.875992  1.102273 -0.013837 -1.519427 -0.092494  0.415984 -0.996862   \n",
       "1  0.465992  1.087628 -0.047477 -0.930583  0.178984  0.173824 -0.604853   \n",
       "2 -1.402175  0.604964  0.123842  0.281721  0.848501 -0.186188 -0.291182   \n",
       "3  0.058829 -0.197667  0.249246  0.399070 -0.488826  0.771099 -0.244885   \n",
       "4 -1.612250 -0.675024  0.110623  2.114412  0.565669  1.132714  1.806258   \n",
       "\n",
       "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0  0.529260 -0.325542  0.140034  ...  0.217877  0.717147 -0.171055 -0.622575   \n",
       "1  0.194623 -0.278678  0.453561  ... -0.431114 -1.088996  0.399380 -1.044356   \n",
       "2 -0.005096 -0.015599  0.080048  ...  0.087910  0.287896 -0.105378  0.094572   \n",
       "3  0.438324 -0.030730  1.209704  ...  0.155438  0.946489 -0.330075  1.257612   \n",
       "4  0.461553 -0.045086  1.535788  ... -0.277512  0.768150  0.247363 -1.850173   \n",
       "\n",
       "        V25       V26       V27       V28    Amount  target  \n",
       "0  0.811479  0.520065 -0.244661 -0.298899 -0.341627       0  \n",
       "1 -0.370659  0.491879 -0.173318 -0.193811 -0.297983       0  \n",
       "2  0.998518 -0.610760  0.087681  0.087681 -0.285216       0  \n",
       "3 -0.470666  0.026986  0.297138  0.591042 -0.346292       0  \n",
       "4 -1.228224 -0.931617 -2.084501 -2.702442 -0.353291       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#표준화 한 feature 데이터 df_new를 target과 합체\n",
    "#target은 어차피 0과 1로 이루어졌기 때문에 굳이 스케일링을 할 필요가 없다.\n",
    "df_new['target'] = df_t\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>136330</td>\n",
       "      <td>2.108286</td>\n",
       "      <td>-0.020359</td>\n",
       "      <td>-2.234273</td>\n",
       "      <td>-0.124080</td>\n",
       "      <td>0.559843</td>\n",
       "      <td>-1.315913</td>\n",
       "      <td>0.631887</td>\n",
       "      <td>-0.385490</td>\n",
       "      <td>0.150628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162940</td>\n",
       "      <td>0.519705</td>\n",
       "      <td>-0.091751</td>\n",
       "      <td>-0.379542</td>\n",
       "      <td>0.418822</td>\n",
       "      <td>0.248646</td>\n",
       "      <td>-0.098857</td>\n",
       "      <td>-0.094773</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>116819</td>\n",
       "      <td>2.080143</td>\n",
       "      <td>-0.075408</td>\n",
       "      <td>-1.359381</td>\n",
       "      <td>0.261263</td>\n",
       "      <td>0.232201</td>\n",
       "      <td>-0.797886</td>\n",
       "      <td>0.233487</td>\n",
       "      <td>-0.330165</td>\n",
       "      <td>0.493154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.317765</td>\n",
       "      <td>-0.784150</td>\n",
       "      <td>0.233078</td>\n",
       "      <td>-0.636937</td>\n",
       "      <td>-0.191137</td>\n",
       "      <td>0.234986</td>\n",
       "      <td>-0.070648</td>\n",
       "      <td>-0.062794</td>\n",
       "      <td>17.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>27917</td>\n",
       "      <td>1.152650</td>\n",
       "      <td>0.204938</td>\n",
       "      <td>0.441832</td>\n",
       "      <td>1.211595</td>\n",
       "      <td>-0.254895</td>\n",
       "      <td>-0.383380</td>\n",
       "      <td>-0.004286</td>\n",
       "      <td>-0.019590</td>\n",
       "      <td>0.085094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066674</td>\n",
       "      <td>0.209828</td>\n",
       "      <td>-0.054351</td>\n",
       "      <td>0.058103</td>\n",
       "      <td>0.515330</td>\n",
       "      <td>-0.299402</td>\n",
       "      <td>0.032551</td>\n",
       "      <td>0.022865</td>\n",
       "      <td>21.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>97443</td>\n",
       "      <td>-0.389692</td>\n",
       "      <td>0.410148</td>\n",
       "      <td>0.616187</td>\n",
       "      <td>-0.686644</td>\n",
       "      <td>1.040312</td>\n",
       "      <td>-0.322199</td>\n",
       "      <td>0.523625</td>\n",
       "      <td>-0.037453</td>\n",
       "      <td>1.319237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116692</td>\n",
       "      <td>0.685267</td>\n",
       "      <td>-0.182303</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>-0.242738</td>\n",
       "      <td>0.009678</td>\n",
       "      <td>0.115370</td>\n",
       "      <td>0.176041</td>\n",
       "      <td>6.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>17920</td>\n",
       "      <td>-1.306986</td>\n",
       "      <td>0.183306</td>\n",
       "      <td>3.164803</td>\n",
       "      <td>0.810135</td>\n",
       "      <td>1.529576</td>\n",
       "      <td>2.388322</td>\n",
       "      <td>0.551279</td>\n",
       "      <td>-0.054401</td>\n",
       "      <td>1.675481</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.203993</td>\n",
       "      <td>0.556523</td>\n",
       "      <td>0.146513</td>\n",
       "      <td>-1.128693</td>\n",
       "      <td>-0.633623</td>\n",
       "      <td>-0.454903</td>\n",
       "      <td>-0.826329</td>\n",
       "      <td>-0.826186</td>\n",
       "      <td>4.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0  136330  2.108286 -0.020359 -2.234273 -0.124080  0.559843 -1.315913   \n",
       "1  116819  2.080143 -0.075408 -1.359381  0.261263  0.232201 -0.797886   \n",
       "2   27917  1.152650  0.204938  0.441832  1.211595 -0.254895 -0.383380   \n",
       "3   97443 -0.389692  0.410148  0.616187 -0.686644  1.040312 -0.322199   \n",
       "4   17920 -1.306986  0.183306  3.164803  0.810135  1.529576  2.388322   \n",
       "\n",
       "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0  0.631887 -0.385490  0.150628  ...  0.162940  0.519705 -0.091751 -0.379542   \n",
       "1  0.233487 -0.330165  0.493154  ... -0.317765 -0.784150  0.233078 -0.636937   \n",
       "2 -0.004286 -0.019590  0.085094  ...  0.066674  0.209828 -0.054351  0.058103   \n",
       "3  0.523625 -0.037453  1.319237  ...  0.116692  0.685267 -0.182303  0.767857   \n",
       "4  0.551279 -0.054401  1.675481  ... -0.203993  0.556523  0.146513 -1.128693   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.418822  0.248646 -0.098857 -0.094773    7.70      0  \n",
       "1 -0.191137  0.234986 -0.070648 -0.062794   17.99      0  \n",
       "2  0.515330 -0.299402  0.032551  0.022865   21.00      0  \n",
       "3 -0.242738  0.009678  0.115370  0.176041    6.60      0  \n",
       "4 -0.633623 -0.454903 -0.826329 -0.826186    4.95      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "28475    0\n",
       "28476    0\n",
       "28477    0\n",
       "28478    0\n",
       "28479    0\n",
       "Name: Class, Length: 28480, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#target의 상태 확인\n",
    "target = df_t\n",
    "target #0과 1로 잘 이루어져 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    28428\n",
       "1       52\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#굉장히 편중된 데이터이기 때문에 mean accuracy가 높게 나올 수 있다\n",
    "#Why? 이렇게 TN의 값이 특히 높은 데이터의 경우 모델이 TN인 경우를 많이 학습하기 때문에 실제 데이터가 들어왔을 때 Nagative라고 예측할 확률이 커짐.\n",
    "#그렇게 되면 TN은 높지만 TP 값은 낮아진다. 그런데 Mean Acuuracy는 TN, TP 둘 다 고려하기 때문에 양이 상대적으로 많은 TN은 잘 맞춰서 값 자체는 높게 나온다.\n",
    "#따라서 이렇게 데이터가 편중된 경우 mean accuracy 이외의 다른 것들도 주의깊게 보아야 한다.\n",
    "\n",
    "target.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#스케일 하지 않은 원래 데이터를 tarin set과 test set으로 나누기 \n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], random_state = 0, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19936, 30), (8544, 30), (19936,), (8544,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#분류기 만들어 학습시키기\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습한 분류기로 test set결과 예측하기\n",
    "y_pred = classifier.predict(X_test) \n",
    "y_pred #이는 생성한 모델에 test x값을 넣었을 때 모델이 예측한 값이며 이를 추후에 실제값인 y_test와 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.98952768e-01, 1.04723177e-03],\n",
       "       [9.99723342e-01, 2.76658434e-04],\n",
       "       [9.99998763e-01, 1.23690279e-06],\n",
       "       ...,\n",
       "       [9.99978474e-01, 2.15256048e-05],\n",
       "       [9.99746678e-01, 2.53321690e-04],\n",
       "       [9.78744639e-01, 2.12553612e-02]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict_praba :: 각각 0일 확률과 1일 확률을 나타냄.\n",
    "classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9992977528089888"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean accuracy 값 :: 굉장히 높게 나온다. 하지만 이것만 보고 성능이 높다고 생각하면 아니된다.\n",
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#스케일 한 데이터를 taraining set과 test set으로 나누자.\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(df_new.iloc[:, :-1], df_new.iloc[:, -1], random_state = 0, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19936, 30), (8544, 30), (19936,), (8544,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape, X_test2.shape, y_train2.shape, y_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#스케일한 데이터의 분류기를 만듬\n",
    "classifier2 = LogisticRegression()\n",
    "classifier2.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_pred2 : 스케일한 데이터를 이용한 로지스틱 회귀모델에 test X를 투입시켜 예측한 결과 데이터\n",
    "y_pred2 = classifier2.predict(X_test2)\n",
    "y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9996488764044944"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#이 또한 Mean accuracy 값이 굉장히 크게 나온다.\n",
    "classifier2.score(X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix 찍어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8532,    0],\n",
       "       [   3,    9]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test2, y_pred2)\n",
    "# sklearn.confusion_matrix 문서 참고\n",
    "#(0,0) TN : 실제 N인데 예측도 N\n",
    "#(0,1) FP : 실제 N인데 예측을 F라고 한 False Positive\n",
    "#(1,0) FN : 실제 P인데 예측은 N이라고 한 False Natative\n",
    "#(1,1) TP : 실제 P인데 예측도 P\n",
    "\n",
    "#confusion matrix : 굉장히 편중된 값으로 보임 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8532\n",
       "1      12\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#실제 질병에 걸린 사람은 12명이다.\n",
    "y_test2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test2, y_pred2)\n",
    "#Precision : Y로 예측된 것 중 실제로도 Y인 것의 비율 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test2, y_pred2)\n",
    "#Recall : 실제로 Y인 것들 중 예측이 Y로 된 경우의 비율\n",
    "#이 데이터의 경우 실제 P지만 예측은 N으로 한 FN값이 존재(critical함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test2, y_pred2) # f1 score : Precision과 Recall의 조화평균. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99357554e-01, 6.42446199e-04],\n",
       "       [9.98609114e-01, 1.39088576e-03],\n",
       "       [9.99870035e-01, 1.29965191e-04],\n",
       "       ...,\n",
       "       [9.99128882e-01, 8.71118236e-04],\n",
       "       [9.99211309e-01, 7.88690616e-04],\n",
       "       [9.98062417e-01, 1.93758336e-03]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test2_proba = classifier2.predict_proba(X_test2)\n",
    "X_test2_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve\n",
    "#x축은 1-Specificity, y축은 Sensitivity로 구성, 그래프의 밑부분 면적이 넓을수록 좋은 모형으로 평가\n",
    "fpr, tpr, thresholds = roc_curve(y_test2, classifier2.decision_function(X_test2))\n",
    "# fpr, tpr, thresholds = roc_curve(y_test2, X_test2_proba[:,1]) #위와 같은 결과를 반환함\n",
    "\n",
    "# fpr = 1-specificity\n",
    "# tpr = sensitivity\n",
    "# thresholds = 해당 fpr, tpr을 리턴할 때의 thesholds(cut-off) 값!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.99999937e+00, 9.99999369e-01, 7.19187052e-01, 1.41874378e-01,\n",
       "       1.38271678e-01, 1.33007434e-03, 1.32829151e-03, 6.37422226e-04,\n",
       "       6.36854589e-04, 2.60804429e-04, 2.60735882e-04, 2.42986099e-10])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Sensitivity')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = fpr\n",
    "y = tpr\n",
    "\n",
    "plt.plot(x,y) # 간단하게 ROC 그려볼 수 있음.\n",
    "plt.xlabel('1-Specificity')\n",
    "plt.ylabel('Sensitivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 3.51617440e-04\n",
      " 3.51617440e-04 1.64439756e-01 1.64439756e-01 3.99554618e-01\n",
      " 3.99906235e-01 7.41795593e-01 7.41795593e-01 1.00000000e+00]\n",
      "[0.         0.08333333 0.75       0.75       0.83333333 0.83333333\n",
      " 0.91666667 0.91666667 0.91666667 0.91666667 1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9244510861072043"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#위의 roc 그래프의 면적은 다음과 같다.\n",
    "auc(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut-off :  0.5\n",
      "[[8532    0]\n",
      " [   3    9]]\n",
      "auc :  0.875\n",
      "recall :  0.75\n",
      "-----------------------------------\n",
      "cut-off :  0.4\n",
      "[[8532    0]\n",
      " [   3    9]]\n",
      "auc :  0.875\n",
      "recall :  0.75\n",
      "-----------------------------------\n",
      "cut-off :  0.3\n",
      "[[8532    0]\n",
      " [   3    9]]\n",
      "auc :  0.875\n",
      "recall :  0.75\n",
      "-----------------------------------\n",
      "cut-off :  0.2\n",
      "[[8531    1]\n",
      " [   3    9]]\n",
      "auc :  0.8749413970932959\n",
      "recall :  0.75\n",
      "-----------------------------------\n",
      "cut-off :  0.1\n",
      "[[8527    5]\n",
      " [   2   10]]\n",
      "auc :  0.9163736521331458\n",
      "recall :  0.8333333333333334\n",
      "-----------------------------------\n",
      "cut-off :  0.01\n",
      "[[8468   64]\n",
      " [   2   10]]\n",
      "auc :  0.9129160806375998\n",
      "recall :  0.8333333333333334\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#auc의 성능을 향상시키기 위해 cut off를 조정해보았다. \n",
    "for thres in [0.5, 0.4, 0.3, 0.2, 0.1, 0.01]:\n",
    "    print('cut-off : ',thres)\n",
    "    print(confusion_matrix(y_test2, X_test2_proba[:,1]>thres))\n",
    "    print('auc : ', roc_auc_score(y_test2, X_test2_proba[:,1]>thres))\n",
    "    print('recall : ', recall_score(y_test2, X_test2_proba[:,1]>thres))\n",
    "    print('-'*35)\n",
    "    \n",
    "#auc값 자체의 성능은 발전시키기 어렵다, 왜냐하면 이미 위의 roc_curve에서 thereshold가 일어나 auc값이 가장 큰 \n",
    "#cut_off를 반환한 것이기 때문에. 따라서 auc보다는 recall 값을 줄이는 데 집중해보도록 하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut-off :  0.0005\n",
      "[[4329 4203]\n",
      " [   1   11]]\n",
      "auc :  0.7120253164556961\n",
      "recall :  0.9166666666666666\n",
      "-----------------------------------\n",
      "cut-off :  0.0004\n",
      "[[3523 5009]\n",
      " [   1   11]]\n",
      "auc :  0.6647913736521331\n",
      "recall :  0.9166666666666666\n",
      "-----------------------------------\n",
      "cut-off :  0.0003\n",
      "[[2594 5938]\n",
      " [   1   11]]\n",
      "auc :  0.6103492733239568\n",
      "recall :  0.9166666666666666\n",
      "-----------------------------------\n",
      "cut-off :  0.0002\n",
      "[[1638 6894]\n",
      " [   0   12]]\n",
      "auc :  0.5959915611814346\n",
      "recall :  1.0\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#auc의 성능을 향상시키기 위해 cut off를 조정해보았다. \n",
    "for thres in [0.0005, 0.0004, 0.0003, 0.0002]:\n",
    "    print('cut-off : ',thres)\n",
    "    print(confusion_matrix(y_test2, X_test2_proba[:,1]>thres))\n",
    "    print('auc : ', roc_auc_score(y_test2, X_test2_proba[:,1]>thres))\n",
    "    print('recall : ', recall_score(y_test2, X_test2_proba[:,1]>thres))\n",
    "    print('-'*35)\n",
    "    \n",
    "#0.0002에서 비로소 recall 값이 1이 되는 것을 볼 수 있다.\n",
    "#FN의 경우 실제는 P이나 예측에서 N이라고 한 것이기 때문에 금융, 건강에 관련된 데이터라면 치명적일 수 있다.\n",
    "#실제로 병에 걸렸지만 걸리지 않았다고 하면 끔찍하다. 따라서 이러한 데이터의 경우 recall 값을 올리는 것이 auc나 mean accuracy 보다 중요할 수 있다고 생각한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
