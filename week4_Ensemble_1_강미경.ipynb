{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble**\n",
    "======"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- 여러가지 머신러닝의 조합을 통해 정확도를 향상시키자!**  \n",
    "앙상블은 Predictors의 그룹을 말함. 수천 명의 사람들로부터 얻은 집계가 전문가의 의견보다 더 나은 것 처럼, 여러 개의 Predictors(DT, SVM, Logistic...)로 부터 얻은 결과는 각 Predictors로부터 얻은 결과보다 더 정확하다.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Errors\n",
    "오차는 모델의 성능을 떨어트리는 주범! 따라서 우리는 모델에서 어떤 것이 오차를 발생시키는 지 알아야 한다.\n",
    "\n",
    "모델에서 나타나는 오차는 수학적으로 세 가지로 나누어질 수 있다.\n",
    "\n",
    "Err(x) = Bias**2 + Variacne + Irreducible Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Bias Error\n",
    "**- 예측값과 실제값이 평균적으로 어느 정도 차이가 나는가?**  \n",
    "따라서 높은 Bias Error을 가지는 모델은 낮은 성능의 모델을 의미함.\n",
    "높은 Bias Error는 Underfitting을 초래.\n",
    "\n",
    "### 1-2. Variance\n",
    "**- 동일한 관찰에 대한 예측이 서로 어떻게 다른가?**  \n",
    "높은 분산은 Overfitting을 초래함. 훈련용 데이터에는 맞지만 실제 데이터와는 상당한 차이가 있을 수 있기 때문에. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Basic Ensemble Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. Max Voting\n",
    "모델들이 각각의 데이터에 대해 예측한 결과값들을 취합하여 대다수의 모델들이 투표한 결과를 마지막 예측값으로 사용하는 방법. (다수결)\n",
    "\n",
    "### Majority Voting (Hard voting / Soft voting)\n",
    "- Hard voting : Majority Voting의 가장 간단한 케이스로, 각 분류기의 majority voting(다수결)을 통해 최종 예측값을 결정. 예를 들어, 데이터에 대해 세 개의 분류기 중 2개는 class 0으로 분류하였고 나머지 하나는 class 1로 분류하였다면, 그 데이터는 결과적으로 class 0으로 분류되는 것. \n",
    "\n",
    "- Soft voting : Scikit Learn의 대부분의 분류기는 클래스의 확률을 추정할 수 있는 메소드가 있음. (Predict_proba()). 이를 바탕으로 최종 클래스를 예측하는 것이 Soft voting이다. 옵션에서 Voting = soft 값으로 바꾸어줌으로써 가능하다.\n",
    "\n",
    "Hard Voting은 예측값 자체를 이용하는 반면, Soft voting은 해당 클래스일 확률값의 평균을 이용한다. Soft voting은 더 명확하게 분류된 클래스에 대해 더 많은 가중치를 주기 때문에 대부분의 상황에서 Hard voting보다 더 높은 성능을 보인다. \n",
    "\n",
    "SVM : 대부분의 모델은 알아서 모델 fit 시에 predict_proba를 정의하지만 SVM는 예외로 관련된 하이퍼 파라미터 값을 True로 주어야지만 이 값을 계산함.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. Averaging\n",
    "Max Voting과 비슷하나 각 예측한 class에 대해 평균을 내어 최종 결정.\n",
    "주로 회귀 문제에서 예측을 만드는 데 사용되거나 분류기 문제에서 확률을 계산할 때 사용된다.\n",
    "ex) 한 데이터에 대해 분류기 5개에서 순서대로 5 4 5 4 4 라고 예측했다면   \n",
    "(5 + 4 + 5 + 4 + 4) / 5 = 4.4  \n",
    "에 따라서 최종 예측은 4가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3. Weighted Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaging의 연장선으로, 모든 모델들은 예측을 위해 각 모델 중요도로 정의되는 다른 가중치를 할당받는다. 예를 들어 어떠한 의사결정을 내리는 데 이 분야의 전문가 둘이 있고 다른 사람들은 일반인이라면, 우리는 전문가의 의견에 더욱 가중치를 두어 결정을 내리게 된다. 이것과 똑같은 맥락이다. 더 중요하다고, 더 정확하다고 판단하는 모델의 예측에 더 높은 가중치를 두어 최종 예측값을 결정하는 방식이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Advanced Ensemble Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. Bagging\n",
    "배깅을 이용하면 성능은 매우 좋아지나 추가적인 업무가 많이 늘어나는 단점이 있고, 사용하는 데이터가 배깅을 사용하는 데 적합하지 않으면 오히려 골칫거리가 될 수 있음.\n",
    "\n",
    "### Stability and Accuracy\n",
    "Bagging은 Bootstrapping의 통계적인 방법에 기반을 둠. Bootstrap Aggregators과 같은 말.\n",
    "\n",
    "### Step\n",
    "1. 복원 추출을 통한 Subset을 생성한다. 이는 서로 다른 Subset들 안에 모든 관측치들이 포함되어 있을 것을 암시한다.\n",
    "2. 모든 Sample Subset에 대해 각각 모델을 적용시킨다.\n",
    "3. 그 모델을 병렬로 동작하며 서로 독립이다.\n",
    "4. 각 모델을 사용하여 n개의 X_test를 예측한다.\n",
    "5. 마지막으로 그 예측값들을 집계하여 최종적인 예측을 형성한다. (이때 Majority Voting 혹은 Averaging을 사용함)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Algorithms\n",
    "1. Bagging meta-estimator\n",
    "앙상블 알고리즘으로, 분류기와 회귀문제 둘 다에서 사용 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging 사용 예시 \n",
    "final_dt = DecisionTreeClassifier(max_leaf_nodes=10, max_depth=5)\n",
    "final_bc = BaggingClassifier(base_estimator=final_dt, n_estimators=40,, random_state=1, oob_score=True)\n",
    "fnnal_bc.fit(X_train, train_y)\n",
    "final_preds = final_bc.predict(X_test)\n",
    "acc_oob = final_bc.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "1. Base_estimator : Base estimator을 정의. 예시에서는 Decision Tree\n",
    "2. N_estimators : 만들어진 estimators의 숫자. 이는 반드시 신중하게 튜닝되어야 함. 큰 숫자는 매우 훈련 시간을 불러올 수 있고, 반면에 너무 작은 숫자는 Best Result를 제공하지 못함.\n",
    "3. Max_samples : Subset의 Size를 제어함. 각각의 Base estimator을 트레이닝 시키기 위한 Sample들의 최대 숫자.\n",
    "4. Max_features : 각각의 estimator을 훈련시키기 위해 요구되는 피처의 최대 숫자를 정의함.\n",
    "5. N_jobs : 병렬로 동작하기 위한 job의 숫자를 설정. 이 값을 나의 cores와 같게 설정해야 함. 만약 -1로 정의하였다면, cores의 숫자로 자동 설정 됨.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. Random Forest\n",
    "Bagging Techniques를 사용하는 알고리즘을 학습하는 앙상블 머신으로 Base estimator은 결정트리이다.   \n",
    "Bagging meta-estimator의 연장선이지만 Data set의 Feature을 랜덤으로 선정한다는 점에서 차이가 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step\n",
    "1. Original Data set으로부터 Bootstrapping을 통해 Random subsets이 만들어진다.\n",
    "2. 결정트리의 각 노드에서는 Best Split 시 오직 한 가지 feature set만이 고려된다. \n",
    "3. 각각의 subset들에 대해 결정트리가 적합되어 진다. 최종 예측은 모든 결정트리 각각의 예측들의 평균을 계산함으로써 이루어진다.\n",
    "\n",
    "[요약] Random Forest는 랜덤으로 데이터 포인트와 Features를 선택하고 이를 바탕으로 여러 개의 Tree를 설계한다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances\n",
    "model.featureimportances를 사용함으로써 각 Feature에 대한 중요도를 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest 사용 예시\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model= RandomForestRegressor()\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "1. N_estimators : Random Forest에서 만들어지는 결정트리의 숫자를 정의함. 일반적으로 높은 숫자는 예측을 더 강하고 안정적이게 만드나, 학습시간도 길게 만든다.  \n",
    "2. Criterion : Split 할 때 사용될 수 있는 함수를 정의함. 그 함수는 각 feature에 대한 split의 quality를 측정하고 best split을 선택한다. (라고는 하는데 사실 뭔 소린지 잘 모르겠어요 ㅠ)  \n",
    "3. Max_features : Split에 사용되는 feature의 최대 숫자를 제한함. 결정트리에서 max feature를 증가시키는 것은 일반적으로 성능을 향상시키지만 너무 큰 숫자는 각 트리의 다양성을 감소시킬 수 있음.  \n",
    "4. Max_depth : 랜덤 포레스트는 여러 개의 결정트리를 사용하는데, 이 파라미터는 그 트리들의 최대 깊이를 제한함.  \n",
    "5. Min_samples_split : leaf_node 안에서 split을 시도하기 전에 요구되는 샘플의 최소 숫자를 정의하는 데 사용됨. 만약 그 샘플의 숫자가 요구된 숫자보다 적다면, 그 노드는 스플릿되지 않음.  \n",
    "6. Max_leaf_nodes : 각 트리를 위한 리프 노드의 최대 숫자를 지정함. 트리는 max_leaf_node의 숫자와 리프노드의 숫자가 같게 되었을 때 멈춘다.  \n",
    "7. N_jobs : 병렬처리를 위한 jobs의 숫자를 나타냄 (위에 더 자세히 설명되어 있음)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3. Boosting\n",
    "부스팅은 앙상블 기법으로, 약한 학습기를 순차적으로 학습시켜 강한 학습기로 만드는 데 목표를 둔다. 이는 팀워크에 대한 개념으로 현재 실행중인 모델은 다음 모델에게 더 초점을 맞추어 분류할 feature을 지시한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "가장 간단한 부스팅 알고리즘 중 하나로, 보통 모델링으로 결정트리가 사용됨. 여러 가지 순차적인 모델들이 만들어지고 각각 가장 최근의 모델로부터 생성된 에러를 고친다. AdaBoost는 각 잘못 분류된 관측치들에게 가중치를 할당한다. 그리고 그 다음 하부모델은 이를 바탕으로 이 값들을 올바르게 예측하기 위해 동작한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step \n",
    "1. 데이터셋의 모든 관측치들은 같은 가중치로 주어진다.\n",
    "2. 데이터의 하부집합에서 한 모델이 만들어지고 이를 바탕으로 예측값이 생성된다.\n",
    "3. 오차(Error)는 그 예측값과 실제값들을 비교함으로써 계산된다.\n",
    "4. 오차를 바탕으로 다음 모델이 생성되고 동작할 때 맞지 않게 예측된 데이터 포인트에게 더 높은 가중치가 주어진다.\n",
    "5. 더 많은 에러는 더 많은 가중치를 관측치에 할당된다.\n",
    "6. Error Function이 바뀌지 않을 때 까지 이 프로세스는 반복된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaBoost 예시코드\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model = AdaBoostClassifier(random_state=1)\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "1. base_estimators : base learner로 사용될 base_estimators를 명시\n",
    "2. N_estimators : base estimators의 숫자를 명시함. default는 10\n",
    "3. Learning_rate : 마지막 Combination에서 estimator의 기여도를 조절함.\n",
    "4. Max_depth : 각 estimators의 최대 깊이를 정의함. 성능을 위해 적절히 튜닝하여야 하는 파라미터.\n",
    "5. N_jobs : 병렬 처리를 위해 사용할 프로세서의 숫자를 정의함.\n",
    "6. Random_state : Random data split을 고정시킬 정수를 명시함.이를 지정하면 같은 데이터에 같은 파라미터가 주어진다면 항상 같은 결과가 나올 것임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stacking\n",
    "각 모델의 예측값(class 자체의 분류 혹은 확률값, 즉 output)을 다음 모델의 input value로 이용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
