{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 반갑습니다 13기 여러분\n",
    "\n",
    "과제를 진행해 볼게요\n",
    "\n",
    "혹시라도 도저히 모르겠거나 해결이 안되신다면 01040493041로 전화주시거나 카톡주세요!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ''' ? ''' 이 있는 부분을 채워주시면 됩니다\n",
    "\n",
    "나는 내 스타일로 하겠다 하시면 그냥 구현 하셔도 됩니다!!\n",
    "\n",
    "참고하셔야 하는 함수들은 링크 달아드렸으니 들어가서 확인해보세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) PCA의 과정을 한번 차근차근 밟아 볼거에요 잘 따라 오세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lin\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "#   기본 모듈들을 불러와 줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [95, 91, 66, 94, 68, 63, 12, 73, 93, 51, 13, 70, 63, 63, 97, 56, 67, 96, 75, 6]\n",
    "x2 = [56, 27, 25, 1, 9, 80, 92, 69, 6, 25, 83, 82, 54, 97, 66, 93, 76, 59, 94, 9]\n",
    "x3 = [57, 34, 9, 79, 4, 77, 100, 42, 6, 96, 61, 66, 9, 25, 84, 46, 16, 63, 53, 30]\n",
    "\n",
    "#   설명변수 x1, x2, x3의 값이 이렇게 있네요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack((x1,x2,x3),axis=0)\n",
    "\n",
    "#   설명변수들을 하나의 행렬로 만들어 줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X.T,columns=['x1','x2','x3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95</td>\n",
       "      <td>56</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91</td>\n",
       "      <td>27</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "      <td>92</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>93</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>51</td>\n",
       "      <td>25</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>83</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>70</td>\n",
       "      <td>82</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>63</td>\n",
       "      <td>54</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>63</td>\n",
       "      <td>97</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>97</td>\n",
       "      <td>66</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>56</td>\n",
       "      <td>93</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>67</td>\n",
       "      <td>76</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>96</td>\n",
       "      <td>59</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>75</td>\n",
       "      <td>94</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x1  x2   x3\n",
       "0   95  56   57\n",
       "1   91  27   34\n",
       "2   66  25    9\n",
       "3   94   1   79\n",
       "4   68   9    4\n",
       "5   63  80   77\n",
       "6   12  92  100\n",
       "7   73  69   42\n",
       "8   93   6    6\n",
       "9   51  25   96\n",
       "10  13  83   61\n",
       "11  70  82   66\n",
       "12  63  54    9\n",
       "13  63  97   25\n",
       "14  97  66   84\n",
       "15  56  93   46\n",
       "16  67  76   16\n",
       "17  96  59   63\n",
       "18  75  94   53\n",
       "19   6   9   30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-1) 먼저 PCA를 시작하기 전에 항상!!!!!! 데이터를 scaling 해주어야 해요\n",
    "\n",
    "https://datascienceschool.net/view-notebook/f43be7d6515b48c0beb909826993c856/ 를 참고하시면 도움이 될거에요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.08573604,  0.02614175,  0.30684189],\n",
       "       [ 0.93801686, -0.86575334, -0.46445467],\n",
       "       [ 0.01477192, -0.92726334, -1.30282049],\n",
       "       [ 1.04880625, -1.66538341,  1.04460382],\n",
       "       [ 0.08863151, -1.41934339, -1.47049366],\n",
       "       [-0.09601747,  0.76426183,  0.97753455],\n",
       "       [-1.97943714,  1.13332186,  1.74883111],\n",
       "       [ 0.2732805 ,  0.42595679, -0.1961776 ],\n",
       "       [ 1.01187645, -1.5116084 , -1.40342439],\n",
       "       [-0.53917504, -0.92726334,  1.61469258],\n",
       "       [-1.94250735,  0.85652683,  0.44098042],\n",
       "       [ 0.16249111,  0.82577183,  0.60865359],\n",
       "       [-0.09601747, -0.03536825, -1.30282049],\n",
       "       [-0.09601747,  1.28709688, -0.76626636],\n",
       "       [ 1.15959564,  0.33369178,  1.21227698],\n",
       "       [-0.35452606,  1.16407687, -0.06203907],\n",
       "       [ 0.05170172,  0.64124181, -1.06807806],\n",
       "       [ 1.12266584,  0.11840676,  0.50804969],\n",
       "       [ 0.3471401 ,  1.19483187,  0.17270336],\n",
       "       [-2.20101593, -1.41934339, -0.5985932 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_std.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.08573604,  0.93801686,  0.01477192,  1.04880625,  0.08863151,\n",
       "        -0.09601747, -1.97943714,  0.2732805 ,  1.01187645, -0.53917504,\n",
       "        -1.94250735,  0.16249111, -0.09601747, -0.09601747,  1.15959564,\n",
       "        -0.35452606,  0.05170172,  1.12266584,  0.3471401 , -2.20101593],\n",
       "       [ 0.02614175, -0.86575334, -0.92726334, -1.66538341, -1.41934339,\n",
       "         0.76426183,  1.13332186,  0.42595679, -1.5116084 , -0.92726334,\n",
       "         0.85652683,  0.82577183, -0.03536825,  1.28709688,  0.33369178,\n",
       "         1.16407687,  0.64124181,  0.11840676,  1.19483187, -1.41934339],\n",
       "       [ 0.30684189, -0.46445467, -1.30282049,  1.04460382, -1.47049366,\n",
       "         0.97753455,  1.74883111, -0.1961776 , -1.40342439,  1.61469258,\n",
       "         0.44098042,  0.60865359, -1.30282049, -0.76626636,  1.21227698,\n",
       "        -0.06203907, -1.06807806,  0.50804969,  0.17270336, -0.5985932 ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-2) 자 그럼 공분산 행렬을 구해볼게요\\\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html 를 참고하시면 도움이 될거에요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cov_matrix = np.cov(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.05263158, -0.2037104 , -0.12079228],\n",
       "       [-0.2037104 ,  1.05263158,  0.3125801 ],\n",
       "       [-0.12079228,  0.3125801 ,  1.05263158]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-3) 이제 고유값과 고유벡터를 구해볼게요\n",
    "\n",
    "방법은 실습코드에 있어요!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = lin.eig(cov_matrix)[0]\n",
    "eigenvectors = lin.eig(cov_matrix)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.48756162 0.94435407 0.72597904]\n",
      "[[ 0.47018528 -0.85137353 -0.23257022]\n",
      " [-0.64960236 -0.15545725 -0.74421087]\n",
      " [-0.59744671 -0.50099516  0.62614797]]\n"
     ]
    }
   ],
   "source": [
    "print(eigenvalues)\n",
    "print(eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.zeros((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat[0][0] = eigenvalues[0]\n",
    "mat[1][1] = eigenvalues[1]\n",
    "mat[2][2] = eigenvalues[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.48756162, 0.        , 0.        ],\n",
       "       [0.        , 0.94435407, 0.        ],\n",
       "       [0.        , 0.        , 0.72597904]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-4) 자 이제 고유값 분해를 할 모든 준비가 되었어요 고유값 분해의 곱으로 원래 공분산 행렬을 구해보세요\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html 를 참고해서 행렬 끼리 곱하시면 됩니다\n",
    "\n",
    "행렬 곱으로 eigenvector x mat x eigenvector.T 하면 될거에요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.05263158, -0.2037104 , -0.12079228],\n",
       "       [-0.2037104 ,  1.05263158,  0.3125801 ],\n",
       "       [-0.12079228,  0.3125801 ,  1.05263158]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(np.dot(eigenvectors,mat),eigenvectors.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-5) 마지막으로 고유 벡터 축으로 값을 변환해 볼게요\n",
    "\n",
    "함수로 한번 정의해 보았어요\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvectors.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.47018528, -0.64960236, -0.59744671])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvectors.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.31019368,  1.28092404,  1.38766381,  0.95087515,  1.84222365,\n",
       "       -1.12563709, -2.71174416, -0.03100441,  2.29618509, -0.61585248,\n",
       "       -1.73320252, -0.82366049,  0.75619512, -0.42344386, -0.39581307,\n",
       "       -0.88581498,  0.24587691,  0.14741103, -0.7161265 ,  0.24475107])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_std.dot(eigenvectors.T[0])\n",
    "# X_std n * 3 // eigenvectors 3*3 결과는 n * 3\n",
    "# 첫 번째 고유벡터 방향으로 투영한 값 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_coordinates(X,eigenvectors):\n",
    "    for i in range(eigenvectors.shape[0]):\n",
    "        if i == 0:\n",
    "            new = [X.dot(eigenvectors.T[i])]\n",
    "        else:\n",
    "            new = np.concatenate((new,[X.dot(eigenvectors.T[i])]),axis=0)\n",
    "    return new.T\n",
    "\n",
    "# 모든 고유 벡터 축으로 데이터를 projection한 값입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.31019368, -1.08215716, -0.07983642],\n",
       "       [ 1.28092404, -0.43132556,  0.13533091],\n",
       "       [ 1.38766381,  0.78428014, -0.12911446],\n",
       "       [ 0.95087515, -1.15737142,  1.6495519 ],\n",
       "       [ 1.84222365,  0.88189889,  0.11493111],\n",
       "       [-1.12563709, -0.52680338,  0.06564012],\n",
       "       [-2.71174416,  0.63290138,  0.71195473],\n",
       "       [-0.03100441, -0.20059783, -0.50339479],\n",
       "       [ 2.29618509,  0.07661447,  0.01087174],\n",
       "       [-0.61585248, -0.205764  ,  1.82651199],\n",
       "       [-1.73320252,  1.29971699,  0.09045178],\n",
       "       [-0.82366049, -0.57164535, -0.27123176],\n",
       "       [ 0.75619512,  0.73995175, -0.76710616],\n",
       "       [-0.42344386,  0.26555394, -1.41533681],\n",
       "       [-0.39581307, -1.64646874,  0.24104031],\n",
       "       [-0.88581498,  0.15195119, -0.82271209],\n",
       "       [ 0.24587691,  0.39139878, -1.15801831],\n",
       "       [ 0.14741103, -1.22874561, -0.03110396],\n",
       "       [-0.7161265 , -0.56781471, -0.86180345],\n",
       "       [ 0.24475107,  2.39442622,  1.19337361]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_coordinates(X_std,eigenvectors)\n",
    "# 새로운 축으로 변환되어 나타난 데이터들입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) PCA를 구현해 보세요\n",
    "\n",
    "위의 과정을 이해하셨다면 충분히 하실 수 있을거에요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def MYPCA(X,number):\n",
    "    scaler = StandardScaler()\n",
    "    x_std = scaler.fit(X).transform(X)\n",
    "    features = x_std.T\n",
    "    cov_matrix = np.cov(features)\n",
    "    \n",
    "    eigenvalues = lin.eig(cov_matrix)[0]\n",
    "    eigenvectors = lin.eig(cov_matrix)[1]\n",
    "    \n",
    "    new_coordinates(x_std,eigenvectors)\n",
    "    \n",
    "    new_coordinate = new_coordinates(x_std,eigenvectors)\n",
    "    \n",
    "    index = eigenvalues.argsort()\n",
    "    index = list(index)\n",
    "    \n",
    "    for i in range(number):\n",
    "        if i==0:\n",
    "            new = [new_coordinate[:,index.index(i)]]\n",
    "        else:\n",
    "            new = np.concatenate(([new_coordinate[:,index.index(i)]],new),axis=0)\n",
    "    return new.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.31019368, -1.08215716, -0.07983642],\n",
       "       [ 1.28092404, -0.43132556,  0.13533091],\n",
       "       [ 1.38766381,  0.78428014, -0.12911446],\n",
       "       [ 0.95087515, -1.15737142,  1.6495519 ],\n",
       "       [ 1.84222365,  0.88189889,  0.11493111],\n",
       "       [-1.12563709, -0.52680338,  0.06564012],\n",
       "       [-2.71174416,  0.63290138,  0.71195473],\n",
       "       [-0.03100441, -0.20059783, -0.50339479],\n",
       "       [ 2.29618509,  0.07661447,  0.01087174],\n",
       "       [-0.61585248, -0.205764  ,  1.82651199],\n",
       "       [-1.73320252,  1.29971699,  0.09045178],\n",
       "       [-0.82366049, -0.57164535, -0.27123176],\n",
       "       [ 0.75619512,  0.73995175, -0.76710616],\n",
       "       [-0.42344386,  0.26555394, -1.41533681],\n",
       "       [-0.39581307, -1.64646874,  0.24104031],\n",
       "       [-0.88581498,  0.15195119, -0.82271209],\n",
       "       [ 0.24587691,  0.39139878, -1.15801831],\n",
       "       [ 0.14741103, -1.22874561, -0.03110396],\n",
       "       [-0.7161265 , -0.56781471, -0.86180345],\n",
       "       [ 0.24475107,  2.39442622,  1.19337361]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MYPCA(X,3)\n",
    "# 새로운 축으로 잘 변환되어서 나타나나요?\n",
    "# 위에서 했던 PCA랑은 차이가 있을 수 있어요 왜냐하면 위에서는 고유값이 큰 축 순서로 정렬을 안했었거든요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) sklearn이랑 비교를 해볼까요?\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html 를 참고하시면 도움이 될거에요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47106118, 0.29904546, 0.22989336])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_\n",
    "#제 1주성분이 설명하는 분산,... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.3163588 , 4.23588566, 3.71397385])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.singular_values_\n",
    "#이건 뭘까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99066774,  0.17323696,  0.06223886],\n",
       "       [ 0.17323696,  1.072166  , -0.29850147],\n",
       "       [ 0.06223886, -0.29850147,  1.04578241]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.get_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.05263158, -0.2037104 , -0.12079228],\n",
       "       [-0.2037104 ,  1.05263158,  0.3125801 ],\n",
       "       [-0.12079228,  0.3125801 ,  1.05263158]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.get_covariance()\n",
    "#이건 사용된 독립변수들의 공분산 행렬?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.31019368, -1.08215716, -0.07983642],\n",
       "       [-1.28092404, -0.43132556,  0.13533091],\n",
       "       [-1.38766381,  0.78428014, -0.12911446],\n",
       "       [-0.95087515, -1.15737142,  1.6495519 ],\n",
       "       [-1.84222365,  0.88189889,  0.11493111],\n",
       "       [ 1.12563709, -0.52680338,  0.06564012],\n",
       "       [ 2.71174416,  0.63290138,  0.71195473],\n",
       "       [ 0.03100441, -0.20059783, -0.50339479],\n",
       "       [-2.29618509,  0.07661447,  0.01087174],\n",
       "       [ 0.61585248, -0.205764  ,  1.82651199],\n",
       "       [ 1.73320252,  1.29971699,  0.09045178],\n",
       "       [ 0.82366049, -0.57164535, -0.27123176],\n",
       "       [-0.75619512,  0.73995175, -0.76710616],\n",
       "       [ 0.42344386,  0.26555394, -1.41533681],\n",
       "       [ 0.39581307, -1.64646874,  0.24104031],\n",
       "       [ 0.88581498,  0.15195119, -0.82271209],\n",
       "       [-0.24587691,  0.39139878, -1.15801831],\n",
       "       [-0.14741103, -1.22874561, -0.03110396],\n",
       "       [ 0.7161265 , -0.56781471, -0.86180345],\n",
       "       [-0.24475107,  2.39442622,  1.19337361]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit_transform(X_std)\n",
    "#transform 까지 적어주어야 변환을 하고 그 결과를 출력해줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.31019368, -1.08215716, -0.07983642],\n",
       "       [ 1.28092404, -0.43132556,  0.13533091],\n",
       "       [ 1.38766381,  0.78428014, -0.12911446],\n",
       "       [ 0.95087515, -1.15737142,  1.6495519 ],\n",
       "       [ 1.84222365,  0.88189889,  0.11493111],\n",
       "       [-1.12563709, -0.52680338,  0.06564012],\n",
       "       [-2.71174416,  0.63290138,  0.71195473],\n",
       "       [-0.03100441, -0.20059783, -0.50339479],\n",
       "       [ 2.29618509,  0.07661447,  0.01087174],\n",
       "       [-0.61585248, -0.205764  ,  1.82651199],\n",
       "       [-1.73320252,  1.29971699,  0.09045178],\n",
       "       [-0.82366049, -0.57164535, -0.27123176],\n",
       "       [ 0.75619512,  0.73995175, -0.76710616],\n",
       "       [-0.42344386,  0.26555394, -1.41533681],\n",
       "       [-0.39581307, -1.64646874,  0.24104031],\n",
       "       [-0.88581498,  0.15195119, -0.82271209],\n",
       "       [ 0.24587691,  0.39139878, -1.15801831],\n",
       "       [ 0.14741103, -1.22874561, -0.03110396],\n",
       "       [-0.7161265 , -0.56781471, -0.86180345],\n",
       "       [ 0.24475107,  2.39442622,  1.19337361]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MYPCA(X,3)\n",
    "#pca클래스를 사용한 결과와 똑같다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) MNIST data에 적용을 해볼게요!\n",
    "\n",
    "mnist data를 따로 내려받지 않게 압축파일에 같이 두었어요~!!!\n",
    "\n",
    "mnist-original.mat 파일과 같은 위치에서 주피터 노트북을 열어주세요~!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lin\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from scipy import io\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# mnist 손글씨 데이터를 불러옵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = io.loadmat('mnist-original.mat') \n",
    "X = mnist['data'].T\n",
    "y = mnist['label'].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data information\n",
    "\n",
    "# 7만개의 작은 숫자 이미지\n",
    "# 행 열이 반대로 되어있음 -> 전치\n",
    "# grayscale 28x28 pixel = 784 feature\n",
    "# 각 picel은 0~255의 값\n",
    "# label = 1~10 label이 총 10개인거에 주목하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data를 각 픽셀에 이름붙여 표현\n",
    "feat_cols = [ 'pixel'+str(i) for i in range(X.shape[1]) ]\n",
    "df = pd.DataFrame(X,columns=feat_cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df에 라벨 y를 붙여서 데이터프레임 생성\n",
    "df['y'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0           0       0       0       0       0       0       0       0       0   \n",
       "1           0       0       0       0       0       0       0       0       0   \n",
       "2           0       0       0       0       0       0       0       0       0   \n",
       "3           0       0       0       0       0       0       0       0       0   \n",
       "4           0       0       0       0       0       0       0       0       0   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "69995       0       0       0       0       0       0       0       0       0   \n",
       "69996       0       0       0       0       0       0       0       0       0   \n",
       "69997       0       0       0       0       0       0       0       0       0   \n",
       "69998       0       0       0       0       0       0       0       0       0   \n",
       "69999       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0           0  ...         0         0         0         0         0   \n",
       "1           0  ...         0         0         0         0         0   \n",
       "2           0  ...         0         0         0         0         0   \n",
       "3           0  ...         0         0         0         0         0   \n",
       "4           0  ...         0         0         0         0         0   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "69995       0  ...         0         0         0         0         0   \n",
       "69996       0  ...         0         0         0         0         0   \n",
       "69997       0  ...         0         0         0         0         0   \n",
       "69998       0  ...         0         0         0         0         0   \n",
       "69999       0  ...         0         0         0         0         0   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783    y  \n",
       "0             0         0         0         0  0.0  \n",
       "1             0         0         0         0  0.0  \n",
       "2             0         0         0         0  0.0  \n",
       "3             0         0         0         0  0.0  \n",
       "4             0         0         0         0  0.0  \n",
       "...         ...       ...       ...       ...  ...  \n",
       "69995         0         0         0         0  9.0  \n",
       "69996         0         0         0         0  9.0  \n",
       "69997         0         0         0         0  9.0  \n",
       "69998         0         0         0         0  9.0  \n",
       "69999         0         0         0         0  9.0  \n",
       "\n",
       "[70000 rows x 785 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [9.],\n",
       "       [9.],\n",
       "       [9.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 지금까지 배운 여러 머신러닝 기법들이 있을거에요\n",
    "\n",
    "4-1) train_test_split을 통해 데이터를 0.8 0.2의 비율로 분할 해 주시고요\n",
    "\n",
    "4-2) PCA를 이용하여 mnist data를 축소해서 학습을 해주세요 / test error가 제일 작으신 분께 상품을 드리겠습니다 ^0^\n",
    "\n",
    "특정한 틀 없이 자유롭게 하시면 됩니다!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca =  PCA() #n_components : 주성분 개수\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. 몇 개의 주성분을 선택할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Scree plot으로 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 784)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sing_vals = range(pca.n_components_)\n",
    "sing_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2680003ab08>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sing_vals, eigvals, 'ro-', linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x268000fb088>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#너무 많아서 보기가 힘드니 100개로 줄여서 보기로 하였다.\n",
    "pca =  PCA(n_components = 100) #n_components : 주성분 개수\n",
    "pca.fit(X_train)\n",
    "sing_vals = range(pca.n_components_)\n",
    "eigvals = pca.explained_variance_ratio_\n",
    "plt.plot(sing_vals, eigvals, 'ro-', linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스크리 플롯으로 보았을 때 20 ~ 40에 Elbow Point가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 누적설명률로 골라보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6447511266753982"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스크리 플롯을 바탕으로, 20에서 끊어내면 총 얼만큼의 변동을 설명할 수 있을까?\n",
    "pca =  PCA(n_components = 20) #n_components : 주성분 개수\n",
    "pca.fit(X_train)\n",
    "eigvals = pca.explained_variance_ratio_\n",
    "sum(eigvals)\n",
    "#설명률이 64%밖에 안된다. 주성분 개수를 좀 더 늘려봐야겠다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7316294365142527"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#30에서 끊어내면 총 얼만큼의 변동을 설명할 수 있을까?\n",
    "pca =  PCA(n_components = 30)\n",
    "pca.fit(X_train)\n",
    "eigvals = pca.explained_variance_ratio_\n",
    "sum(eigvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7870419026728016"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#40에서 끊어내면 총 얼만큼의 변동을 설명할 수 있을까?\n",
    "pca =  PCA(n_components = 40)\n",
    "pca.fit(X_train)\n",
    "eigvals = pca.explained_variance_ratio_\n",
    "sum(eigvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===> 주성분 개수 30 ~ 40개 일때 전체 데이터 분산의 70~80%를 설명한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Kaiser's Rule (고유값 1이상의 주성분은 몇 개인지 알아보자)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca =  PCA()\n",
    "pca.fit(X_train)\n",
    "eigvals = pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(eigvals >= 1)\n",
    "# 1이상인 것들은 엄청 많다. 650개나 있다. \n",
    "# 태한이오빠에게 물어보니 이 데이터는 원래 그렇다고 한다. \n",
    "# 그러니 카이사 룰보다는 위에서 구한 것들을 바탕으로 주성분을 선택하자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Modeling\n",
    "위의 결과를 바탕으로 주성분 개수는 30개일 때 40개일 때 두 개로 나누어 분석을 실시합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. Component 30-cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Com1</th>\n",
       "      <th>Com2</th>\n",
       "      <th>Com3</th>\n",
       "      <th>Com4</th>\n",
       "      <th>Com5</th>\n",
       "      <th>Com6</th>\n",
       "      <th>Com7</th>\n",
       "      <th>Com8</th>\n",
       "      <th>Com9</th>\n",
       "      <th>Com10</th>\n",
       "      <th>...</th>\n",
       "      <th>Com21</th>\n",
       "      <th>Com22</th>\n",
       "      <th>Com23</th>\n",
       "      <th>Com24</th>\n",
       "      <th>Com25</th>\n",
       "      <th>Com26</th>\n",
       "      <th>Com27</th>\n",
       "      <th>Com28</th>\n",
       "      <th>Com29</th>\n",
       "      <th>Com30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-241.401991</td>\n",
       "      <td>273.953104</td>\n",
       "      <td>868.692070</td>\n",
       "      <td>-481.131177</td>\n",
       "      <td>-460.497659</td>\n",
       "      <td>64.873244</td>\n",
       "      <td>109.783332</td>\n",
       "      <td>135.444373</td>\n",
       "      <td>432.628999</td>\n",
       "      <td>-9.933314</td>\n",
       "      <td>...</td>\n",
       "      <td>-44.652165</td>\n",
       "      <td>-220.051528</td>\n",
       "      <td>265.052727</td>\n",
       "      <td>-106.590544</td>\n",
       "      <td>70.004746</td>\n",
       "      <td>119.752735</td>\n",
       "      <td>41.073423</td>\n",
       "      <td>21.025040</td>\n",
       "      <td>-32.711384</td>\n",
       "      <td>305.591231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-330.370704</td>\n",
       "      <td>-612.732093</td>\n",
       "      <td>386.073937</td>\n",
       "      <td>-231.343692</td>\n",
       "      <td>-185.075654</td>\n",
       "      <td>487.317855</td>\n",
       "      <td>280.298025</td>\n",
       "      <td>48.270838</td>\n",
       "      <td>-20.797023</td>\n",
       "      <td>-480.176604</td>\n",
       "      <td>...</td>\n",
       "      <td>178.700099</td>\n",
       "      <td>-145.303165</td>\n",
       "      <td>130.830925</td>\n",
       "      <td>18.605387</td>\n",
       "      <td>66.879865</td>\n",
       "      <td>155.471674</td>\n",
       "      <td>-171.076035</td>\n",
       "      <td>-275.288421</td>\n",
       "      <td>-148.406533</td>\n",
       "      <td>64.471091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-187.022039</td>\n",
       "      <td>-745.558162</td>\n",
       "      <td>291.990794</td>\n",
       "      <td>-300.326123</td>\n",
       "      <td>-261.682733</td>\n",
       "      <td>-135.614707</td>\n",
       "      <td>65.893117</td>\n",
       "      <td>52.328778</td>\n",
       "      <td>320.062774</td>\n",
       "      <td>-589.928552</td>\n",
       "      <td>...</td>\n",
       "      <td>316.437974</td>\n",
       "      <td>24.766145</td>\n",
       "      <td>-329.590719</td>\n",
       "      <td>-92.764300</td>\n",
       "      <td>-51.168476</td>\n",
       "      <td>-242.706402</td>\n",
       "      <td>177.934294</td>\n",
       "      <td>-101.492945</td>\n",
       "      <td>144.564830</td>\n",
       "      <td>-209.425290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>666.090140</td>\n",
       "      <td>-645.507034</td>\n",
       "      <td>-175.316849</td>\n",
       "      <td>532.903514</td>\n",
       "      <td>186.392958</td>\n",
       "      <td>160.034157</td>\n",
       "      <td>-702.722187</td>\n",
       "      <td>0.622236</td>\n",
       "      <td>913.299807</td>\n",
       "      <td>-708.372067</td>\n",
       "      <td>...</td>\n",
       "      <td>-416.498576</td>\n",
       "      <td>282.060869</td>\n",
       "      <td>-52.709489</td>\n",
       "      <td>-392.488034</td>\n",
       "      <td>287.126675</td>\n",
       "      <td>-250.002248</td>\n",
       "      <td>-17.483804</td>\n",
       "      <td>60.595239</td>\n",
       "      <td>413.774061</td>\n",
       "      <td>-80.513751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-396.710534</td>\n",
       "      <td>-425.506796</td>\n",
       "      <td>-244.191298</td>\n",
       "      <td>-483.954170</td>\n",
       "      <td>212.053375</td>\n",
       "      <td>515.308769</td>\n",
       "      <td>803.904983</td>\n",
       "      <td>219.957663</td>\n",
       "      <td>-409.401774</td>\n",
       "      <td>-174.889158</td>\n",
       "      <td>...</td>\n",
       "      <td>-88.658554</td>\n",
       "      <td>111.841904</td>\n",
       "      <td>-50.724489</td>\n",
       "      <td>39.191503</td>\n",
       "      <td>130.719283</td>\n",
       "      <td>142.180054</td>\n",
       "      <td>209.543434</td>\n",
       "      <td>184.174078</td>\n",
       "      <td>-289.750657</td>\n",
       "      <td>-20.878277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55995</th>\n",
       "      <td>-409.882643</td>\n",
       "      <td>-398.597792</td>\n",
       "      <td>-25.972570</td>\n",
       "      <td>30.484170</td>\n",
       "      <td>528.856158</td>\n",
       "      <td>-259.396803</td>\n",
       "      <td>-666.106148</td>\n",
       "      <td>213.612587</td>\n",
       "      <td>36.163452</td>\n",
       "      <td>268.995341</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.558752</td>\n",
       "      <td>226.521509</td>\n",
       "      <td>-80.537462</td>\n",
       "      <td>-252.573200</td>\n",
       "      <td>294.959060</td>\n",
       "      <td>-42.497125</td>\n",
       "      <td>46.591117</td>\n",
       "      <td>-267.023137</td>\n",
       "      <td>-133.799030</td>\n",
       "      <td>-205.161669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55996</th>\n",
       "      <td>-872.753566</td>\n",
       "      <td>550.881219</td>\n",
       "      <td>194.314486</td>\n",
       "      <td>323.976556</td>\n",
       "      <td>-150.053871</td>\n",
       "      <td>467.882412</td>\n",
       "      <td>-363.779260</td>\n",
       "      <td>-87.665511</td>\n",
       "      <td>245.214496</td>\n",
       "      <td>72.414959</td>\n",
       "      <td>...</td>\n",
       "      <td>-26.898416</td>\n",
       "      <td>131.463556</td>\n",
       "      <td>151.776731</td>\n",
       "      <td>-222.721401</td>\n",
       "      <td>148.044683</td>\n",
       "      <td>16.159897</td>\n",
       "      <td>-73.334711</td>\n",
       "      <td>88.618945</td>\n",
       "      <td>-31.860813</td>\n",
       "      <td>-158.190581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55997</th>\n",
       "      <td>43.366120</td>\n",
       "      <td>-801.137368</td>\n",
       "      <td>-208.526509</td>\n",
       "      <td>294.915846</td>\n",
       "      <td>112.406840</td>\n",
       "      <td>272.569302</td>\n",
       "      <td>-333.447407</td>\n",
       "      <td>33.630293</td>\n",
       "      <td>-38.603534</td>\n",
       "      <td>348.987278</td>\n",
       "      <td>...</td>\n",
       "      <td>143.081818</td>\n",
       "      <td>130.877267</td>\n",
       "      <td>-134.246522</td>\n",
       "      <td>252.089075</td>\n",
       "      <td>-364.182482</td>\n",
       "      <td>-89.512377</td>\n",
       "      <td>-234.000468</td>\n",
       "      <td>135.118128</td>\n",
       "      <td>-35.081348</td>\n",
       "      <td>-8.575964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55998</th>\n",
       "      <td>-908.035319</td>\n",
       "      <td>640.541665</td>\n",
       "      <td>120.002867</td>\n",
       "      <td>226.822940</td>\n",
       "      <td>-392.843456</td>\n",
       "      <td>162.583943</td>\n",
       "      <td>-80.981872</td>\n",
       "      <td>-105.590704</td>\n",
       "      <td>329.223355</td>\n",
       "      <td>54.138034</td>\n",
       "      <td>...</td>\n",
       "      <td>215.474739</td>\n",
       "      <td>83.369449</td>\n",
       "      <td>67.033928</td>\n",
       "      <td>-32.136813</td>\n",
       "      <td>4.383493</td>\n",
       "      <td>-100.846010</td>\n",
       "      <td>6.354600</td>\n",
       "      <td>-47.984702</td>\n",
       "      <td>-33.845198</td>\n",
       "      <td>-62.042049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55999</th>\n",
       "      <td>134.965054</td>\n",
       "      <td>-514.955702</td>\n",
       "      <td>-527.655930</td>\n",
       "      <td>347.679302</td>\n",
       "      <td>-841.613715</td>\n",
       "      <td>153.123894</td>\n",
       "      <td>46.787354</td>\n",
       "      <td>346.516132</td>\n",
       "      <td>440.184235</td>\n",
       "      <td>-93.439574</td>\n",
       "      <td>...</td>\n",
       "      <td>-331.585834</td>\n",
       "      <td>-274.627607</td>\n",
       "      <td>367.969117</td>\n",
       "      <td>-168.856710</td>\n",
       "      <td>93.381214</td>\n",
       "      <td>96.687100</td>\n",
       "      <td>96.744838</td>\n",
       "      <td>-228.712798</td>\n",
       "      <td>486.178680</td>\n",
       "      <td>14.251479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56000 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Com1        Com2        Com3        Com4        Com5        Com6  \\\n",
       "0     -241.401991  273.953104  868.692070 -481.131177 -460.497659   64.873244   \n",
       "1     -330.370704 -612.732093  386.073937 -231.343692 -185.075654  487.317855   \n",
       "2     -187.022039 -745.558162  291.990794 -300.326123 -261.682733 -135.614707   \n",
       "3      666.090140 -645.507034 -175.316849  532.903514  186.392958  160.034157   \n",
       "4     -396.710534 -425.506796 -244.191298 -483.954170  212.053375  515.308769   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "55995 -409.882643 -398.597792  -25.972570   30.484170  528.856158 -259.396803   \n",
       "55996 -872.753566  550.881219  194.314486  323.976556 -150.053871  467.882412   \n",
       "55997   43.366120 -801.137368 -208.526509  294.915846  112.406840  272.569302   \n",
       "55998 -908.035319  640.541665  120.002867  226.822940 -392.843456  162.583943   \n",
       "55999  134.965054 -514.955702 -527.655930  347.679302 -841.613715  153.123894   \n",
       "\n",
       "             Com7        Com8        Com9       Com10  ...       Com21  \\\n",
       "0      109.783332  135.444373  432.628999   -9.933314  ...  -44.652165   \n",
       "1      280.298025   48.270838  -20.797023 -480.176604  ...  178.700099   \n",
       "2       65.893117   52.328778  320.062774 -589.928552  ...  316.437974   \n",
       "3     -702.722187    0.622236  913.299807 -708.372067  ... -416.498576   \n",
       "4      803.904983  219.957663 -409.401774 -174.889158  ...  -88.658554   \n",
       "...           ...         ...         ...         ...  ...         ...   \n",
       "55995 -666.106148  213.612587   36.163452  268.995341  ...  -17.558752   \n",
       "55996 -363.779260  -87.665511  245.214496   72.414959  ...  -26.898416   \n",
       "55997 -333.447407   33.630293  -38.603534  348.987278  ...  143.081818   \n",
       "55998  -80.981872 -105.590704  329.223355   54.138034  ...  215.474739   \n",
       "55999   46.787354  346.516132  440.184235  -93.439574  ... -331.585834   \n",
       "\n",
       "            Com22       Com23       Com24       Com25       Com26       Com27  \\\n",
       "0     -220.051528  265.052727 -106.590544   70.004746  119.752735   41.073423   \n",
       "1     -145.303165  130.830925   18.605387   66.879865  155.471674 -171.076035   \n",
       "2       24.766145 -329.590719  -92.764300  -51.168476 -242.706402  177.934294   \n",
       "3      282.060869  -52.709489 -392.488034  287.126675 -250.002248  -17.483804   \n",
       "4      111.841904  -50.724489   39.191503  130.719283  142.180054  209.543434   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "55995  226.521509  -80.537462 -252.573200  294.959060  -42.497125   46.591117   \n",
       "55996  131.463556  151.776731 -222.721401  148.044683   16.159897  -73.334711   \n",
       "55997  130.877267 -134.246522  252.089075 -364.182482  -89.512377 -234.000468   \n",
       "55998   83.369449   67.033928  -32.136813    4.383493 -100.846010    6.354600   \n",
       "55999 -274.627607  367.969117 -168.856710   93.381214   96.687100   96.744838   \n",
       "\n",
       "            Com28       Com29       Com30  \n",
       "0       21.025040  -32.711384  305.591231  \n",
       "1     -275.288421 -148.406533   64.471091  \n",
       "2     -101.492945  144.564830 -209.425290  \n",
       "3       60.595239  413.774061  -80.513751  \n",
       "4      184.174078 -289.750657  -20.878277  \n",
       "...           ...         ...         ...  \n",
       "55995 -267.023137 -133.799030 -205.161669  \n",
       "55996   88.618945  -31.860813 -158.190581  \n",
       "55997  135.118128  -35.081348   -8.575964  \n",
       "55998  -47.984702  -33.845198  -62.042049  \n",
       "55999 -228.712798  486.178680   14.251479  \n",
       "\n",
       "[56000 rows x 30 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_30 #주성분 30개 선택해서 만든 X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Com1</th>\n",
       "      <th>Com2</th>\n",
       "      <th>Com3</th>\n",
       "      <th>Com4</th>\n",
       "      <th>Com5</th>\n",
       "      <th>Com6</th>\n",
       "      <th>Com7</th>\n",
       "      <th>Com8</th>\n",
       "      <th>Com9</th>\n",
       "      <th>Com10</th>\n",
       "      <th>...</th>\n",
       "      <th>Com21</th>\n",
       "      <th>Com22</th>\n",
       "      <th>Com23</th>\n",
       "      <th>Com24</th>\n",
       "      <th>Com25</th>\n",
       "      <th>Com26</th>\n",
       "      <th>Com27</th>\n",
       "      <th>Com28</th>\n",
       "      <th>Com29</th>\n",
       "      <th>Com30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-610.537110</td>\n",
       "      <td>81.876931</td>\n",
       "      <td>284.411784</td>\n",
       "      <td>-232.026138</td>\n",
       "      <td>-46.998164</td>\n",
       "      <td>119.210050</td>\n",
       "      <td>-81.359873</td>\n",
       "      <td>-143.691122</td>\n",
       "      <td>-85.864541</td>\n",
       "      <td>-187.383540</td>\n",
       "      <td>...</td>\n",
       "      <td>-218.026829</td>\n",
       "      <td>433.626157</td>\n",
       "      <td>71.331183</td>\n",
       "      <td>-30.606277</td>\n",
       "      <td>-133.857265</td>\n",
       "      <td>4.845708</td>\n",
       "      <td>-12.320946</td>\n",
       "      <td>-199.136564</td>\n",
       "      <td>-196.095380</td>\n",
       "      <td>324.535278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-251.099420</td>\n",
       "      <td>314.787337</td>\n",
       "      <td>574.729279</td>\n",
       "      <td>-596.500884</td>\n",
       "      <td>197.138371</td>\n",
       "      <td>24.661011</td>\n",
       "      <td>-272.418107</td>\n",
       "      <td>8.435356</td>\n",
       "      <td>65.415845</td>\n",
       "      <td>227.501592</td>\n",
       "      <td>...</td>\n",
       "      <td>-74.176836</td>\n",
       "      <td>25.445625</td>\n",
       "      <td>-64.648000</td>\n",
       "      <td>92.453836</td>\n",
       "      <td>147.161519</td>\n",
       "      <td>-112.590722</td>\n",
       "      <td>-107.455839</td>\n",
       "      <td>270.237072</td>\n",
       "      <td>484.322808</td>\n",
       "      <td>-11.678008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-111.885904</td>\n",
       "      <td>-732.534515</td>\n",
       "      <td>112.567919</td>\n",
       "      <td>-429.864723</td>\n",
       "      <td>-64.472595</td>\n",
       "      <td>-130.995114</td>\n",
       "      <td>-190.053446</td>\n",
       "      <td>-162.586762</td>\n",
       "      <td>-365.539066</td>\n",
       "      <td>-158.097826</td>\n",
       "      <td>...</td>\n",
       "      <td>150.323096</td>\n",
       "      <td>-22.030471</td>\n",
       "      <td>101.746015</td>\n",
       "      <td>-63.152657</td>\n",
       "      <td>-280.563670</td>\n",
       "      <td>-315.232175</td>\n",
       "      <td>2.477928</td>\n",
       "      <td>21.313074</td>\n",
       "      <td>169.598166</td>\n",
       "      <td>-154.893119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>687.990155</td>\n",
       "      <td>62.173377</td>\n",
       "      <td>-324.350809</td>\n",
       "      <td>412.747990</td>\n",
       "      <td>107.190327</td>\n",
       "      <td>-681.679636</td>\n",
       "      <td>-35.000549</td>\n",
       "      <td>-78.866631</td>\n",
       "      <td>-199.094433</td>\n",
       "      <td>-391.926442</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.966687</td>\n",
       "      <td>-355.458834</td>\n",
       "      <td>148.002258</td>\n",
       "      <td>8.379644</td>\n",
       "      <td>166.836369</td>\n",
       "      <td>-40.788025</td>\n",
       "      <td>205.780272</td>\n",
       "      <td>106.720206</td>\n",
       "      <td>-186.537967</td>\n",
       "      <td>-46.244790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>533.371165</td>\n",
       "      <td>156.356114</td>\n",
       "      <td>479.663347</td>\n",
       "      <td>448.243658</td>\n",
       "      <td>102.094420</td>\n",
       "      <td>-81.115809</td>\n",
       "      <td>-162.333816</td>\n",
       "      <td>180.056697</td>\n",
       "      <td>-357.386795</td>\n",
       "      <td>-223.280220</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.591634</td>\n",
       "      <td>-296.973129</td>\n",
       "      <td>84.106024</td>\n",
       "      <td>-432.967469</td>\n",
       "      <td>410.764617</td>\n",
       "      <td>153.137735</td>\n",
       "      <td>-42.482205</td>\n",
       "      <td>-4.450753</td>\n",
       "      <td>-280.549291</td>\n",
       "      <td>-161.227781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13995</th>\n",
       "      <td>219.426183</td>\n",
       "      <td>166.578395</td>\n",
       "      <td>328.058177</td>\n",
       "      <td>-880.729066</td>\n",
       "      <td>467.032973</td>\n",
       "      <td>85.923351</td>\n",
       "      <td>-118.565663</td>\n",
       "      <td>-432.991404</td>\n",
       "      <td>-188.602385</td>\n",
       "      <td>248.098892</td>\n",
       "      <td>...</td>\n",
       "      <td>238.501033</td>\n",
       "      <td>-101.641478</td>\n",
       "      <td>-352.866121</td>\n",
       "      <td>138.518869</td>\n",
       "      <td>138.621768</td>\n",
       "      <td>69.321068</td>\n",
       "      <td>207.486280</td>\n",
       "      <td>-363.074766</td>\n",
       "      <td>221.335096</td>\n",
       "      <td>-48.905011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13996</th>\n",
       "      <td>-320.287532</td>\n",
       "      <td>59.419531</td>\n",
       "      <td>677.406316</td>\n",
       "      <td>-194.872404</td>\n",
       "      <td>-393.176616</td>\n",
       "      <td>-119.924067</td>\n",
       "      <td>224.092092</td>\n",
       "      <td>304.392529</td>\n",
       "      <td>250.452424</td>\n",
       "      <td>-105.214473</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.556647</td>\n",
       "      <td>-101.351083</td>\n",
       "      <td>-92.486575</td>\n",
       "      <td>-198.336558</td>\n",
       "      <td>-176.624162</td>\n",
       "      <td>266.376460</td>\n",
       "      <td>57.477672</td>\n",
       "      <td>89.036364</td>\n",
       "      <td>83.300858</td>\n",
       "      <td>103.573941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>-585.497240</td>\n",
       "      <td>98.056802</td>\n",
       "      <td>100.971779</td>\n",
       "      <td>-420.779864</td>\n",
       "      <td>-76.006778</td>\n",
       "      <td>-283.722151</td>\n",
       "      <td>251.587318</td>\n",
       "      <td>-637.118976</td>\n",
       "      <td>456.991636</td>\n",
       "      <td>208.192277</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.309039</td>\n",
       "      <td>4.200654</td>\n",
       "      <td>500.723106</td>\n",
       "      <td>-102.087998</td>\n",
       "      <td>-67.033759</td>\n",
       "      <td>-184.583179</td>\n",
       "      <td>118.757492</td>\n",
       "      <td>-5.276139</td>\n",
       "      <td>54.926328</td>\n",
       "      <td>-42.875655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13998</th>\n",
       "      <td>918.001747</td>\n",
       "      <td>-401.794290</td>\n",
       "      <td>123.982547</td>\n",
       "      <td>1245.602098</td>\n",
       "      <td>872.748140</td>\n",
       "      <td>-6.691902</td>\n",
       "      <td>481.234541</td>\n",
       "      <td>-224.556773</td>\n",
       "      <td>230.539170</td>\n",
       "      <td>-234.965203</td>\n",
       "      <td>...</td>\n",
       "      <td>173.745494</td>\n",
       "      <td>55.314314</td>\n",
       "      <td>-270.701955</td>\n",
       "      <td>-140.460352</td>\n",
       "      <td>-297.777240</td>\n",
       "      <td>27.448666</td>\n",
       "      <td>-319.605931</td>\n",
       "      <td>-99.436022</td>\n",
       "      <td>186.127826</td>\n",
       "      <td>-75.137264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13999</th>\n",
       "      <td>330.367371</td>\n",
       "      <td>-42.634339</td>\n",
       "      <td>780.374769</td>\n",
       "      <td>-210.662853</td>\n",
       "      <td>416.797019</td>\n",
       "      <td>540.771108</td>\n",
       "      <td>-412.630533</td>\n",
       "      <td>-23.982619</td>\n",
       "      <td>-250.651888</td>\n",
       "      <td>96.966160</td>\n",
       "      <td>...</td>\n",
       "      <td>185.276973</td>\n",
       "      <td>154.908888</td>\n",
       "      <td>75.612319</td>\n",
       "      <td>-276.801836</td>\n",
       "      <td>84.980579</td>\n",
       "      <td>-176.874219</td>\n",
       "      <td>155.931830</td>\n",
       "      <td>-232.640291</td>\n",
       "      <td>-53.959587</td>\n",
       "      <td>139.951080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14000 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Com1        Com2        Com3         Com4        Com5  \\\n",
       "0     -610.537110   81.876931  284.411784  -232.026138  -46.998164   \n",
       "1     -251.099420  314.787337  574.729279  -596.500884  197.138371   \n",
       "2     -111.885904 -732.534515  112.567919  -429.864723  -64.472595   \n",
       "3      687.990155   62.173377 -324.350809   412.747990  107.190327   \n",
       "4      533.371165  156.356114  479.663347   448.243658  102.094420   \n",
       "...           ...         ...         ...          ...         ...   \n",
       "13995  219.426183  166.578395  328.058177  -880.729066  467.032973   \n",
       "13996 -320.287532   59.419531  677.406316  -194.872404 -393.176616   \n",
       "13997 -585.497240   98.056802  100.971779  -420.779864  -76.006778   \n",
       "13998  918.001747 -401.794290  123.982547  1245.602098  872.748140   \n",
       "13999  330.367371  -42.634339  780.374769  -210.662853  416.797019   \n",
       "\n",
       "             Com6        Com7        Com8        Com9       Com10  ...  \\\n",
       "0      119.210050  -81.359873 -143.691122  -85.864541 -187.383540  ...   \n",
       "1       24.661011 -272.418107    8.435356   65.415845  227.501592  ...   \n",
       "2     -130.995114 -190.053446 -162.586762 -365.539066 -158.097826  ...   \n",
       "3     -681.679636  -35.000549  -78.866631 -199.094433 -391.926442  ...   \n",
       "4      -81.115809 -162.333816  180.056697 -357.386795 -223.280220  ...   \n",
       "...           ...         ...         ...         ...         ...  ...   \n",
       "13995   85.923351 -118.565663 -432.991404 -188.602385  248.098892  ...   \n",
       "13996 -119.924067  224.092092  304.392529  250.452424 -105.214473  ...   \n",
       "13997 -283.722151  251.587318 -637.118976  456.991636  208.192277  ...   \n",
       "13998   -6.691902  481.234541 -224.556773  230.539170 -234.965203  ...   \n",
       "13999  540.771108 -412.630533  -23.982619 -250.651888   96.966160  ...   \n",
       "\n",
       "            Com21       Com22       Com23       Com24       Com25       Com26  \\\n",
       "0     -218.026829  433.626157   71.331183  -30.606277 -133.857265    4.845708   \n",
       "1      -74.176836   25.445625  -64.648000   92.453836  147.161519 -112.590722   \n",
       "2      150.323096  -22.030471  101.746015  -63.152657 -280.563670 -315.232175   \n",
       "3      -18.966687 -355.458834  148.002258    8.379644  166.836369  -40.788025   \n",
       "4      -23.591634 -296.973129   84.106024 -432.967469  410.764617  153.137735   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "13995  238.501033 -101.641478 -352.866121  138.518869  138.621768   69.321068   \n",
       "13996  -16.556647 -101.351083  -92.486575 -198.336558 -176.624162  266.376460   \n",
       "13997  -10.309039    4.200654  500.723106 -102.087998  -67.033759 -184.583179   \n",
       "13998  173.745494   55.314314 -270.701955 -140.460352 -297.777240   27.448666   \n",
       "13999  185.276973  154.908888   75.612319 -276.801836   84.980579 -176.874219   \n",
       "\n",
       "            Com27       Com28       Com29       Com30  \n",
       "0      -12.320946 -199.136564 -196.095380  324.535278  \n",
       "1     -107.455839  270.237072  484.322808  -11.678008  \n",
       "2        2.477928   21.313074  169.598166 -154.893119  \n",
       "3      205.780272  106.720206 -186.537967  -46.244790  \n",
       "4      -42.482205   -4.450753 -280.549291 -161.227781  \n",
       "...           ...         ...         ...         ...  \n",
       "13995  207.486280 -363.074766  221.335096  -48.905011  \n",
       "13996   57.477672   89.036364   83.300858  103.573941  \n",
       "13997  118.757492   -5.276139   54.926328  -42.875655  \n",
       "13998 -319.605931  -99.436022  186.127826  -75.137264  \n",
       "13999  155.931830 -232.640291  -53.959587  139.951080  \n",
       "\n",
       "[14000 rows x 30 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_30 #마찬가지로 위의 pca 모델로부터 transform해서 얻어낸 X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=15, random_state=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=15,\n",
       "                       n_jobs=None, oob_score=False, random_state=77, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.fit(X_train_30, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9345"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = forest.predict(X_test_30)\n",
    "forest.score(X_test_30, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9345"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(y_test, y_pred)\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component 40-cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier()\n",
    "forest.fit(X_train_40, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.954"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = forest.predict(X_test_40)\n",
    "forest.score(X_test_40, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1289,    0,    2,    0,    2,    1,   11,    0,    2,    3],\n",
       "       [   0, 1580,    9,    7,    2,    4,    1,    4,    1,    0],\n",
       "       [  11,    3, 1365,   12,    5,    2,    5,   11,   19,    2],\n",
       "       [   1,    1,   13, 1314,    0,   19,    4,   11,   24,   17],\n",
       "       [   1,    3,    6,    2, 1348,    1,   11,    6,   10,   31],\n",
       "       [   5,    0,    6,   21,    5, 1190,   12,    3,    5,    5],\n",
       "       [   4,    2,    4,    2,    3,    7, 1371,    0,    1,    0],\n",
       "       [   1,    7,   19,    1,   11,    2,    0, 1406,    1,   24],\n",
       "       [   4,   11,   11,   29,    5,   22,    3,    6, 1250,   13],\n",
       "       [   3,    6,    5,   21,   31,   10,    1,   25,    7, 1243]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-0.21-cp37-none-win_amd64.whl (63.4 MB)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\강미경\\appdata\\roaming\\python\\python37\\site-packages (from catboost) (1.18.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\강미경\\appdata\\roaming\\python\\python37\\site-packages (from catboost) (1.4.1)\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.13.2-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (1.0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (3.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\강미경\\appdata\\roaming\\python\\python37\\site-packages (from catboost) (1.14.0)\n",
      "Collecting plotly\n",
      "  Downloading plotly-4.5.0-py2.py3-none-any.whl (7.1 MB)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.1.0)\n",
      "Collecting retrying>=1.3.3\n",
      "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\강미경\\appdata\\roaming\\python\\python37\\site-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (45.1.0)\n",
      "Building wheels for collected packages: retrying\n",
      "  Building wheel for retrying (setup.py): started\n",
      "  Building wheel for retrying (setup.py): finished with status 'done'\n",
      "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11435 sha256=c70470136bbad4b13b7cbedd3d70b552b08d25996e5e547f10d2689154364669\n",
      "  Stored in directory: c:\\users\\강미경\\appdata\\local\\pip\\cache\\wheels\\f9\\8d\\8d\\f6af3f7f9eea3553bc2fe6d53e4b287dad18b06a861ac56ddf\n",
      "Successfully built retrying\n",
      "Installing collected packages: graphviz, retrying, plotly, catboost\n",
      "Successfully installed catboost-0.21 graphviz-0.13.2 plotly-4.5.0 retrying-1.3.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 2.1982499\ttotal: 269ms\tremaining: 4m 28s\n",
      "1:\tlearn: 2.1097940\ttotal: 507ms\tremaining: 4m 12s\n",
      "2:\tlearn: 2.0357138\ttotal: 747ms\tremaining: 4m 8s\n",
      "3:\tlearn: 1.9691766\ttotal: 969ms\tremaining: 4m 1s\n",
      "4:\tlearn: 1.9106539\ttotal: 1.18s\tremaining: 3m 54s\n",
      "5:\tlearn: 1.8551938\ttotal: 1.39s\tremaining: 3m 50s\n",
      "6:\tlearn: 1.8052264\ttotal: 1.62s\tremaining: 3m 49s\n",
      "7:\tlearn: 1.7605535\ttotal: 1.85s\tremaining: 3m 49s\n",
      "8:\tlearn: 1.7156664\ttotal: 2.07s\tremaining: 3m 48s\n",
      "9:\tlearn: 1.6764571\ttotal: 2.3s\tremaining: 3m 47s\n",
      "10:\tlearn: 1.6406914\ttotal: 2.53s\tremaining: 3m 47s\n",
      "11:\tlearn: 1.6070694\ttotal: 2.76s\tremaining: 3m 47s\n",
      "12:\tlearn: 1.5742034\ttotal: 2.99s\tremaining: 3m 47s\n",
      "13:\tlearn: 1.5433269\ttotal: 3.21s\tremaining: 3m 45s\n",
      "14:\tlearn: 1.5134501\ttotal: 3.42s\tremaining: 3m 44s\n",
      "15:\tlearn: 1.4845450\ttotal: 3.65s\tremaining: 3m 44s\n",
      "16:\tlearn: 1.4576372\ttotal: 3.87s\tremaining: 3m 43s\n",
      "17:\tlearn: 1.4330205\ttotal: 4.06s\tremaining: 3m 41s\n",
      "18:\tlearn: 1.4082284\ttotal: 4.28s\tremaining: 3m 41s\n",
      "19:\tlearn: 1.3860474\ttotal: 4.52s\tremaining: 3m 41s\n",
      "20:\tlearn: 1.3641544\ttotal: 4.7s\tremaining: 3m 39s\n",
      "21:\tlearn: 1.3429692\ttotal: 4.91s\tremaining: 3m 38s\n",
      "22:\tlearn: 1.3222007\ttotal: 5.14s\tremaining: 3m 38s\n",
      "23:\tlearn: 1.3037477\ttotal: 5.36s\tremaining: 3m 37s\n",
      "24:\tlearn: 1.2845919\ttotal: 5.58s\tremaining: 3m 37s\n",
      "25:\tlearn: 1.2678950\ttotal: 5.8s\tremaining: 3m 37s\n",
      "26:\tlearn: 1.2489599\ttotal: 6.03s\tremaining: 3m 37s\n",
      "27:\tlearn: 1.2320816\ttotal: 6.26s\tremaining: 3m 37s\n",
      "28:\tlearn: 1.2165023\ttotal: 6.49s\tremaining: 3m 37s\n",
      "29:\tlearn: 1.1990475\ttotal: 6.72s\tremaining: 3m 37s\n",
      "30:\tlearn: 1.1845200\ttotal: 6.95s\tremaining: 3m 37s\n",
      "31:\tlearn: 1.1707353\ttotal: 7.15s\tremaining: 3m 36s\n",
      "32:\tlearn: 1.1568513\ttotal: 7.34s\tremaining: 3m 35s\n",
      "33:\tlearn: 1.1411990\ttotal: 7.57s\tremaining: 3m 34s\n",
      "34:\tlearn: 1.1263457\ttotal: 7.81s\tremaining: 3m 35s\n",
      "35:\tlearn: 1.1113536\ttotal: 8s\tremaining: 3m 34s\n",
      "36:\tlearn: 1.0997082\ttotal: 8.21s\tremaining: 3m 33s\n",
      "37:\tlearn: 1.0879597\ttotal: 8.43s\tremaining: 3m 33s\n",
      "38:\tlearn: 1.0758388\ttotal: 8.64s\tremaining: 3m 32s\n",
      "39:\tlearn: 1.0635801\ttotal: 8.86s\tremaining: 3m 32s\n",
      "40:\tlearn: 1.0527549\ttotal: 9.08s\tremaining: 3m 32s\n",
      "41:\tlearn: 1.0427845\ttotal: 9.3s\tremaining: 3m 32s\n",
      "42:\tlearn: 1.0313328\ttotal: 9.52s\tremaining: 3m 31s\n",
      "43:\tlearn: 1.0213781\ttotal: 9.74s\tremaining: 3m 31s\n",
      "44:\tlearn: 1.0100329\ttotal: 9.95s\tremaining: 3m 31s\n",
      "45:\tlearn: 1.0004595\ttotal: 10.2s\tremaining: 3m 31s\n",
      "46:\tlearn: 0.9911747\ttotal: 10.4s\tremaining: 3m 30s\n",
      "47:\tlearn: 0.9803652\ttotal: 10.6s\tremaining: 3m 30s\n",
      "48:\tlearn: 0.9712851\ttotal: 10.8s\tremaining: 3m 29s\n",
      "49:\tlearn: 0.9617530\ttotal: 10.9s\tremaining: 3m 27s\n",
      "50:\tlearn: 0.9518813\ttotal: 11.1s\tremaining: 3m 27s\n",
      "51:\tlearn: 0.9440375\ttotal: 11.3s\tremaining: 3m 26s\n",
      "52:\tlearn: 0.9356545\ttotal: 11.5s\tremaining: 3m 26s\n",
      "53:\tlearn: 0.9277130\ttotal: 11.8s\tremaining: 3m 25s\n",
      "54:\tlearn: 0.9200316\ttotal: 12s\tremaining: 3m 25s\n",
      "55:\tlearn: 0.9133728\ttotal: 12.2s\tremaining: 3m 25s\n",
      "56:\tlearn: 0.9061039\ttotal: 12.3s\tremaining: 3m 24s\n",
      "57:\tlearn: 0.8975894\ttotal: 12.5s\tremaining: 3m 23s\n",
      "58:\tlearn: 0.8890910\ttotal: 12.7s\tremaining: 3m 22s\n",
      "59:\tlearn: 0.8818779\ttotal: 12.9s\tremaining: 3m 22s\n",
      "60:\tlearn: 0.8748780\ttotal: 13.1s\tremaining: 3m 20s\n",
      "61:\tlearn: 0.8686788\ttotal: 13.2s\tremaining: 3m 20s\n",
      "62:\tlearn: 0.8611873\ttotal: 13.5s\tremaining: 3m 20s\n",
      "63:\tlearn: 0.8538669\ttotal: 13.7s\tremaining: 3m 19s\n",
      "64:\tlearn: 0.8473809\ttotal: 13.9s\tremaining: 3m 19s\n",
      "65:\tlearn: 0.8406378\ttotal: 14s\tremaining: 3m 18s\n",
      "66:\tlearn: 0.8347956\ttotal: 14.2s\tremaining: 3m 17s\n",
      "67:\tlearn: 0.8274974\ttotal: 14.4s\tremaining: 3m 18s\n",
      "68:\tlearn: 0.8195596\ttotal: 14.7s\tremaining: 3m 17s\n",
      "69:\tlearn: 0.8140431\ttotal: 14.9s\tremaining: 3m 17s\n",
      "70:\tlearn: 0.8077263\ttotal: 15.1s\tremaining: 3m 17s\n",
      "71:\tlearn: 0.8014940\ttotal: 15.3s\tremaining: 3m 17s\n",
      "72:\tlearn: 0.7948872\ttotal: 15.5s\tremaining: 3m 16s\n",
      "73:\tlearn: 0.7879532\ttotal: 15.7s\tremaining: 3m 16s\n",
      "74:\tlearn: 0.7820793\ttotal: 16s\tremaining: 3m 16s\n",
      "75:\tlearn: 0.7762900\ttotal: 16.2s\tremaining: 3m 16s\n",
      "76:\tlearn: 0.7713487\ttotal: 16.4s\tremaining: 3m 16s\n",
      "77:\tlearn: 0.7644492\ttotal: 16.6s\tremaining: 3m 16s\n",
      "78:\tlearn: 0.7579527\ttotal: 16.8s\tremaining: 3m 16s\n",
      "79:\tlearn: 0.7526685\ttotal: 17.1s\tremaining: 3m 16s\n",
      "80:\tlearn: 0.7478055\ttotal: 17.3s\tremaining: 3m 15s\n",
      "81:\tlearn: 0.7432221\ttotal: 17.5s\tremaining: 3m 15s\n",
      "82:\tlearn: 0.7389013\ttotal: 17.7s\tremaining: 3m 15s\n",
      "83:\tlearn: 0.7334217\ttotal: 17.9s\tremaining: 3m 15s\n",
      "84:\tlearn: 0.7287014\ttotal: 18.1s\tremaining: 3m 14s\n",
      "85:\tlearn: 0.7241957\ttotal: 18.3s\tremaining: 3m 14s\n",
      "86:\tlearn: 0.7186370\ttotal: 18.6s\tremaining: 3m 14s\n",
      "87:\tlearn: 0.7141680\ttotal: 18.8s\tremaining: 3m 14s\n",
      "88:\tlearn: 0.7096303\ttotal: 19s\tremaining: 3m 14s\n",
      "89:\tlearn: 0.7042676\ttotal: 19.2s\tremaining: 3m 13s\n",
      "90:\tlearn: 0.6989186\ttotal: 19.4s\tremaining: 3m 13s\n",
      "91:\tlearn: 0.6947250\ttotal: 19.6s\tremaining: 3m 13s\n",
      "92:\tlearn: 0.6908250\ttotal: 19.8s\tremaining: 3m 13s\n",
      "93:\tlearn: 0.6852045\ttotal: 20s\tremaining: 3m 13s\n",
      "94:\tlearn: 0.6809486\ttotal: 20.3s\tremaining: 3m 12s\n",
      "95:\tlearn: 0.6757974\ttotal: 20.5s\tremaining: 3m 12s\n",
      "96:\tlearn: 0.6715710\ttotal: 20.7s\tremaining: 3m 12s\n",
      "97:\tlearn: 0.6677381\ttotal: 20.9s\tremaining: 3m 12s\n",
      "98:\tlearn: 0.6635446\ttotal: 21.1s\tremaining: 3m 12s\n",
      "99:\tlearn: 0.6589423\ttotal: 21.3s\tremaining: 3m 11s\n",
      "100:\tlearn: 0.6548412\ttotal: 21.5s\tremaining: 3m 10s\n",
      "101:\tlearn: 0.6509060\ttotal: 21.6s\tremaining: 3m 10s\n",
      "102:\tlearn: 0.6473530\ttotal: 21.9s\tremaining: 3m 10s\n",
      "103:\tlearn: 0.6433047\ttotal: 22s\tremaining: 3m 9s\n",
      "104:\tlearn: 0.6395459\ttotal: 22.2s\tremaining: 3m 9s\n",
      "105:\tlearn: 0.6354072\ttotal: 22.4s\tremaining: 3m 8s\n",
      "106:\tlearn: 0.6305538\ttotal: 22.6s\tremaining: 3m 8s\n",
      "107:\tlearn: 0.6269192\ttotal: 22.8s\tremaining: 3m 7s\n",
      "108:\tlearn: 0.6231228\ttotal: 23s\tremaining: 3m 7s\n",
      "109:\tlearn: 0.6194173\ttotal: 23.1s\tremaining: 3m 7s\n",
      "110:\tlearn: 0.6151601\ttotal: 23.3s\tremaining: 3m 6s\n",
      "111:\tlearn: 0.6115735\ttotal: 23.5s\tremaining: 3m 6s\n",
      "112:\tlearn: 0.6075047\ttotal: 23.7s\tremaining: 3m 6s\n",
      "113:\tlearn: 0.6037266\ttotal: 23.9s\tremaining: 3m 5s\n",
      "114:\tlearn: 0.6006958\ttotal: 24s\tremaining: 3m 4s\n",
      "115:\tlearn: 0.5969760\ttotal: 24.3s\tremaining: 3m 4s\n",
      "116:\tlearn: 0.5941152\ttotal: 24.5s\tremaining: 3m 4s\n",
      "117:\tlearn: 0.5912718\ttotal: 24.7s\tremaining: 3m 4s\n",
      "118:\tlearn: 0.5871815\ttotal: 24.9s\tremaining: 3m 4s\n",
      "119:\tlearn: 0.5840672\ttotal: 25.1s\tremaining: 3m 4s\n",
      "120:\tlearn: 0.5807637\ttotal: 25.3s\tremaining: 3m 3s\n",
      "121:\tlearn: 0.5774549\ttotal: 25.5s\tremaining: 3m 3s\n",
      "122:\tlearn: 0.5744424\ttotal: 25.8s\tremaining: 3m 3s\n",
      "123:\tlearn: 0.5715366\ttotal: 26s\tremaining: 3m 3s\n",
      "124:\tlearn: 0.5685765\ttotal: 26.2s\tremaining: 3m 3s\n",
      "125:\tlearn: 0.5660688\ttotal: 26.3s\tremaining: 3m 2s\n",
      "126:\tlearn: 0.5632438\ttotal: 26.5s\tremaining: 3m 2s\n",
      "127:\tlearn: 0.5606683\ttotal: 26.7s\tremaining: 3m 2s\n",
      "128:\tlearn: 0.5578545\ttotal: 26.9s\tremaining: 3m 1s\n",
      "129:\tlearn: 0.5543745\ttotal: 27.2s\tremaining: 3m 1s\n",
      "130:\tlearn: 0.5506623\ttotal: 27.4s\tremaining: 3m 1s\n",
      "131:\tlearn: 0.5480514\ttotal: 27.5s\tremaining: 3m 1s\n",
      "132:\tlearn: 0.5448428\ttotal: 27.7s\tremaining: 3m\n",
      "133:\tlearn: 0.5429006\ttotal: 27.9s\tremaining: 3m\n",
      "134:\tlearn: 0.5396358\ttotal: 28.2s\tremaining: 3m\n",
      "135:\tlearn: 0.5369058\ttotal: 28.4s\tremaining: 3m\n",
      "136:\tlearn: 0.5343740\ttotal: 28.5s\tremaining: 2m 59s\n",
      "137:\tlearn: 0.5312736\ttotal: 28.7s\tremaining: 2m 59s\n",
      "138:\tlearn: 0.5282560\ttotal: 28.9s\tremaining: 2m 58s\n",
      "139:\tlearn: 0.5258957\ttotal: 29s\tremaining: 2m 58s\n",
      "140:\tlearn: 0.5234437\ttotal: 29.2s\tremaining: 2m 57s\n",
      "141:\tlearn: 0.5212554\ttotal: 29.4s\tremaining: 2m 57s\n",
      "142:\tlearn: 0.5182858\ttotal: 29.6s\tremaining: 2m 57s\n",
      "143:\tlearn: 0.5160393\ttotal: 29.8s\tremaining: 2m 57s\n",
      "144:\tlearn: 0.5136238\ttotal: 30s\tremaining: 2m 56s\n",
      "145:\tlearn: 0.5112921\ttotal: 30.1s\tremaining: 2m 56s\n",
      "146:\tlearn: 0.5084219\ttotal: 30.3s\tremaining: 2m 56s\n",
      "147:\tlearn: 0.5054925\ttotal: 30.6s\tremaining: 2m 56s\n",
      "148:\tlearn: 0.5033975\ttotal: 30.8s\tremaining: 2m 55s\n",
      "149:\tlearn: 0.5014468\ttotal: 31s\tremaining: 2m 55s\n",
      "150:\tlearn: 0.4993618\ttotal: 31.2s\tremaining: 2m 55s\n",
      "151:\tlearn: 0.4969921\ttotal: 31.4s\tremaining: 2m 54s\n",
      "152:\tlearn: 0.4949608\ttotal: 31.5s\tremaining: 2m 54s\n",
      "153:\tlearn: 0.4929957\ttotal: 31.7s\tremaining: 2m 54s\n",
      "154:\tlearn: 0.4904747\ttotal: 31.9s\tremaining: 2m 53s\n",
      "155:\tlearn: 0.4884311\ttotal: 32s\tremaining: 2m 53s\n",
      "156:\tlearn: 0.4863581\ttotal: 32.2s\tremaining: 2m 53s\n",
      "157:\tlearn: 0.4840811\ttotal: 32.4s\tremaining: 2m 52s\n",
      "158:\tlearn: 0.4821776\ttotal: 32.6s\tremaining: 2m 52s\n",
      "159:\tlearn: 0.4804226\ttotal: 32.8s\tremaining: 2m 51s\n",
      "160:\tlearn: 0.4784829\ttotal: 32.9s\tremaining: 2m 51s\n",
      "161:\tlearn: 0.4769831\ttotal: 33.1s\tremaining: 2m 51s\n",
      "162:\tlearn: 0.4748003\ttotal: 33.3s\tremaining: 2m 51s\n",
      "163:\tlearn: 0.4727817\ttotal: 33.5s\tremaining: 2m 50s\n",
      "164:\tlearn: 0.4706156\ttotal: 33.7s\tremaining: 2m 50s\n",
      "165:\tlearn: 0.4685680\ttotal: 33.8s\tremaining: 2m 49s\n",
      "166:\tlearn: 0.4666288\ttotal: 34s\tremaining: 2m 49s\n",
      "167:\tlearn: 0.4648661\ttotal: 34.1s\tremaining: 2m 49s\n",
      "168:\tlearn: 0.4633805\ttotal: 34.3s\tremaining: 2m 48s\n",
      "169:\tlearn: 0.4617496\ttotal: 34.5s\tremaining: 2m 48s\n",
      "170:\tlearn: 0.4594786\ttotal: 34.7s\tremaining: 2m 48s\n",
      "171:\tlearn: 0.4573616\ttotal: 34.9s\tremaining: 2m 47s\n",
      "172:\tlearn: 0.4549338\ttotal: 35s\tremaining: 2m 47s\n",
      "173:\tlearn: 0.4530181\ttotal: 35.2s\tremaining: 2m 47s\n",
      "174:\tlearn: 0.4510193\ttotal: 35.4s\tremaining: 2m 46s\n",
      "175:\tlearn: 0.4489535\ttotal: 35.6s\tremaining: 2m 46s\n",
      "176:\tlearn: 0.4474708\ttotal: 35.8s\tremaining: 2m 46s\n",
      "177:\tlearn: 0.4459917\ttotal: 35.9s\tremaining: 2m 45s\n",
      "178:\tlearn: 0.4437476\ttotal: 36.1s\tremaining: 2m 45s\n",
      "179:\tlearn: 0.4420923\ttotal: 36.3s\tremaining: 2m 45s\n",
      "180:\tlearn: 0.4404769\ttotal: 36.5s\tremaining: 2m 44s\n",
      "181:\tlearn: 0.4388779\ttotal: 36.7s\tremaining: 2m 44s\n",
      "182:\tlearn: 0.4372176\ttotal: 36.9s\tremaining: 2m 44s\n",
      "183:\tlearn: 0.4355812\ttotal: 37.1s\tremaining: 2m 44s\n",
      "184:\tlearn: 0.4334685\ttotal: 37.3s\tremaining: 2m 44s\n",
      "185:\tlearn: 0.4316733\ttotal: 37.5s\tremaining: 2m 43s\n",
      "186:\tlearn: 0.4296315\ttotal: 37.6s\tremaining: 2m 43s\n",
      "187:\tlearn: 0.4285227\ttotal: 37.9s\tremaining: 2m 43s\n",
      "188:\tlearn: 0.4266184\ttotal: 38s\tremaining: 2m 43s\n",
      "189:\tlearn: 0.4249179\ttotal: 38.2s\tremaining: 2m 42s\n",
      "190:\tlearn: 0.4231826\ttotal: 38.4s\tremaining: 2m 42s\n",
      "191:\tlearn: 0.4214741\ttotal: 38.6s\tremaining: 2m 42s\n",
      "192:\tlearn: 0.4198475\ttotal: 38.8s\tremaining: 2m 42s\n",
      "193:\tlearn: 0.4182124\ttotal: 38.9s\tremaining: 2m 41s\n",
      "194:\tlearn: 0.4170419\ttotal: 39.1s\tremaining: 2m 41s\n",
      "195:\tlearn: 0.4154169\ttotal: 39.3s\tremaining: 2m 41s\n",
      "196:\tlearn: 0.4140024\ttotal: 39.5s\tremaining: 2m 40s\n",
      "197:\tlearn: 0.4124800\ttotal: 39.6s\tremaining: 2m 40s\n",
      "198:\tlearn: 0.4108978\ttotal: 39.8s\tremaining: 2m 40s\n",
      "199:\tlearn: 0.4094682\ttotal: 40s\tremaining: 2m 40s\n",
      "200:\tlearn: 0.4079274\ttotal: 40.2s\tremaining: 2m 39s\n",
      "201:\tlearn: 0.4059192\ttotal: 40.4s\tremaining: 2m 39s\n",
      "202:\tlearn: 0.4046938\ttotal: 40.5s\tremaining: 2m 39s\n",
      "203:\tlearn: 0.4034403\ttotal: 40.7s\tremaining: 2m 38s\n",
      "204:\tlearn: 0.4021003\ttotal: 40.9s\tremaining: 2m 38s\n",
      "205:\tlearn: 0.4002460\ttotal: 41.1s\tremaining: 2m 38s\n",
      "206:\tlearn: 0.3986951\ttotal: 41.3s\tremaining: 2m 38s\n",
      "207:\tlearn: 0.3974934\ttotal: 41.5s\tremaining: 2m 38s\n",
      "208:\tlearn: 0.3963511\ttotal: 41.7s\tremaining: 2m 38s\n",
      "209:\tlearn: 0.3947862\ttotal: 42s\tremaining: 2m 37s\n",
      "210:\tlearn: 0.3935370\ttotal: 42.2s\tremaining: 2m 37s\n",
      "211:\tlearn: 0.3921720\ttotal: 42.4s\tremaining: 2m 37s\n",
      "212:\tlearn: 0.3908939\ttotal: 42.7s\tremaining: 2m 37s\n",
      "213:\tlearn: 0.3894970\ttotal: 43s\tremaining: 2m 37s\n",
      "214:\tlearn: 0.3885437\ttotal: 43.2s\tremaining: 2m 37s\n",
      "215:\tlearn: 0.3876120\ttotal: 43.5s\tremaining: 2m 37s\n",
      "216:\tlearn: 0.3862529\ttotal: 43.7s\tremaining: 2m 37s\n",
      "217:\tlearn: 0.3850920\ttotal: 43.9s\tremaining: 2m 37s\n",
      "218:\tlearn: 0.3841314\ttotal: 44.2s\tremaining: 2m 37s\n",
      "219:\tlearn: 0.3828099\ttotal: 44.4s\tremaining: 2m 37s\n",
      "220:\tlearn: 0.3810307\ttotal: 44.6s\tremaining: 2m 37s\n",
      "221:\tlearn: 0.3796654\ttotal: 44.8s\tremaining: 2m 37s\n",
      "222:\tlearn: 0.3784715\ttotal: 45s\tremaining: 2m 36s\n",
      "223:\tlearn: 0.3770461\ttotal: 45.3s\tremaining: 2m 36s\n",
      "224:\tlearn: 0.3758134\ttotal: 45.5s\tremaining: 2m 36s\n",
      "225:\tlearn: 0.3746914\ttotal: 45.8s\tremaining: 2m 36s\n",
      "226:\tlearn: 0.3736490\ttotal: 46s\tremaining: 2m 36s\n",
      "227:\tlearn: 0.3726085\ttotal: 46.2s\tremaining: 2m 36s\n",
      "228:\tlearn: 0.3714894\ttotal: 46.4s\tremaining: 2m 36s\n",
      "229:\tlearn: 0.3704168\ttotal: 46.6s\tremaining: 2m 36s\n",
      "230:\tlearn: 0.3693991\ttotal: 46.9s\tremaining: 2m 36s\n",
      "231:\tlearn: 0.3685744\ttotal: 47.1s\tremaining: 2m 35s\n",
      "232:\tlearn: 0.3671254\ttotal: 47.4s\tremaining: 2m 36s\n",
      "233:\tlearn: 0.3659683\ttotal: 47.7s\tremaining: 2m 35s\n",
      "234:\tlearn: 0.3648791\ttotal: 47.8s\tremaining: 2m 35s\n",
      "235:\tlearn: 0.3636390\ttotal: 48.1s\tremaining: 2m 35s\n",
      "236:\tlearn: 0.3625453\ttotal: 48.3s\tremaining: 2m 35s\n",
      "237:\tlearn: 0.3609619\ttotal: 48.5s\tremaining: 2m 35s\n",
      "238:\tlearn: 0.3600599\ttotal: 48.8s\tremaining: 2m 35s\n",
      "239:\tlearn: 0.3590655\ttotal: 49s\tremaining: 2m 35s\n",
      "240:\tlearn: 0.3581965\ttotal: 49.3s\tremaining: 2m 35s\n",
      "241:\tlearn: 0.3571800\ttotal: 49.6s\tremaining: 2m 35s\n",
      "242:\tlearn: 0.3561555\ttotal: 49.8s\tremaining: 2m 35s\n",
      "243:\tlearn: 0.3551576\ttotal: 50s\tremaining: 2m 35s\n",
      "244:\tlearn: 0.3540300\ttotal: 50.3s\tremaining: 2m 34s\n",
      "245:\tlearn: 0.3530289\ttotal: 50.5s\tremaining: 2m 34s\n",
      "246:\tlearn: 0.3520258\ttotal: 50.6s\tremaining: 2m 34s\n",
      "247:\tlearn: 0.3510563\ttotal: 50.8s\tremaining: 2m 34s\n",
      "248:\tlearn: 0.3497602\ttotal: 51.1s\tremaining: 2m 34s\n",
      "249:\tlearn: 0.3487084\ttotal: 51.2s\tremaining: 2m 33s\n",
      "250:\tlearn: 0.3473195\ttotal: 51.4s\tremaining: 2m 33s\n",
      "251:\tlearn: 0.3464619\ttotal: 51.6s\tremaining: 2m 33s\n",
      "252:\tlearn: 0.3456107\ttotal: 51.8s\tremaining: 2m 32s\n",
      "253:\tlearn: 0.3444069\ttotal: 51.9s\tremaining: 2m 32s\n",
      "254:\tlearn: 0.3434601\ttotal: 52.1s\tremaining: 2m 32s\n",
      "255:\tlearn: 0.3425765\ttotal: 52.3s\tremaining: 2m 31s\n",
      "256:\tlearn: 0.3416047\ttotal: 52.4s\tremaining: 2m 31s\n",
      "257:\tlearn: 0.3406648\ttotal: 52.6s\tremaining: 2m 31s\n",
      "258:\tlearn: 0.3397594\ttotal: 52.8s\tremaining: 2m 31s\n",
      "259:\tlearn: 0.3387625\ttotal: 53s\tremaining: 2m 30s\n",
      "260:\tlearn: 0.3376207\ttotal: 53.1s\tremaining: 2m 30s\n",
      "261:\tlearn: 0.3367350\ttotal: 53.3s\tremaining: 2m 30s\n",
      "262:\tlearn: 0.3358816\ttotal: 53.5s\tremaining: 2m 29s\n",
      "263:\tlearn: 0.3349823\ttotal: 53.6s\tremaining: 2m 29s\n",
      "264:\tlearn: 0.3342520\ttotal: 53.8s\tremaining: 2m 29s\n",
      "265:\tlearn: 0.3333669\ttotal: 54s\tremaining: 2m 29s\n",
      "266:\tlearn: 0.3324788\ttotal: 54.2s\tremaining: 2m 28s\n",
      "267:\tlearn: 0.3315358\ttotal: 54.3s\tremaining: 2m 28s\n",
      "268:\tlearn: 0.3304836\ttotal: 54.5s\tremaining: 2m 28s\n",
      "269:\tlearn: 0.3293279\ttotal: 54.7s\tremaining: 2m 28s\n",
      "270:\tlearn: 0.3284092\ttotal: 54.9s\tremaining: 2m 27s\n",
      "271:\tlearn: 0.3276988\ttotal: 55.1s\tremaining: 2m 27s\n",
      "272:\tlearn: 0.3269321\ttotal: 55.3s\tremaining: 2m 27s\n",
      "273:\tlearn: 0.3259029\ttotal: 55.4s\tremaining: 2m 26s\n",
      "274:\tlearn: 0.3251212\ttotal: 55.6s\tremaining: 2m 26s\n",
      "275:\tlearn: 0.3243093\ttotal: 55.8s\tremaining: 2m 26s\n",
      "276:\tlearn: 0.3232795\ttotal: 56s\tremaining: 2m 26s\n",
      "277:\tlearn: 0.3224022\ttotal: 56.1s\tremaining: 2m 25s\n",
      "278:\tlearn: 0.3216042\ttotal: 56.3s\tremaining: 2m 25s\n",
      "279:\tlearn: 0.3207070\ttotal: 56.5s\tremaining: 2m 25s\n",
      "280:\tlearn: 0.3199607\ttotal: 56.7s\tremaining: 2m 25s\n",
      "281:\tlearn: 0.3190071\ttotal: 56.9s\tremaining: 2m 24s\n",
      "282:\tlearn: 0.3178767\ttotal: 57s\tremaining: 2m 24s\n",
      "283:\tlearn: 0.3170890\ttotal: 57.3s\tremaining: 2m 24s\n",
      "284:\tlearn: 0.3164411\ttotal: 57.5s\tremaining: 2m 24s\n",
      "285:\tlearn: 0.3155696\ttotal: 57.7s\tremaining: 2m 24s\n",
      "286:\tlearn: 0.3149705\ttotal: 57.9s\tremaining: 2m 23s\n",
      "287:\tlearn: 0.3140618\ttotal: 58s\tremaining: 2m 23s\n",
      "288:\tlearn: 0.3134492\ttotal: 58.2s\tremaining: 2m 23s\n",
      "289:\tlearn: 0.3124595\ttotal: 58.4s\tremaining: 2m 23s\n",
      "290:\tlearn: 0.3116404\ttotal: 58.6s\tremaining: 2m 22s\n",
      "291:\tlearn: 0.3107936\ttotal: 58.8s\tremaining: 2m 22s\n",
      "292:\tlearn: 0.3098476\ttotal: 59s\tremaining: 2m 22s\n",
      "293:\tlearn: 0.3091009\ttotal: 59.2s\tremaining: 2m 22s\n",
      "294:\tlearn: 0.3082717\ttotal: 59.3s\tremaining: 2m 21s\n",
      "295:\tlearn: 0.3076242\ttotal: 59.5s\tremaining: 2m 21s\n",
      "296:\tlearn: 0.3069262\ttotal: 59.7s\tremaining: 2m 21s\n",
      "297:\tlearn: 0.3063868\ttotal: 59.8s\tremaining: 2m 20s\n",
      "298:\tlearn: 0.3055237\ttotal: 1m\tremaining: 2m 20s\n",
      "299:\tlearn: 0.3047689\ttotal: 1m\tremaining: 2m 20s\n",
      "300:\tlearn: 0.3040826\ttotal: 1m\tremaining: 2m 20s\n",
      "301:\tlearn: 0.3032284\ttotal: 1m\tremaining: 2m 19s\n",
      "302:\tlearn: 0.3024691\ttotal: 1m\tremaining: 2m 19s\n",
      "303:\tlearn: 0.3016233\ttotal: 1m\tremaining: 2m 19s\n",
      "304:\tlearn: 0.3011209\ttotal: 1m 1s\tremaining: 2m 19s\n",
      "305:\tlearn: 0.3003694\ttotal: 1m 1s\tremaining: 2m 19s\n",
      "306:\tlearn: 0.2997471\ttotal: 1m 1s\tremaining: 2m 18s\n",
      "307:\tlearn: 0.2989635\ttotal: 1m 1s\tremaining: 2m 18s\n",
      "308:\tlearn: 0.2983882\ttotal: 1m 1s\tremaining: 2m 18s\n",
      "309:\tlearn: 0.2979218\ttotal: 1m 1s\tremaining: 2m 17s\n",
      "310:\tlearn: 0.2971614\ttotal: 1m 2s\tremaining: 2m 17s\n",
      "311:\tlearn: 0.2964555\ttotal: 1m 2s\tremaining: 2m 17s\n",
      "312:\tlearn: 0.2958647\ttotal: 1m 2s\tremaining: 2m 17s\n",
      "313:\tlearn: 0.2950751\ttotal: 1m 2s\tremaining: 2m 16s\n",
      "314:\tlearn: 0.2945996\ttotal: 1m 2s\tremaining: 2m 16s\n",
      "315:\tlearn: 0.2938882\ttotal: 1m 3s\tremaining: 2m 16s\n",
      "316:\tlearn: 0.2933350\ttotal: 1m 3s\tremaining: 2m 16s\n",
      "317:\tlearn: 0.2924407\ttotal: 1m 3s\tremaining: 2m 15s\n",
      "318:\tlearn: 0.2919716\ttotal: 1m 3s\tremaining: 2m 15s\n",
      "319:\tlearn: 0.2914191\ttotal: 1m 3s\tremaining: 2m 15s\n",
      "320:\tlearn: 0.2907497\ttotal: 1m 3s\tremaining: 2m 15s\n",
      "321:\tlearn: 0.2901529\ttotal: 1m 4s\tremaining: 2m 15s\n",
      "322:\tlearn: 0.2894613\ttotal: 1m 4s\tremaining: 2m 14s\n",
      "323:\tlearn: 0.2885792\ttotal: 1m 4s\tremaining: 2m 14s\n",
      "324:\tlearn: 0.2879663\ttotal: 1m 4s\tremaining: 2m 14s\n",
      "325:\tlearn: 0.2873449\ttotal: 1m 4s\tremaining: 2m 14s\n",
      "326:\tlearn: 0.2867317\ttotal: 1m 5s\tremaining: 2m 13s\n",
      "327:\tlearn: 0.2861625\ttotal: 1m 5s\tremaining: 2m 13s\n",
      "328:\tlearn: 0.2854334\ttotal: 1m 5s\tremaining: 2m 13s\n",
      "329:\tlearn: 0.2848636\ttotal: 1m 5s\tremaining: 2m 13s\n",
      "330:\tlearn: 0.2844215\ttotal: 1m 5s\tremaining: 2m 12s\n",
      "331:\tlearn: 0.2836254\ttotal: 1m 5s\tremaining: 2m 12s\n",
      "332:\tlearn: 0.2831187\ttotal: 1m 6s\tremaining: 2m 12s\n",
      "333:\tlearn: 0.2826269\ttotal: 1m 6s\tremaining: 2m 12s\n",
      "334:\tlearn: 0.2819924\ttotal: 1m 6s\tremaining: 2m 11s\n",
      "335:\tlearn: 0.2814446\ttotal: 1m 6s\tremaining: 2m 11s\n",
      "336:\tlearn: 0.2809314\ttotal: 1m 6s\tremaining: 2m 11s\n",
      "337:\tlearn: 0.2803076\ttotal: 1m 6s\tremaining: 2m 11s\n",
      "338:\tlearn: 0.2796405\ttotal: 1m 7s\tremaining: 2m 10s\n",
      "339:\tlearn: 0.2790825\ttotal: 1m 7s\tremaining: 2m 10s\n",
      "340:\tlearn: 0.2783797\ttotal: 1m 7s\tremaining: 2m 10s\n",
      "341:\tlearn: 0.2776235\ttotal: 1m 7s\tremaining: 2m 10s\n",
      "342:\tlearn: 0.2772098\ttotal: 1m 7s\tremaining: 2m 9s\n",
      "343:\tlearn: 0.2766336\ttotal: 1m 8s\tremaining: 2m 9s\n",
      "344:\tlearn: 0.2762392\ttotal: 1m 8s\tremaining: 2m 9s\n",
      "345:\tlearn: 0.2756646\ttotal: 1m 8s\tremaining: 2m 9s\n",
      "346:\tlearn: 0.2752109\ttotal: 1m 8s\tremaining: 2m 9s\n",
      "347:\tlearn: 0.2747434\ttotal: 1m 8s\tremaining: 2m 8s\n",
      "348:\tlearn: 0.2742890\ttotal: 1m 8s\tremaining: 2m 8s\n",
      "349:\tlearn: 0.2737231\ttotal: 1m 9s\tremaining: 2m 8s\n",
      "350:\tlearn: 0.2730555\ttotal: 1m 9s\tremaining: 2m 8s\n",
      "351:\tlearn: 0.2725363\ttotal: 1m 9s\tremaining: 2m 7s\n",
      "352:\tlearn: 0.2720033\ttotal: 1m 9s\tremaining: 2m 7s\n",
      "353:\tlearn: 0.2715139\ttotal: 1m 9s\tremaining: 2m 7s\n",
      "354:\tlearn: 0.2710709\ttotal: 1m 9s\tremaining: 2m 7s\n",
      "355:\tlearn: 0.2705817\ttotal: 1m 10s\tremaining: 2m 6s\n",
      "356:\tlearn: 0.2700529\ttotal: 1m 10s\tremaining: 2m 6s\n",
      "357:\tlearn: 0.2691410\ttotal: 1m 10s\tremaining: 2m 6s\n",
      "358:\tlearn: 0.2686075\ttotal: 1m 10s\tremaining: 2m 6s\n",
      "359:\tlearn: 0.2679656\ttotal: 1m 10s\tremaining: 2m 6s\n",
      "360:\tlearn: 0.2675153\ttotal: 1m 11s\tremaining: 2m 5s\n",
      "361:\tlearn: 0.2668304\ttotal: 1m 11s\tremaining: 2m 5s\n",
      "362:\tlearn: 0.2664460\ttotal: 1m 11s\tremaining: 2m 5s\n",
      "363:\tlearn: 0.2658527\ttotal: 1m 11s\tremaining: 2m 5s\n",
      "364:\tlearn: 0.2649531\ttotal: 1m 11s\tremaining: 2m 4s\n",
      "365:\tlearn: 0.2646038\ttotal: 1m 11s\tremaining: 2m 4s\n",
      "366:\tlearn: 0.2641637\ttotal: 1m 12s\tremaining: 2m 4s\n",
      "367:\tlearn: 0.2638540\ttotal: 1m 12s\tremaining: 2m 4s\n",
      "368:\tlearn: 0.2631451\ttotal: 1m 12s\tremaining: 2m 3s\n",
      "369:\tlearn: 0.2625938\ttotal: 1m 12s\tremaining: 2m 3s\n",
      "370:\tlearn: 0.2621764\ttotal: 1m 12s\tremaining: 2m 3s\n",
      "371:\tlearn: 0.2617306\ttotal: 1m 13s\tremaining: 2m 3s\n",
      "372:\tlearn: 0.2613250\ttotal: 1m 13s\tremaining: 2m 3s\n",
      "373:\tlearn: 0.2609649\ttotal: 1m 13s\tremaining: 2m 2s\n",
      "374:\tlearn: 0.2604942\ttotal: 1m 13s\tremaining: 2m 2s\n",
      "375:\tlearn: 0.2600677\ttotal: 1m 13s\tremaining: 2m 2s\n",
      "376:\tlearn: 0.2594775\ttotal: 1m 13s\tremaining: 2m 2s\n",
      "377:\tlearn: 0.2590460\ttotal: 1m 14s\tremaining: 2m 1s\n",
      "378:\tlearn: 0.2585476\ttotal: 1m 14s\tremaining: 2m 1s\n",
      "379:\tlearn: 0.2581925\ttotal: 1m 14s\tremaining: 2m 1s\n",
      "380:\tlearn: 0.2577684\ttotal: 1m 14s\tremaining: 2m 1s\n",
      "381:\tlearn: 0.2574359\ttotal: 1m 14s\tremaining: 2m\n",
      "382:\tlearn: 0.2568451\ttotal: 1m 14s\tremaining: 2m\n",
      "383:\tlearn: 0.2562832\ttotal: 1m 15s\tremaining: 2m\n",
      "384:\tlearn: 0.2557937\ttotal: 1m 15s\tremaining: 2m\n",
      "385:\tlearn: 0.2551776\ttotal: 1m 15s\tremaining: 1m 59s\n",
      "386:\tlearn: 0.2546763\ttotal: 1m 15s\tremaining: 1m 59s\n",
      "387:\tlearn: 0.2541302\ttotal: 1m 15s\tremaining: 1m 59s\n",
      "388:\tlearn: 0.2538260\ttotal: 1m 15s\tremaining: 1m 59s\n",
      "389:\tlearn: 0.2531714\ttotal: 1m 16s\tremaining: 1m 58s\n",
      "390:\tlearn: 0.2526633\ttotal: 1m 16s\tremaining: 1m 58s\n",
      "391:\tlearn: 0.2522970\ttotal: 1m 16s\tremaining: 1m 58s\n",
      "392:\tlearn: 0.2518434\ttotal: 1m 16s\tremaining: 1m 58s\n",
      "393:\tlearn: 0.2514302\ttotal: 1m 16s\tremaining: 1m 58s\n",
      "394:\tlearn: 0.2510080\ttotal: 1m 16s\tremaining: 1m 57s\n",
      "395:\tlearn: 0.2504787\ttotal: 1m 17s\tremaining: 1m 57s\n",
      "396:\tlearn: 0.2499657\ttotal: 1m 17s\tremaining: 1m 57s\n",
      "397:\tlearn: 0.2491678\ttotal: 1m 17s\tremaining: 1m 57s\n",
      "398:\tlearn: 0.2488023\ttotal: 1m 17s\tremaining: 1m 56s\n",
      "399:\tlearn: 0.2483085\ttotal: 1m 17s\tremaining: 1m 56s\n",
      "400:\tlearn: 0.2478583\ttotal: 1m 17s\tremaining: 1m 56s\n",
      "401:\tlearn: 0.2473616\ttotal: 1m 18s\tremaining: 1m 56s\n",
      "402:\tlearn: 0.2469963\ttotal: 1m 18s\tremaining: 1m 55s\n",
      "403:\tlearn: 0.2462031\ttotal: 1m 18s\tremaining: 1m 55s\n",
      "404:\tlearn: 0.2458566\ttotal: 1m 18s\tremaining: 1m 55s\n",
      "405:\tlearn: 0.2452839\ttotal: 1m 18s\tremaining: 1m 55s\n",
      "406:\tlearn: 0.2449962\ttotal: 1m 18s\tremaining: 1m 55s\n",
      "407:\tlearn: 0.2445560\ttotal: 1m 19s\tremaining: 1m 54s\n",
      "408:\tlearn: 0.2442864\ttotal: 1m 19s\tremaining: 1m 54s\n",
      "409:\tlearn: 0.2438236\ttotal: 1m 19s\tremaining: 1m 54s\n",
      "410:\tlearn: 0.2432899\ttotal: 1m 19s\tremaining: 1m 54s\n",
      "411:\tlearn: 0.2429468\ttotal: 1m 19s\tremaining: 1m 54s\n",
      "412:\tlearn: 0.2424984\ttotal: 1m 20s\tremaining: 1m 53s\n",
      "413:\tlearn: 0.2420706\ttotal: 1m 20s\tremaining: 1m 53s\n",
      "414:\tlearn: 0.2416685\ttotal: 1m 20s\tremaining: 1m 53s\n",
      "415:\tlearn: 0.2413690\ttotal: 1m 20s\tremaining: 1m 53s\n",
      "416:\tlearn: 0.2409876\ttotal: 1m 20s\tremaining: 1m 52s\n",
      "417:\tlearn: 0.2406349\ttotal: 1m 20s\tremaining: 1m 52s\n",
      "418:\tlearn: 0.2402911\ttotal: 1m 21s\tremaining: 1m 52s\n",
      "419:\tlearn: 0.2399208\ttotal: 1m 21s\tremaining: 1m 52s\n",
      "420:\tlearn: 0.2395037\ttotal: 1m 21s\tremaining: 1m 51s\n",
      "421:\tlearn: 0.2391862\ttotal: 1m 21s\tremaining: 1m 51s\n",
      "422:\tlearn: 0.2388261\ttotal: 1m 21s\tremaining: 1m 51s\n",
      "423:\tlearn: 0.2382894\ttotal: 1m 21s\tremaining: 1m 51s\n",
      "424:\tlearn: 0.2377007\ttotal: 1m 22s\tremaining: 1m 51s\n",
      "425:\tlearn: 0.2373140\ttotal: 1m 22s\tremaining: 1m 50s\n",
      "426:\tlearn: 0.2368603\ttotal: 1m 22s\tremaining: 1m 50s\n",
      "427:\tlearn: 0.2364519\ttotal: 1m 22s\tremaining: 1m 50s\n",
      "428:\tlearn: 0.2360039\ttotal: 1m 22s\tremaining: 1m 50s\n",
      "429:\tlearn: 0.2356572\ttotal: 1m 22s\tremaining: 1m 49s\n",
      "430:\tlearn: 0.2353612\ttotal: 1m 23s\tremaining: 1m 49s\n",
      "431:\tlearn: 0.2349488\ttotal: 1m 23s\tremaining: 1m 49s\n",
      "432:\tlearn: 0.2343797\ttotal: 1m 23s\tremaining: 1m 49s\n",
      "433:\tlearn: 0.2339870\ttotal: 1m 23s\tremaining: 1m 49s\n",
      "434:\tlearn: 0.2336481\ttotal: 1m 23s\tremaining: 1m 48s\n",
      "435:\tlearn: 0.2333575\ttotal: 1m 23s\tremaining: 1m 48s\n",
      "436:\tlearn: 0.2329771\ttotal: 1m 24s\tremaining: 1m 48s\n",
      "437:\tlearn: 0.2325743\ttotal: 1m 24s\tremaining: 1m 48s\n",
      "438:\tlearn: 0.2322068\ttotal: 1m 24s\tremaining: 1m 47s\n",
      "439:\tlearn: 0.2317278\ttotal: 1m 24s\tremaining: 1m 47s\n",
      "440:\tlearn: 0.2314508\ttotal: 1m 24s\tremaining: 1m 47s\n",
      "441:\tlearn: 0.2310493\ttotal: 1m 24s\tremaining: 1m 47s\n",
      "442:\tlearn: 0.2307537\ttotal: 1m 25s\tremaining: 1m 46s\n",
      "443:\tlearn: 0.2303596\ttotal: 1m 25s\tremaining: 1m 46s\n",
      "444:\tlearn: 0.2299946\ttotal: 1m 25s\tremaining: 1m 46s\n",
      "445:\tlearn: 0.2297282\ttotal: 1m 25s\tremaining: 1m 46s\n",
      "446:\tlearn: 0.2293248\ttotal: 1m 25s\tremaining: 1m 46s\n",
      "447:\tlearn: 0.2288643\ttotal: 1m 25s\tremaining: 1m 45s\n",
      "448:\tlearn: 0.2284947\ttotal: 1m 26s\tremaining: 1m 45s\n",
      "449:\tlearn: 0.2280936\ttotal: 1m 26s\tremaining: 1m 45s\n",
      "450:\tlearn: 0.2278446\ttotal: 1m 26s\tremaining: 1m 45s\n",
      "451:\tlearn: 0.2273601\ttotal: 1m 26s\tremaining: 1m 45s\n",
      "452:\tlearn: 0.2270157\ttotal: 1m 26s\tremaining: 1m 44s\n",
      "453:\tlearn: 0.2265546\ttotal: 1m 26s\tremaining: 1m 44s\n",
      "454:\tlearn: 0.2262513\ttotal: 1m 27s\tremaining: 1m 44s\n",
      "455:\tlearn: 0.2257962\ttotal: 1m 27s\tremaining: 1m 44s\n",
      "456:\tlearn: 0.2254437\ttotal: 1m 27s\tremaining: 1m 43s\n",
      "457:\tlearn: 0.2251178\ttotal: 1m 27s\tremaining: 1m 43s\n",
      "458:\tlearn: 0.2247750\ttotal: 1m 27s\tremaining: 1m 43s\n",
      "459:\tlearn: 0.2243902\ttotal: 1m 27s\tremaining: 1m 43s\n",
      "460:\tlearn: 0.2239821\ttotal: 1m 28s\tremaining: 1m 43s\n",
      "461:\tlearn: 0.2237013\ttotal: 1m 28s\tremaining: 1m 42s\n",
      "462:\tlearn: 0.2234276\ttotal: 1m 28s\tremaining: 1m 42s\n",
      "463:\tlearn: 0.2231294\ttotal: 1m 28s\tremaining: 1m 42s\n",
      "464:\tlearn: 0.2227850\ttotal: 1m 28s\tremaining: 1m 42s\n",
      "465:\tlearn: 0.2224425\ttotal: 1m 29s\tremaining: 1m 42s\n",
      "466:\tlearn: 0.2221219\ttotal: 1m 29s\tremaining: 1m 41s\n",
      "467:\tlearn: 0.2218260\ttotal: 1m 29s\tremaining: 1m 41s\n",
      "468:\tlearn: 0.2213065\ttotal: 1m 29s\tremaining: 1m 41s\n",
      "469:\tlearn: 0.2209299\ttotal: 1m 29s\tremaining: 1m 41s\n",
      "470:\tlearn: 0.2205876\ttotal: 1m 29s\tremaining: 1m 40s\n",
      "471:\tlearn: 0.2202957\ttotal: 1m 30s\tremaining: 1m 40s\n",
      "472:\tlearn: 0.2201034\ttotal: 1m 30s\tremaining: 1m 40s\n",
      "473:\tlearn: 0.2199459\ttotal: 1m 30s\tremaining: 1m 40s\n",
      "474:\tlearn: 0.2197498\ttotal: 1m 30s\tremaining: 1m 40s\n",
      "475:\tlearn: 0.2194629\ttotal: 1m 30s\tremaining: 1m 39s\n",
      "476:\tlearn: 0.2191873\ttotal: 1m 30s\tremaining: 1m 39s\n",
      "477:\tlearn: 0.2189598\ttotal: 1m 30s\tremaining: 1m 39s\n",
      "478:\tlearn: 0.2185234\ttotal: 1m 31s\tremaining: 1m 39s\n",
      "479:\tlearn: 0.2181343\ttotal: 1m 31s\tremaining: 1m 38s\n",
      "480:\tlearn: 0.2177711\ttotal: 1m 31s\tremaining: 1m 38s\n",
      "481:\tlearn: 0.2174325\ttotal: 1m 31s\tremaining: 1m 38s\n",
      "482:\tlearn: 0.2169870\ttotal: 1m 31s\tremaining: 1m 38s\n",
      "483:\tlearn: 0.2166396\ttotal: 1m 31s\tremaining: 1m 38s\n",
      "484:\tlearn: 0.2164233\ttotal: 1m 32s\tremaining: 1m 37s\n",
      "485:\tlearn: 0.2161826\ttotal: 1m 32s\tremaining: 1m 37s\n",
      "486:\tlearn: 0.2157723\ttotal: 1m 32s\tremaining: 1m 37s\n",
      "487:\tlearn: 0.2155874\ttotal: 1m 32s\tremaining: 1m 37s\n",
      "488:\tlearn: 0.2152879\ttotal: 1m 32s\tremaining: 1m 37s\n",
      "489:\tlearn: 0.2150364\ttotal: 1m 33s\tremaining: 1m 36s\n",
      "490:\tlearn: 0.2146655\ttotal: 1m 33s\tremaining: 1m 36s\n",
      "491:\tlearn: 0.2143964\ttotal: 1m 33s\tremaining: 1m 36s\n",
      "492:\tlearn: 0.2142041\ttotal: 1m 33s\tremaining: 1m 36s\n",
      "493:\tlearn: 0.2140003\ttotal: 1m 33s\tremaining: 1m 36s\n",
      "494:\tlearn: 0.2136979\ttotal: 1m 33s\tremaining: 1m 35s\n",
      "495:\tlearn: 0.2132662\ttotal: 1m 34s\tremaining: 1m 35s\n",
      "496:\tlearn: 0.2130180\ttotal: 1m 34s\tremaining: 1m 35s\n",
      "497:\tlearn: 0.2127462\ttotal: 1m 34s\tremaining: 1m 35s\n",
      "498:\tlearn: 0.2125205\ttotal: 1m 34s\tremaining: 1m 34s\n",
      "499:\tlearn: 0.2121760\ttotal: 1m 34s\tremaining: 1m 34s\n",
      "500:\tlearn: 0.2119157\ttotal: 1m 34s\tremaining: 1m 34s\n",
      "501:\tlearn: 0.2115232\ttotal: 1m 35s\tremaining: 1m 34s\n",
      "502:\tlearn: 0.2112475\ttotal: 1m 35s\tremaining: 1m 34s\n",
      "503:\tlearn: 0.2109681\ttotal: 1m 35s\tremaining: 1m 33s\n",
      "504:\tlearn: 0.2107264\ttotal: 1m 35s\tremaining: 1m 33s\n",
      "505:\tlearn: 0.2104044\ttotal: 1m 35s\tremaining: 1m 33s\n",
      "506:\tlearn: 0.2100434\ttotal: 1m 35s\tremaining: 1m 33s\n",
      "507:\tlearn: 0.2098287\ttotal: 1m 36s\tremaining: 1m 33s\n",
      "508:\tlearn: 0.2095261\ttotal: 1m 36s\tremaining: 1m 32s\n",
      "509:\tlearn: 0.2092188\ttotal: 1m 36s\tremaining: 1m 32s\n",
      "510:\tlearn: 0.2088052\ttotal: 1m 36s\tremaining: 1m 32s\n",
      "511:\tlearn: 0.2085902\ttotal: 1m 36s\tremaining: 1m 32s\n",
      "512:\tlearn: 0.2084551\ttotal: 1m 36s\tremaining: 1m 32s\n",
      "513:\tlearn: 0.2081731\ttotal: 1m 37s\tremaining: 1m 31s\n",
      "514:\tlearn: 0.2078790\ttotal: 1m 37s\tremaining: 1m 31s\n",
      "515:\tlearn: 0.2074807\ttotal: 1m 37s\tremaining: 1m 31s\n",
      "516:\tlearn: 0.2072102\ttotal: 1m 37s\tremaining: 1m 31s\n",
      "517:\tlearn: 0.2070317\ttotal: 1m 37s\tremaining: 1m 30s\n",
      "518:\tlearn: 0.2068269\ttotal: 1m 37s\tremaining: 1m 30s\n",
      "519:\tlearn: 0.2066254\ttotal: 1m 38s\tremaining: 1m 30s\n",
      "520:\tlearn: 0.2063752\ttotal: 1m 38s\tremaining: 1m 30s\n",
      "521:\tlearn: 0.2061896\ttotal: 1m 38s\tremaining: 1m 30s\n",
      "522:\tlearn: 0.2058364\ttotal: 1m 38s\tremaining: 1m 30s\n",
      "523:\tlearn: 0.2055334\ttotal: 1m 38s\tremaining: 1m 29s\n",
      "524:\tlearn: 0.2053198\ttotal: 1m 39s\tremaining: 1m 29s\n",
      "525:\tlearn: 0.2051525\ttotal: 1m 39s\tremaining: 1m 29s\n",
      "526:\tlearn: 0.2048240\ttotal: 1m 39s\tremaining: 1m 29s\n",
      "527:\tlearn: 0.2044466\ttotal: 1m 39s\tremaining: 1m 28s\n",
      "528:\tlearn: 0.2042007\ttotal: 1m 39s\tremaining: 1m 28s\n",
      "529:\tlearn: 0.2039594\ttotal: 1m 39s\tremaining: 1m 28s\n",
      "530:\tlearn: 0.2037939\ttotal: 1m 40s\tremaining: 1m 28s\n",
      "531:\tlearn: 0.2035839\ttotal: 1m 40s\tremaining: 1m 28s\n",
      "532:\tlearn: 0.2033827\ttotal: 1m 40s\tremaining: 1m 27s\n",
      "533:\tlearn: 0.2031267\ttotal: 1m 40s\tremaining: 1m 27s\n",
      "534:\tlearn: 0.2029539\ttotal: 1m 40s\tremaining: 1m 27s\n",
      "535:\tlearn: 0.2028064\ttotal: 1m 40s\tremaining: 1m 27s\n",
      "536:\tlearn: 0.2026123\ttotal: 1m 41s\tremaining: 1m 27s\n",
      "537:\tlearn: 0.2024564\ttotal: 1m 41s\tremaining: 1m 26s\n",
      "538:\tlearn: 0.2020590\ttotal: 1m 41s\tremaining: 1m 26s\n",
      "539:\tlearn: 0.2018981\ttotal: 1m 41s\tremaining: 1m 26s\n",
      "540:\tlearn: 0.2016796\ttotal: 1m 41s\tremaining: 1m 26s\n",
      "541:\tlearn: 0.2014692\ttotal: 1m 41s\tremaining: 1m 26s\n",
      "542:\tlearn: 0.2011810\ttotal: 1m 42s\tremaining: 1m 25s\n",
      "543:\tlearn: 0.2008982\ttotal: 1m 42s\tremaining: 1m 25s\n",
      "544:\tlearn: 0.2005832\ttotal: 1m 42s\tremaining: 1m 25s\n",
      "545:\tlearn: 0.2004216\ttotal: 1m 42s\tremaining: 1m 25s\n",
      "546:\tlearn: 0.2002772\ttotal: 1m 42s\tremaining: 1m 25s\n",
      "547:\tlearn: 0.2000002\ttotal: 1m 42s\tremaining: 1m 24s\n",
      "548:\tlearn: 0.1996534\ttotal: 1m 43s\tremaining: 1m 24s\n",
      "549:\tlearn: 0.1994983\ttotal: 1m 43s\tremaining: 1m 24s\n",
      "550:\tlearn: 0.1993188\ttotal: 1m 43s\tremaining: 1m 24s\n",
      "551:\tlearn: 0.1991945\ttotal: 1m 43s\tremaining: 1m 24s\n",
      "552:\tlearn: 0.1990159\ttotal: 1m 43s\tremaining: 1m 23s\n",
      "553:\tlearn: 0.1987856\ttotal: 1m 43s\tremaining: 1m 23s\n",
      "554:\tlearn: 0.1984958\ttotal: 1m 44s\tremaining: 1m 23s\n",
      "555:\tlearn: 0.1982664\ttotal: 1m 44s\tremaining: 1m 23s\n",
      "556:\tlearn: 0.1979995\ttotal: 1m 44s\tremaining: 1m 22s\n",
      "557:\tlearn: 0.1976728\ttotal: 1m 44s\tremaining: 1m 22s\n",
      "558:\tlearn: 0.1974965\ttotal: 1m 44s\tremaining: 1m 22s\n",
      "559:\tlearn: 0.1972805\ttotal: 1m 44s\tremaining: 1m 22s\n",
      "560:\tlearn: 0.1969785\ttotal: 1m 45s\tremaining: 1m 22s\n",
      "561:\tlearn: 0.1967009\ttotal: 1m 45s\tremaining: 1m 21s\n",
      "562:\tlearn: 0.1965104\ttotal: 1m 45s\tremaining: 1m 21s\n",
      "563:\tlearn: 0.1963739\ttotal: 1m 45s\tremaining: 1m 21s\n",
      "564:\tlearn: 0.1960251\ttotal: 1m 45s\tremaining: 1m 21s\n",
      "565:\tlearn: 0.1957773\ttotal: 1m 45s\tremaining: 1m 21s\n",
      "566:\tlearn: 0.1956467\ttotal: 1m 46s\tremaining: 1m 20s\n",
      "567:\tlearn: 0.1953772\ttotal: 1m 46s\tremaining: 1m 20s\n",
      "568:\tlearn: 0.1951454\ttotal: 1m 46s\tremaining: 1m 20s\n",
      "569:\tlearn: 0.1949755\ttotal: 1m 46s\tremaining: 1m 20s\n",
      "570:\tlearn: 0.1947749\ttotal: 1m 46s\tremaining: 1m 20s\n",
      "571:\tlearn: 0.1946059\ttotal: 1m 46s\tremaining: 1m 19s\n",
      "572:\tlearn: 0.1944351\ttotal: 1m 47s\tremaining: 1m 19s\n",
      "573:\tlearn: 0.1941323\ttotal: 1m 47s\tremaining: 1m 19s\n",
      "574:\tlearn: 0.1938068\ttotal: 1m 47s\tremaining: 1m 19s\n",
      "575:\tlearn: 0.1936078\ttotal: 1m 47s\tremaining: 1m 19s\n",
      "576:\tlearn: 0.1934303\ttotal: 1m 47s\tremaining: 1m 18s\n",
      "577:\tlearn: 0.1931238\ttotal: 1m 47s\tremaining: 1m 18s\n",
      "578:\tlearn: 0.1928224\ttotal: 1m 48s\tremaining: 1m 18s\n",
      "579:\tlearn: 0.1926795\ttotal: 1m 48s\tremaining: 1m 18s\n",
      "580:\tlearn: 0.1924791\ttotal: 1m 48s\tremaining: 1m 18s\n",
      "581:\tlearn: 0.1922871\ttotal: 1m 48s\tremaining: 1m 17s\n",
      "582:\tlearn: 0.1920780\ttotal: 1m 48s\tremaining: 1m 17s\n",
      "583:\tlearn: 0.1917026\ttotal: 1m 48s\tremaining: 1m 17s\n",
      "584:\tlearn: 0.1914681\ttotal: 1m 49s\tremaining: 1m 17s\n",
      "585:\tlearn: 0.1912648\ttotal: 1m 49s\tremaining: 1m 17s\n",
      "586:\tlearn: 0.1910954\ttotal: 1m 49s\tremaining: 1m 17s\n",
      "587:\tlearn: 0.1908170\ttotal: 1m 49s\tremaining: 1m 16s\n",
      "588:\tlearn: 0.1905914\ttotal: 1m 49s\tremaining: 1m 16s\n",
      "589:\tlearn: 0.1904597\ttotal: 1m 49s\tremaining: 1m 16s\n",
      "590:\tlearn: 0.1902957\ttotal: 1m 50s\tremaining: 1m 16s\n",
      "591:\tlearn: 0.1900277\ttotal: 1m 50s\tremaining: 1m 15s\n",
      "592:\tlearn: 0.1897399\ttotal: 1m 50s\tremaining: 1m 15s\n",
      "593:\tlearn: 0.1895218\ttotal: 1m 50s\tremaining: 1m 15s\n",
      "594:\tlearn: 0.1892782\ttotal: 1m 50s\tremaining: 1m 15s\n",
      "595:\tlearn: 0.1891422\ttotal: 1m 50s\tremaining: 1m 15s\n",
      "596:\tlearn: 0.1890115\ttotal: 1m 51s\tremaining: 1m 14s\n",
      "597:\tlearn: 0.1888043\ttotal: 1m 51s\tremaining: 1m 14s\n",
      "598:\tlearn: 0.1885124\ttotal: 1m 51s\tremaining: 1m 14s\n",
      "599:\tlearn: 0.1883884\ttotal: 1m 51s\tremaining: 1m 14s\n",
      "600:\tlearn: 0.1882677\ttotal: 1m 51s\tremaining: 1m 14s\n",
      "601:\tlearn: 0.1879690\ttotal: 1m 51s\tremaining: 1m 13s\n",
      "602:\tlearn: 0.1877875\ttotal: 1m 52s\tremaining: 1m 13s\n",
      "603:\tlearn: 0.1876255\ttotal: 1m 52s\tremaining: 1m 13s\n",
      "604:\tlearn: 0.1874227\ttotal: 1m 52s\tremaining: 1m 13s\n",
      "605:\tlearn: 0.1871351\ttotal: 1m 52s\tremaining: 1m 13s\n",
      "606:\tlearn: 0.1870057\ttotal: 1m 52s\tremaining: 1m 13s\n",
      "607:\tlearn: 0.1868494\ttotal: 1m 52s\tremaining: 1m 12s\n",
      "608:\tlearn: 0.1867314\ttotal: 1m 53s\tremaining: 1m 12s\n",
      "609:\tlearn: 0.1865536\ttotal: 1m 53s\tremaining: 1m 12s\n",
      "610:\tlearn: 0.1863523\ttotal: 1m 53s\tremaining: 1m 12s\n",
      "611:\tlearn: 0.1862159\ttotal: 1m 53s\tremaining: 1m 12s\n",
      "612:\tlearn: 0.1860485\ttotal: 1m 53s\tremaining: 1m 11s\n",
      "613:\tlearn: 0.1859328\ttotal: 1m 53s\tremaining: 1m 11s\n",
      "614:\tlearn: 0.1858062\ttotal: 1m 54s\tremaining: 1m 11s\n",
      "615:\tlearn: 0.1856726\ttotal: 1m 54s\tremaining: 1m 11s\n",
      "616:\tlearn: 0.1855210\ttotal: 1m 54s\tremaining: 1m 11s\n",
      "617:\tlearn: 0.1852621\ttotal: 1m 54s\tremaining: 1m 10s\n",
      "618:\tlearn: 0.1850387\ttotal: 1m 54s\tremaining: 1m 10s\n",
      "619:\tlearn: 0.1848858\ttotal: 1m 54s\tremaining: 1m 10s\n",
      "620:\tlearn: 0.1848065\ttotal: 1m 55s\tremaining: 1m 10s\n",
      "621:\tlearn: 0.1846625\ttotal: 1m 55s\tremaining: 1m 10s\n",
      "622:\tlearn: 0.1845084\ttotal: 1m 55s\tremaining: 1m 9s\n",
      "623:\tlearn: 0.1843261\ttotal: 1m 55s\tremaining: 1m 9s\n",
      "624:\tlearn: 0.1841134\ttotal: 1m 55s\tremaining: 1m 9s\n",
      "625:\tlearn: 0.1839533\ttotal: 1m 55s\tremaining: 1m 9s\n",
      "626:\tlearn: 0.1837054\ttotal: 1m 56s\tremaining: 1m 9s\n",
      "627:\tlearn: 0.1835175\ttotal: 1m 56s\tremaining: 1m 8s\n",
      "628:\tlearn: 0.1833382\ttotal: 1m 56s\tremaining: 1m 8s\n",
      "629:\tlearn: 0.1831693\ttotal: 1m 56s\tremaining: 1m 8s\n",
      "630:\tlearn: 0.1830616\ttotal: 1m 56s\tremaining: 1m 8s\n",
      "631:\tlearn: 0.1829002\ttotal: 1m 56s\tremaining: 1m 8s\n",
      "632:\tlearn: 0.1827615\ttotal: 1m 56s\tremaining: 1m 7s\n",
      "633:\tlearn: 0.1826400\ttotal: 1m 57s\tremaining: 1m 7s\n",
      "634:\tlearn: 0.1823567\ttotal: 1m 57s\tremaining: 1m 7s\n",
      "635:\tlearn: 0.1822228\ttotal: 1m 57s\tremaining: 1m 7s\n",
      "636:\tlearn: 0.1820042\ttotal: 1m 57s\tremaining: 1m 7s\n",
      "637:\tlearn: 0.1817926\ttotal: 1m 57s\tremaining: 1m 6s\n",
      "638:\tlearn: 0.1816922\ttotal: 1m 57s\tremaining: 1m 6s\n",
      "639:\tlearn: 0.1814419\ttotal: 1m 58s\tremaining: 1m 6s\n",
      "640:\tlearn: 0.1812445\ttotal: 1m 58s\tremaining: 1m 6s\n",
      "641:\tlearn: 0.1811537\ttotal: 1m 58s\tremaining: 1m 6s\n",
      "642:\tlearn: 0.1809581\ttotal: 1m 58s\tremaining: 1m 5s\n",
      "643:\tlearn: 0.1807624\ttotal: 1m 58s\tremaining: 1m 5s\n",
      "644:\tlearn: 0.1805582\ttotal: 1m 58s\tremaining: 1m 5s\n",
      "645:\tlearn: 0.1802374\ttotal: 1m 59s\tremaining: 1m 5s\n",
      "646:\tlearn: 0.1801434\ttotal: 1m 59s\tremaining: 1m 5s\n",
      "647:\tlearn: 0.1799513\ttotal: 1m 59s\tremaining: 1m 4s\n",
      "648:\tlearn: 0.1797708\ttotal: 1m 59s\tremaining: 1m 4s\n",
      "649:\tlearn: 0.1795412\ttotal: 1m 59s\tremaining: 1m 4s\n",
      "650:\tlearn: 0.1794248\ttotal: 1m 59s\tremaining: 1m 4s\n",
      "651:\tlearn: 0.1793179\ttotal: 2m\tremaining: 1m 4s\n",
      "652:\tlearn: 0.1792030\ttotal: 2m\tremaining: 1m 3s\n",
      "653:\tlearn: 0.1790406\ttotal: 2m\tremaining: 1m 3s\n",
      "654:\tlearn: 0.1789019\ttotal: 2m\tremaining: 1m 3s\n",
      "655:\tlearn: 0.1785059\ttotal: 2m\tremaining: 1m 3s\n",
      "656:\tlearn: 0.1783839\ttotal: 2m\tremaining: 1m 3s\n",
      "657:\tlearn: 0.1780957\ttotal: 2m 1s\tremaining: 1m 2s\n",
      "658:\tlearn: 0.1778545\ttotal: 2m 1s\tremaining: 1m 2s\n",
      "659:\tlearn: 0.1776667\ttotal: 2m 1s\tremaining: 1m 2s\n",
      "660:\tlearn: 0.1774623\ttotal: 2m 1s\tremaining: 1m 2s\n",
      "661:\tlearn: 0.1771200\ttotal: 2m 1s\tremaining: 1m 2s\n",
      "662:\tlearn: 0.1768712\ttotal: 2m 1s\tremaining: 1m 1s\n",
      "663:\tlearn: 0.1767962\ttotal: 2m 2s\tremaining: 1m 1s\n",
      "664:\tlearn: 0.1766637\ttotal: 2m 2s\tremaining: 1m 1s\n",
      "665:\tlearn: 0.1764697\ttotal: 2m 2s\tremaining: 1m 1s\n",
      "666:\tlearn: 0.1763022\ttotal: 2m 2s\tremaining: 1m 1s\n",
      "667:\tlearn: 0.1761415\ttotal: 2m 2s\tremaining: 1m 1s\n",
      "668:\tlearn: 0.1759981\ttotal: 2m 2s\tremaining: 1m\n",
      "669:\tlearn: 0.1758109\ttotal: 2m 3s\tremaining: 1m\n",
      "670:\tlearn: 0.1755891\ttotal: 2m 3s\tremaining: 1m\n",
      "671:\tlearn: 0.1754496\ttotal: 2m 3s\tremaining: 1m\n",
      "672:\tlearn: 0.1751653\ttotal: 2m 3s\tremaining: 1m\n",
      "673:\tlearn: 0.1749744\ttotal: 2m 3s\tremaining: 59.9s\n",
      "674:\tlearn: 0.1748207\ttotal: 2m 3s\tremaining: 59.7s\n",
      "675:\tlearn: 0.1746788\ttotal: 2m 4s\tremaining: 59.5s\n",
      "676:\tlearn: 0.1746016\ttotal: 2m 4s\tremaining: 59.3s\n",
      "677:\tlearn: 0.1743772\ttotal: 2m 4s\tremaining: 59.1s\n",
      "678:\tlearn: 0.1742764\ttotal: 2m 4s\tremaining: 58.9s\n",
      "679:\tlearn: 0.1741418\ttotal: 2m 4s\tremaining: 58.7s\n",
      "680:\tlearn: 0.1740741\ttotal: 2m 4s\tremaining: 58.5s\n",
      "681:\tlearn: 0.1739730\ttotal: 2m 5s\tremaining: 58.4s\n",
      "682:\tlearn: 0.1737572\ttotal: 2m 5s\tremaining: 58.2s\n",
      "683:\tlearn: 0.1736029\ttotal: 2m 5s\tremaining: 58s\n",
      "684:\tlearn: 0.1734533\ttotal: 2m 5s\tremaining: 57.8s\n",
      "685:\tlearn: 0.1732719\ttotal: 2m 5s\tremaining: 57.6s\n",
      "686:\tlearn: 0.1731603\ttotal: 2m 5s\tremaining: 57.4s\n",
      "687:\tlearn: 0.1730381\ttotal: 2m 6s\tremaining: 57.2s\n",
      "688:\tlearn: 0.1729382\ttotal: 2m 6s\tremaining: 57s\n",
      "689:\tlearn: 0.1727887\ttotal: 2m 6s\tremaining: 56.8s\n",
      "690:\tlearn: 0.1726820\ttotal: 2m 6s\tremaining: 56.6s\n",
      "691:\tlearn: 0.1725497\ttotal: 2m 6s\tremaining: 56.4s\n",
      "692:\tlearn: 0.1722636\ttotal: 2m 6s\tremaining: 56.2s\n",
      "693:\tlearn: 0.1721118\ttotal: 2m 7s\tremaining: 56.1s\n",
      "694:\tlearn: 0.1719835\ttotal: 2m 7s\tremaining: 55.9s\n",
      "695:\tlearn: 0.1717583\ttotal: 2m 7s\tremaining: 55.7s\n",
      "696:\tlearn: 0.1715883\ttotal: 2m 7s\tremaining: 55.5s\n",
      "697:\tlearn: 0.1714740\ttotal: 2m 7s\tremaining: 55.3s\n",
      "698:\tlearn: 0.1713135\ttotal: 2m 8s\tremaining: 55.1s\n",
      "699:\tlearn: 0.1712276\ttotal: 2m 8s\tremaining: 54.9s\n",
      "700:\tlearn: 0.1710813\ttotal: 2m 8s\tremaining: 54.7s\n",
      "701:\tlearn: 0.1709733\ttotal: 2m 8s\tremaining: 54.6s\n",
      "702:\tlearn: 0.1708782\ttotal: 2m 8s\tremaining: 54.4s\n",
      "703:\tlearn: 0.1705514\ttotal: 2m 8s\tremaining: 54.2s\n",
      "704:\tlearn: 0.1703956\ttotal: 2m 9s\tremaining: 54s\n",
      "705:\tlearn: 0.1703076\ttotal: 2m 9s\tremaining: 53.8s\n",
      "706:\tlearn: 0.1701280\ttotal: 2m 9s\tremaining: 53.6s\n",
      "707:\tlearn: 0.1699850\ttotal: 2m 9s\tremaining: 53.4s\n",
      "708:\tlearn: 0.1698225\ttotal: 2m 9s\tremaining: 53.2s\n",
      "709:\tlearn: 0.1697301\ttotal: 2m 9s\tremaining: 53.1s\n",
      "710:\tlearn: 0.1694971\ttotal: 2m 10s\tremaining: 52.9s\n",
      "711:\tlearn: 0.1693634\ttotal: 2m 10s\tremaining: 52.7s\n",
      "712:\tlearn: 0.1692736\ttotal: 2m 10s\tremaining: 52.5s\n",
      "713:\tlearn: 0.1690151\ttotal: 2m 10s\tremaining: 52.3s\n",
      "714:\tlearn: 0.1688808\ttotal: 2m 10s\tremaining: 52.1s\n",
      "715:\tlearn: 0.1687802\ttotal: 2m 10s\tremaining: 51.9s\n",
      "716:\tlearn: 0.1687156\ttotal: 2m 11s\tremaining: 51.7s\n",
      "717:\tlearn: 0.1685231\ttotal: 2m 11s\tremaining: 51.5s\n",
      "718:\tlearn: 0.1683450\ttotal: 2m 11s\tremaining: 51.3s\n",
      "719:\tlearn: 0.1681940\ttotal: 2m 11s\tremaining: 51.2s\n",
      "720:\tlearn: 0.1680680\ttotal: 2m 11s\tremaining: 51s\n",
      "721:\tlearn: 0.1678206\ttotal: 2m 11s\tremaining: 50.8s\n",
      "722:\tlearn: 0.1676444\ttotal: 2m 12s\tremaining: 50.6s\n",
      "723:\tlearn: 0.1674190\ttotal: 2m 12s\tremaining: 50.4s\n",
      "724:\tlearn: 0.1673475\ttotal: 2m 12s\tremaining: 50.2s\n",
      "725:\tlearn: 0.1671546\ttotal: 2m 12s\tremaining: 50s\n",
      "726:\tlearn: 0.1670554\ttotal: 2m 12s\tremaining: 49.8s\n",
      "727:\tlearn: 0.1669637\ttotal: 2m 12s\tremaining: 49.6s\n",
      "728:\tlearn: 0.1668115\ttotal: 2m 13s\tremaining: 49.4s\n",
      "729:\tlearn: 0.1667223\ttotal: 2m 13s\tremaining: 49.3s\n",
      "730:\tlearn: 0.1666011\ttotal: 2m 13s\tremaining: 49.1s\n",
      "731:\tlearn: 0.1664848\ttotal: 2m 13s\tremaining: 48.9s\n",
      "732:\tlearn: 0.1663584\ttotal: 2m 13s\tremaining: 48.7s\n",
      "733:\tlearn: 0.1661703\ttotal: 2m 13s\tremaining: 48.5s\n",
      "734:\tlearn: 0.1659838\ttotal: 2m 14s\tremaining: 48.3s\n",
      "735:\tlearn: 0.1657907\ttotal: 2m 14s\tremaining: 48.1s\n",
      "736:\tlearn: 0.1656827\ttotal: 2m 14s\tremaining: 47.9s\n",
      "737:\tlearn: 0.1655724\ttotal: 2m 14s\tremaining: 47.7s\n",
      "738:\tlearn: 0.1654673\ttotal: 2m 14s\tremaining: 47.6s\n",
      "739:\tlearn: 0.1653536\ttotal: 2m 14s\tremaining: 47.4s\n",
      "740:\tlearn: 0.1651996\ttotal: 2m 14s\tremaining: 47.2s\n",
      "741:\tlearn: 0.1650627\ttotal: 2m 15s\tremaining: 47s\n",
      "742:\tlearn: 0.1647824\ttotal: 2m 15s\tremaining: 46.8s\n",
      "743:\tlearn: 0.1646603\ttotal: 2m 15s\tremaining: 46.6s\n",
      "744:\tlearn: 0.1643632\ttotal: 2m 15s\tremaining: 46.4s\n",
      "745:\tlearn: 0.1642251\ttotal: 2m 15s\tremaining: 46.2s\n",
      "746:\tlearn: 0.1641176\ttotal: 2m 15s\tremaining: 46s\n",
      "747:\tlearn: 0.1639563\ttotal: 2m 16s\tremaining: 45.8s\n",
      "748:\tlearn: 0.1638335\ttotal: 2m 16s\tremaining: 45.7s\n",
      "749:\tlearn: 0.1636992\ttotal: 2m 16s\tremaining: 45.5s\n",
      "750:\tlearn: 0.1636406\ttotal: 2m 16s\tremaining: 45.3s\n",
      "751:\tlearn: 0.1635969\ttotal: 2m 16s\tremaining: 45.1s\n",
      "752:\tlearn: 0.1635436\ttotal: 2m 16s\tremaining: 44.9s\n",
      "753:\tlearn: 0.1634349\ttotal: 2m 17s\tremaining: 44.7s\n",
      "754:\tlearn: 0.1632849\ttotal: 2m 17s\tremaining: 44.5s\n",
      "755:\tlearn: 0.1632104\ttotal: 2m 17s\tremaining: 44.3s\n",
      "756:\tlearn: 0.1631247\ttotal: 2m 17s\tremaining: 44.1s\n",
      "757:\tlearn: 0.1630504\ttotal: 2m 17s\tremaining: 44s\n",
      "758:\tlearn: 0.1629340\ttotal: 2m 17s\tremaining: 43.8s\n",
      "759:\tlearn: 0.1626777\ttotal: 2m 18s\tremaining: 43.6s\n",
      "760:\tlearn: 0.1625542\ttotal: 2m 18s\tremaining: 43.4s\n",
      "761:\tlearn: 0.1624263\ttotal: 2m 18s\tremaining: 43.2s\n",
      "762:\tlearn: 0.1623259\ttotal: 2m 18s\tremaining: 43s\n",
      "763:\tlearn: 0.1622146\ttotal: 2m 18s\tremaining: 42.8s\n",
      "764:\tlearn: 0.1621052\ttotal: 2m 18s\tremaining: 42.6s\n",
      "765:\tlearn: 0.1619047\ttotal: 2m 19s\tremaining: 42.5s\n",
      "766:\tlearn: 0.1617618\ttotal: 2m 19s\tremaining: 42.3s\n",
      "767:\tlearn: 0.1616136\ttotal: 2m 19s\tremaining: 42.1s\n",
      "768:\tlearn: 0.1615110\ttotal: 2m 19s\tremaining: 41.9s\n",
      "769:\tlearn: 0.1614421\ttotal: 2m 19s\tremaining: 41.7s\n",
      "770:\tlearn: 0.1612609\ttotal: 2m 19s\tremaining: 41.5s\n",
      "771:\tlearn: 0.1611387\ttotal: 2m 20s\tremaining: 41.4s\n",
      "772:\tlearn: 0.1610612\ttotal: 2m 20s\tremaining: 41.2s\n",
      "773:\tlearn: 0.1609593\ttotal: 2m 20s\tremaining: 41s\n",
      "774:\tlearn: 0.1609025\ttotal: 2m 20s\tremaining: 40.8s\n",
      "775:\tlearn: 0.1607632\ttotal: 2m 20s\tremaining: 40.6s\n",
      "776:\tlearn: 0.1606372\ttotal: 2m 20s\tremaining: 40.4s\n",
      "777:\tlearn: 0.1605270\ttotal: 2m 21s\tremaining: 40.2s\n",
      "778:\tlearn: 0.1604214\ttotal: 2m 21s\tremaining: 40.1s\n",
      "779:\tlearn: 0.1603225\ttotal: 2m 21s\tremaining: 39.9s\n",
      "780:\tlearn: 0.1602453\ttotal: 2m 21s\tremaining: 39.7s\n",
      "781:\tlearn: 0.1600672\ttotal: 2m 21s\tremaining: 39.5s\n",
      "782:\tlearn: 0.1599681\ttotal: 2m 21s\tremaining: 39.3s\n",
      "783:\tlearn: 0.1598880\ttotal: 2m 22s\tremaining: 39.1s\n",
      "784:\tlearn: 0.1598104\ttotal: 2m 22s\tremaining: 38.9s\n",
      "785:\tlearn: 0.1597203\ttotal: 2m 22s\tremaining: 38.8s\n",
      "786:\tlearn: 0.1595762\ttotal: 2m 22s\tremaining: 38.6s\n",
      "787:\tlearn: 0.1593541\ttotal: 2m 22s\tremaining: 38.4s\n",
      "788:\tlearn: 0.1592195\ttotal: 2m 22s\tremaining: 38.2s\n",
      "789:\tlearn: 0.1591536\ttotal: 2m 22s\tremaining: 38s\n",
      "790:\tlearn: 0.1590213\ttotal: 2m 23s\tremaining: 37.8s\n",
      "791:\tlearn: 0.1589454\ttotal: 2m 23s\tremaining: 37.6s\n",
      "792:\tlearn: 0.1588566\ttotal: 2m 23s\tremaining: 37.5s\n",
      "793:\tlearn: 0.1587734\ttotal: 2m 23s\tremaining: 37.3s\n",
      "794:\tlearn: 0.1586881\ttotal: 2m 23s\tremaining: 37.1s\n",
      "795:\tlearn: 0.1586114\ttotal: 2m 24s\tremaining: 36.9s\n",
      "796:\tlearn: 0.1584960\ttotal: 2m 24s\tremaining: 36.7s\n",
      "797:\tlearn: 0.1583854\ttotal: 2m 24s\tremaining: 36.5s\n",
      "798:\tlearn: 0.1582870\ttotal: 2m 24s\tremaining: 36.3s\n",
      "799:\tlearn: 0.1582098\ttotal: 2m 24s\tremaining: 36.2s\n",
      "800:\tlearn: 0.1580997\ttotal: 2m 24s\tremaining: 36s\n",
      "801:\tlearn: 0.1580336\ttotal: 2m 24s\tremaining: 35.8s\n",
      "802:\tlearn: 0.1579192\ttotal: 2m 25s\tremaining: 35.6s\n",
      "803:\tlearn: 0.1578662\ttotal: 2m 25s\tremaining: 35.4s\n",
      "804:\tlearn: 0.1578263\ttotal: 2m 25s\tremaining: 35.2s\n",
      "805:\tlearn: 0.1577570\ttotal: 2m 25s\tremaining: 35s\n",
      "806:\tlearn: 0.1575085\ttotal: 2m 25s\tremaining: 34.9s\n",
      "807:\tlearn: 0.1574157\ttotal: 2m 25s\tremaining: 34.7s\n",
      "808:\tlearn: 0.1572930\ttotal: 2m 26s\tremaining: 34.5s\n",
      "809:\tlearn: 0.1571739\ttotal: 2m 26s\tremaining: 34.3s\n",
      "810:\tlearn: 0.1570975\ttotal: 2m 26s\tremaining: 34.1s\n",
      "811:\tlearn: 0.1568519\ttotal: 2m 26s\tremaining: 33.9s\n",
      "812:\tlearn: 0.1568042\ttotal: 2m 26s\tremaining: 33.7s\n",
      "813:\tlearn: 0.1567182\ttotal: 2m 26s\tremaining: 33.6s\n",
      "814:\tlearn: 0.1566695\ttotal: 2m 27s\tremaining: 33.4s\n",
      "815:\tlearn: 0.1565962\ttotal: 2m 27s\tremaining: 33.2s\n",
      "816:\tlearn: 0.1565582\ttotal: 2m 27s\tremaining: 33s\n",
      "817:\tlearn: 0.1563245\ttotal: 2m 27s\tremaining: 32.8s\n",
      "818:\tlearn: 0.1561548\ttotal: 2m 27s\tremaining: 32.7s\n",
      "819:\tlearn: 0.1560784\ttotal: 2m 27s\tremaining: 32.5s\n",
      "820:\tlearn: 0.1559739\ttotal: 2m 28s\tremaining: 32.3s\n",
      "821:\tlearn: 0.1559025\ttotal: 2m 28s\tremaining: 32.1s\n",
      "822:\tlearn: 0.1556998\ttotal: 2m 28s\tremaining: 31.9s\n",
      "823:\tlearn: 0.1555711\ttotal: 2m 28s\tremaining: 31.7s\n",
      "824:\tlearn: 0.1554899\ttotal: 2m 28s\tremaining: 31.6s\n",
      "825:\tlearn: 0.1554209\ttotal: 2m 28s\tremaining: 31.4s\n",
      "826:\tlearn: 0.1552901\ttotal: 2m 29s\tremaining: 31.2s\n",
      "827:\tlearn: 0.1551539\ttotal: 2m 29s\tremaining: 31s\n",
      "828:\tlearn: 0.1550877\ttotal: 2m 29s\tremaining: 30.8s\n",
      "829:\tlearn: 0.1550541\ttotal: 2m 29s\tremaining: 30.6s\n",
      "830:\tlearn: 0.1549957\ttotal: 2m 29s\tremaining: 30.5s\n",
      "831:\tlearn: 0.1548951\ttotal: 2m 29s\tremaining: 30.3s\n",
      "832:\tlearn: 0.1547696\ttotal: 2m 30s\tremaining: 30.1s\n",
      "833:\tlearn: 0.1547098\ttotal: 2m 30s\tremaining: 29.9s\n",
      "834:\tlearn: 0.1545632\ttotal: 2m 30s\tremaining: 29.7s\n",
      "835:\tlearn: 0.1544890\ttotal: 2m 30s\tremaining: 29.5s\n",
      "836:\tlearn: 0.1544027\ttotal: 2m 30s\tremaining: 29.4s\n",
      "837:\tlearn: 0.1543140\ttotal: 2m 30s\tremaining: 29.2s\n",
      "838:\tlearn: 0.1542082\ttotal: 2m 31s\tremaining: 29s\n",
      "839:\tlearn: 0.1540603\ttotal: 2m 31s\tremaining: 28.8s\n",
      "840:\tlearn: 0.1539273\ttotal: 2m 31s\tremaining: 28.6s\n",
      "841:\tlearn: 0.1538225\ttotal: 2m 31s\tremaining: 28.4s\n",
      "842:\tlearn: 0.1537458\ttotal: 2m 31s\tremaining: 28.3s\n",
      "843:\tlearn: 0.1536376\ttotal: 2m 31s\tremaining: 28.1s\n",
      "844:\tlearn: 0.1534903\ttotal: 2m 32s\tremaining: 27.9s\n",
      "845:\tlearn: 0.1534496\ttotal: 2m 32s\tremaining: 27.7s\n",
      "846:\tlearn: 0.1533665\ttotal: 2m 32s\tremaining: 27.5s\n",
      "847:\tlearn: 0.1533064\ttotal: 2m 32s\tremaining: 27.3s\n",
      "848:\tlearn: 0.1532122\ttotal: 2m 32s\tremaining: 27.2s\n",
      "849:\tlearn: 0.1531344\ttotal: 2m 32s\tremaining: 27s\n",
      "850:\tlearn: 0.1530094\ttotal: 2m 33s\tremaining: 26.8s\n",
      "851:\tlearn: 0.1528486\ttotal: 2m 33s\tremaining: 26.6s\n",
      "852:\tlearn: 0.1527081\ttotal: 2m 33s\tremaining: 26.4s\n",
      "853:\tlearn: 0.1525585\ttotal: 2m 33s\tremaining: 26.2s\n",
      "854:\tlearn: 0.1524553\ttotal: 2m 33s\tremaining: 26.1s\n",
      "855:\tlearn: 0.1522706\ttotal: 2m 33s\tremaining: 25.9s\n",
      "856:\tlearn: 0.1521409\ttotal: 2m 33s\tremaining: 25.7s\n",
      "857:\tlearn: 0.1520074\ttotal: 2m 34s\tremaining: 25.5s\n",
      "858:\tlearn: 0.1519049\ttotal: 2m 34s\tremaining: 25.3s\n",
      "859:\tlearn: 0.1517678\ttotal: 2m 34s\tremaining: 25.1s\n",
      "860:\tlearn: 0.1517125\ttotal: 2m 34s\tremaining: 25s\n",
      "861:\tlearn: 0.1516419\ttotal: 2m 34s\tremaining: 24.8s\n",
      "862:\tlearn: 0.1515402\ttotal: 2m 34s\tremaining: 24.6s\n",
      "863:\tlearn: 0.1514585\ttotal: 2m 35s\tremaining: 24.4s\n",
      "864:\tlearn: 0.1513748\ttotal: 2m 35s\tremaining: 24.2s\n",
      "865:\tlearn: 0.1513145\ttotal: 2m 35s\tremaining: 24s\n",
      "866:\tlearn: 0.1512469\ttotal: 2m 35s\tremaining: 23.9s\n",
      "867:\tlearn: 0.1511840\ttotal: 2m 35s\tremaining: 23.7s\n",
      "868:\tlearn: 0.1510929\ttotal: 2m 35s\tremaining: 23.5s\n",
      "869:\tlearn: 0.1509337\ttotal: 2m 36s\tremaining: 23.3s\n",
      "870:\tlearn: 0.1508846\ttotal: 2m 36s\tremaining: 23.1s\n",
      "871:\tlearn: 0.1507399\ttotal: 2m 36s\tremaining: 23s\n",
      "872:\tlearn: 0.1506680\ttotal: 2m 36s\tremaining: 22.8s\n",
      "873:\tlearn: 0.1505359\ttotal: 2m 36s\tremaining: 22.6s\n",
      "874:\tlearn: 0.1504661\ttotal: 2m 36s\tremaining: 22.4s\n",
      "875:\tlearn: 0.1502651\ttotal: 2m 37s\tremaining: 22.2s\n",
      "876:\tlearn: 0.1502093\ttotal: 2m 37s\tremaining: 22.1s\n",
      "877:\tlearn: 0.1501370\ttotal: 2m 37s\tremaining: 21.9s\n",
      "878:\tlearn: 0.1500871\ttotal: 2m 37s\tremaining: 21.7s\n",
      "879:\tlearn: 0.1499729\ttotal: 2m 37s\tremaining: 21.5s\n",
      "880:\tlearn: 0.1498648\ttotal: 2m 37s\tremaining: 21.3s\n",
      "881:\tlearn: 0.1497467\ttotal: 2m 38s\tremaining: 21.1s\n",
      "882:\tlearn: 0.1496871\ttotal: 2m 38s\tremaining: 21s\n",
      "883:\tlearn: 0.1496128\ttotal: 2m 38s\tremaining: 20.8s\n",
      "884:\tlearn: 0.1494854\ttotal: 2m 38s\tremaining: 20.6s\n",
      "885:\tlearn: 0.1493188\ttotal: 2m 38s\tremaining: 20.4s\n",
      "886:\tlearn: 0.1492366\ttotal: 2m 38s\tremaining: 20.2s\n",
      "887:\tlearn: 0.1491591\ttotal: 2m 39s\tremaining: 20.1s\n",
      "888:\tlearn: 0.1490915\ttotal: 2m 39s\tremaining: 19.9s\n",
      "889:\tlearn: 0.1489574\ttotal: 2m 39s\tremaining: 19.7s\n",
      "890:\tlearn: 0.1488535\ttotal: 2m 39s\tremaining: 19.5s\n",
      "891:\tlearn: 0.1486353\ttotal: 2m 39s\tremaining: 19.3s\n",
      "892:\tlearn: 0.1485647\ttotal: 2m 39s\tremaining: 19.2s\n",
      "893:\tlearn: 0.1485117\ttotal: 2m 39s\tremaining: 19s\n",
      "894:\tlearn: 0.1483653\ttotal: 2m 40s\tremaining: 18.8s\n",
      "895:\tlearn: 0.1483150\ttotal: 2m 40s\tremaining: 18.6s\n",
      "896:\tlearn: 0.1482333\ttotal: 2m 40s\tremaining: 18.4s\n",
      "897:\tlearn: 0.1481328\ttotal: 2m 40s\tremaining: 18.3s\n",
      "898:\tlearn: 0.1480515\ttotal: 2m 40s\tremaining: 18.1s\n",
      "899:\tlearn: 0.1479749\ttotal: 2m 40s\tremaining: 17.9s\n",
      "900:\tlearn: 0.1478763\ttotal: 2m 41s\tremaining: 17.7s\n",
      "901:\tlearn: 0.1477846\ttotal: 2m 41s\tremaining: 17.5s\n",
      "902:\tlearn: 0.1477388\ttotal: 2m 41s\tremaining: 17.3s\n",
      "903:\tlearn: 0.1474863\ttotal: 2m 41s\tremaining: 17.2s\n",
      "904:\tlearn: 0.1473453\ttotal: 2m 41s\tremaining: 17s\n",
      "905:\tlearn: 0.1471805\ttotal: 2m 41s\tremaining: 16.8s\n",
      "906:\tlearn: 0.1470852\ttotal: 2m 42s\tremaining: 16.6s\n",
      "907:\tlearn: 0.1469663\ttotal: 2m 42s\tremaining: 16.4s\n",
      "908:\tlearn: 0.1468649\ttotal: 2m 42s\tremaining: 16.3s\n",
      "909:\tlearn: 0.1467790\ttotal: 2m 42s\tremaining: 16.1s\n",
      "910:\tlearn: 0.1466968\ttotal: 2m 42s\tremaining: 15.9s\n",
      "911:\tlearn: 0.1465734\ttotal: 2m 43s\tremaining: 15.7s\n",
      "912:\tlearn: 0.1463914\ttotal: 2m 43s\tremaining: 15.6s\n",
      "913:\tlearn: 0.1463381\ttotal: 2m 43s\tremaining: 15.4s\n",
      "914:\tlearn: 0.1462673\ttotal: 2m 43s\tremaining: 15.2s\n",
      "915:\tlearn: 0.1461429\ttotal: 2m 43s\tremaining: 15s\n",
      "916:\tlearn: 0.1460353\ttotal: 2m 43s\tremaining: 14.8s\n",
      "917:\tlearn: 0.1459271\ttotal: 2m 44s\tremaining: 14.7s\n",
      "918:\tlearn: 0.1458173\ttotal: 2m 44s\tremaining: 14.5s\n",
      "919:\tlearn: 0.1456511\ttotal: 2m 44s\tremaining: 14.3s\n",
      "920:\tlearn: 0.1455388\ttotal: 2m 44s\tremaining: 14.1s\n",
      "921:\tlearn: 0.1454733\ttotal: 2m 44s\tremaining: 13.9s\n",
      "922:\tlearn: 0.1453626\ttotal: 2m 44s\tremaining: 13.7s\n",
      "923:\tlearn: 0.1453055\ttotal: 2m 44s\tremaining: 13.6s\n",
      "924:\tlearn: 0.1452677\ttotal: 2m 45s\tremaining: 13.4s\n",
      "925:\tlearn: 0.1451496\ttotal: 2m 45s\tremaining: 13.2s\n",
      "926:\tlearn: 0.1450731\ttotal: 2m 45s\tremaining: 13s\n",
      "927:\tlearn: 0.1449542\ttotal: 2m 45s\tremaining: 12.8s\n",
      "928:\tlearn: 0.1448143\ttotal: 2m 45s\tremaining: 12.7s\n",
      "929:\tlearn: 0.1446722\ttotal: 2m 45s\tremaining: 12.5s\n",
      "930:\tlearn: 0.1445073\ttotal: 2m 46s\tremaining: 12.3s\n",
      "931:\tlearn: 0.1444402\ttotal: 2m 46s\tremaining: 12.1s\n",
      "932:\tlearn: 0.1443922\ttotal: 2m 46s\tremaining: 11.9s\n",
      "933:\tlearn: 0.1443295\ttotal: 2m 46s\tremaining: 11.8s\n",
      "934:\tlearn: 0.1442787\ttotal: 2m 46s\tremaining: 11.6s\n",
      "935:\tlearn: 0.1442368\ttotal: 2m 46s\tremaining: 11.4s\n",
      "936:\tlearn: 0.1441727\ttotal: 2m 47s\tremaining: 11.2s\n",
      "937:\tlearn: 0.1441216\ttotal: 2m 47s\tremaining: 11.1s\n",
      "938:\tlearn: 0.1440319\ttotal: 2m 47s\tremaining: 10.9s\n",
      "939:\tlearn: 0.1439109\ttotal: 2m 47s\tremaining: 10.7s\n",
      "940:\tlearn: 0.1438082\ttotal: 2m 47s\tremaining: 10.5s\n",
      "941:\tlearn: 0.1437277\ttotal: 2m 47s\tremaining: 10.3s\n",
      "942:\tlearn: 0.1436451\ttotal: 2m 48s\tremaining: 10.2s\n",
      "943:\tlearn: 0.1435922\ttotal: 2m 48s\tremaining: 9.98s\n",
      "944:\tlearn: 0.1435085\ttotal: 2m 48s\tremaining: 9.8s\n",
      "945:\tlearn: 0.1434532\ttotal: 2m 48s\tremaining: 9.62s\n",
      "946:\tlearn: 0.1433483\ttotal: 2m 48s\tremaining: 9.44s\n",
      "947:\tlearn: 0.1432390\ttotal: 2m 48s\tremaining: 9.26s\n",
      "948:\tlearn: 0.1431435\ttotal: 2m 48s\tremaining: 9.08s\n",
      "949:\tlearn: 0.1431088\ttotal: 2m 49s\tremaining: 8.9s\n",
      "950:\tlearn: 0.1429984\ttotal: 2m 49s\tremaining: 8.72s\n",
      "951:\tlearn: 0.1428794\ttotal: 2m 49s\tremaining: 8.55s\n",
      "952:\tlearn: 0.1428050\ttotal: 2m 49s\tremaining: 8.37s\n",
      "953:\tlearn: 0.1426436\ttotal: 2m 49s\tremaining: 8.19s\n",
      "954:\tlearn: 0.1425569\ttotal: 2m 50s\tremaining: 8.01s\n",
      "955:\tlearn: 0.1424653\ttotal: 2m 50s\tremaining: 7.83s\n",
      "956:\tlearn: 0.1423683\ttotal: 2m 50s\tremaining: 7.65s\n",
      "957:\tlearn: 0.1422777\ttotal: 2m 50s\tremaining: 7.47s\n",
      "958:\tlearn: 0.1421812\ttotal: 2m 50s\tremaining: 7.3s\n",
      "959:\tlearn: 0.1420716\ttotal: 2m 50s\tremaining: 7.12s\n",
      "960:\tlearn: 0.1419960\ttotal: 2m 51s\tremaining: 6.94s\n",
      "961:\tlearn: 0.1419195\ttotal: 2m 51s\tremaining: 6.76s\n",
      "962:\tlearn: 0.1418420\ttotal: 2m 51s\tremaining: 6.58s\n",
      "963:\tlearn: 0.1417639\ttotal: 2m 51s\tremaining: 6.41s\n",
      "964:\tlearn: 0.1416946\ttotal: 2m 51s\tremaining: 6.23s\n",
      "965:\tlearn: 0.1416042\ttotal: 2m 51s\tremaining: 6.05s\n",
      "966:\tlearn: 0.1415374\ttotal: 2m 52s\tremaining: 5.87s\n",
      "967:\tlearn: 0.1414811\ttotal: 2m 52s\tremaining: 5.69s\n",
      "968:\tlearn: 0.1414235\ttotal: 2m 52s\tremaining: 5.51s\n",
      "969:\tlearn: 0.1413632\ttotal: 2m 52s\tremaining: 5.34s\n",
      "970:\tlearn: 0.1412419\ttotal: 2m 52s\tremaining: 5.16s\n",
      "971:\tlearn: 0.1411713\ttotal: 2m 52s\tremaining: 4.98s\n",
      "972:\tlearn: 0.1411114\ttotal: 2m 53s\tremaining: 4.8s\n",
      "973:\tlearn: 0.1409606\ttotal: 2m 53s\tremaining: 4.62s\n",
      "974:\tlearn: 0.1408347\ttotal: 2m 53s\tremaining: 4.45s\n",
      "975:\tlearn: 0.1407694\ttotal: 2m 53s\tremaining: 4.27s\n",
      "976:\tlearn: 0.1406655\ttotal: 2m 53s\tremaining: 4.09s\n",
      "977:\tlearn: 0.1406304\ttotal: 2m 53s\tremaining: 3.91s\n",
      "978:\tlearn: 0.1405571\ttotal: 2m 54s\tremaining: 3.73s\n",
      "979:\tlearn: 0.1404404\ttotal: 2m 54s\tremaining: 3.56s\n",
      "980:\tlearn: 0.1403091\ttotal: 2m 54s\tremaining: 3.38s\n",
      "981:\tlearn: 0.1401443\ttotal: 2m 54s\tremaining: 3.2s\n",
      "982:\tlearn: 0.1400603\ttotal: 2m 54s\tremaining: 3.02s\n",
      "983:\tlearn: 0.1399734\ttotal: 2m 54s\tremaining: 2.84s\n",
      "984:\tlearn: 0.1398533\ttotal: 2m 55s\tremaining: 2.67s\n",
      "985:\tlearn: 0.1397114\ttotal: 2m 55s\tremaining: 2.49s\n",
      "986:\tlearn: 0.1395873\ttotal: 2m 55s\tremaining: 2.31s\n",
      "987:\tlearn: 0.1394925\ttotal: 2m 55s\tremaining: 2.13s\n",
      "988:\tlearn: 0.1394351\ttotal: 2m 55s\tremaining: 1.95s\n",
      "989:\tlearn: 0.1393607\ttotal: 2m 55s\tremaining: 1.78s\n",
      "990:\tlearn: 0.1392798\ttotal: 2m 56s\tremaining: 1.6s\n",
      "991:\tlearn: 0.1392374\ttotal: 2m 56s\tremaining: 1.42s\n",
      "992:\tlearn: 0.1391753\ttotal: 2m 56s\tremaining: 1.24s\n",
      "993:\tlearn: 0.1390811\ttotal: 2m 56s\tremaining: 1.06s\n",
      "994:\tlearn: 0.1390257\ttotal: 2m 56s\tremaining: 888ms\n",
      "995:\tlearn: 0.1389675\ttotal: 2m 56s\tremaining: 710ms\n",
      "996:\tlearn: 0.1388612\ttotal: 2m 56s\tremaining: 532ms\n",
      "997:\tlearn: 0.1388163\ttotal: 2m 57s\tremaining: 355ms\n",
      "998:\tlearn: 0.1387633\ttotal: 2m 57s\tremaining: 177ms\n",
      "999:\tlearn: 0.1386746\ttotal: 2m 57s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x26814fc0b88>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catb = CatBoostClassifier()\n",
    "catb.fit(X_train_40, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = catb.predict(X_test_40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1288,    0,    3,    1,    2,    1,    8,    1,    4,    2],\n",
       "       [   0, 1585,    7,    6,    3,    2,    1,    2,    1,    1],\n",
       "       [  10,    4, 1362,   15,    8,    1,    6,   12,   17,    0],\n",
       "       [   1,    3,   17, 1308,    0,   24,    3,   13,   22,   13],\n",
       "       [   3,    4,    3,    1, 1349,    1,   12,    6,    9,   31],\n",
       "       [   4,    1,    3,   22,    4, 1187,   17,    4,    7,    3],\n",
       "       [   6,    2,    2,    1,    5,    9, 1366,    0,    3,    0],\n",
       "       [   0,   10,   14,    1,    9,    2,    0, 1400,    1,   35],\n",
       "       [   5,   10,   10,   15,    3,   17,    4,    6, 1275,    9],\n",
       "       [   4,    5,    5,   20,   40,    5,    0,   28,   12, 1233]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9537857142857142"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 캣 부스트도 소용이 없다. 다시 주성분 개수를 조절해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca43 = PCA(n_components = 43)\n",
    "X_train_43 = pd.DataFrame(pca43.fit_transform(X_train), columns = [\"Com\" + str(i) for i in range(1,44)])\n",
    "X_test_43 = pd.DataFrame(pca43.transform(X_test), columns = [\"Com\" + str(i) for i in range(1,44)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier()\n",
    "forest.fit(X_train_43, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9535714285714286"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = forest.predict(X_test_43)\n",
    "forest.score(X_test_43, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 47, 43으로 조절해보아도 크게 차이가 없다....  \n",
    "인생이란 ...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
